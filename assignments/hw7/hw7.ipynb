{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xDUIT6-7igYM"
      },
      "source": [
        "# HW7: Autoregressive HMMs\n",
        "\n",
        "---\n",
        "\n",
        "**Name:**\n",
        "\n",
        "**Names of any collaborators:**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms72VUz4Odvx"
      },
      "source": [
        "![](https://ars.els-cdn.com/content/image/1-s2.0-S0896627315010375-gr1.jpg)\n",
        "\n",
        "In this lab we'll develop hidden Markov models, specifically Gaussian autoregressive hidden Markov models, to analyze depth videos of freely behaving mice. We'll implement the model developed by Wiltschko et al (2015) and extended in Markowitz et al (2018). Figure 1 of Wiltschko et al is reproduced above. \n",
        "\n",
        "**References**\n",
        "\n",
        "Markowitz, J. E., Gillis, W. F., Beron, C. C., Neufeld, S. Q., Robertson, K., Bhagat, N. D., ... & Sabatini, B. L. (2018). The striatum organizes 3D behavior via moment-to-moment action selection. Cell, 174(1), 44-58.\n",
        "\n",
        "Wiltschko, A. B., Johnson, M. J., Iurilli, G., Peterson, R. E., Katon, J. M., Pashkovski, S. L., ... & Datta, S. R. (2015). Mapping sub-second structure in mouse behavior. Neuron, 88(6), 1121-1135.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "osPXb_Uy3yQ0"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQEnxDJ5h0G8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pynwb\n",
        "!wget -nc https://raw.githubusercontent.com/slinderman/stats305c/main/assignments/hw7/helpers.py\n",
        "!wget -nc https://www.dropbox.com/s/564wzasu1w7iogh/moseq_data.zip\n",
        "!unzip -n moseq_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDbCFyC431bM",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# First, import necessary libraries.\n",
        "import torch\n",
        "from torch.distributions import MultivariateNormal, Categorical\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from tqdm.auto import trange\n",
        "from google.colab import files\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.cm import get_cmap\n",
        "import seaborn as sns\n",
        "sns.set_context(\"notebook\")\n",
        "\n",
        "# We've written a few helpers for plotting, etc.\n",
        "import helpers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CD-FmY4jv2pG"
      },
      "source": [
        "## Part 1: Implement the forward-backward algorithm\n",
        "\n",
        "First, implement the forward-backward algorithm for computing the posterior distribution on latent states of a hidden Markov model, $q(z) = p(z \\mid x, \\Theta)$. Specifically, this algorithm will return a $T \\times K$ matrix where each entry represents the posterior probability that $q(z_t = k)$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgo3i5YialWH"
      },
      "source": [
        "### Problem 1a [Code]: Implement the forward pass\n",
        "\n",
        "As we derived in class, the forward pass recursively computes the _normalized_ forward messages $\\tilde{\\alpha}_t$ and the marginal log likelihood $\\log p(x \\mid \\Theta) = \\sum_{t} \\log A_t$.\n",
        "\n",
        "**Notes**:\n",
        "- This function takes in the _log_ likelihoods, $\\log \\ell_{tk}$, so you'll have to exponentiate in the forward pass\n",
        "- You need to be careful exponentiating though. If the log likelihoods are very negative, they'll all be essentially zero when exponentiated and you'll run into a divide-by-zero error when you compute the normalized forward message. Alternatively, if they're large positive numbers, your exponent will blow up and you'll get nan's in your calculations. \n",
        "- To avoid numerical issues, subtract $\\max_k (\\log \\ell_{tk})$ prior to exponentiating. It won't affect the normalized messages, but you will have to account for it in your computation of the marginal likelihood.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7mPzdb_aez3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def forward_pass(initial_probs, transition_matrix, log_likes):\n",
        "    \"\"\"\n",
        "    Perform the (normalized) forward pass of the HMM.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    initial_probs: $\\pi$, the initial state probabilities. Length K, sums to 1.\n",
        "    transition_matrix: $P$, a KxK transition matrix. Rows sum to 1.\n",
        "    log_likes: $\\log \\ell_{t,k}$, a TxK matrix of _log_ likelihoods.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    alphas: TxK matrix with _normalized_ forward messages $\\tilde{\\alpha}_{t,k}$\n",
        "    marginal_ll: Scalar marginal log likelihood $\\log p(x | \\Theta)$\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    alphas = ...\n",
        "    marginal_ll = ...\n",
        "    #\n",
        "    ##\n",
        "    return alphas, marginal_ll"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RmWcflIhgw3b"
      },
      "source": [
        "### Problem 1b [Code]: Implement the backward pass\n",
        "\n",
        "Recursively compute the backward messages $\\beta_t$. Again, normalize to avoid underflow, and be careful when you exponentiate the log likelihoods. The same trick of subtracting the max before exponentiating will work here too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKhMtK8ifj2Q",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def backward_pass(transition_matrix, log_likes):\n",
        "    \"\"\"\n",
        "    Perform the (normalized) backward pass of the HMM.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    transition_matrix: $P$, a KxK transition matrix. Rows sum to 1.\n",
        "    log_likes: $\\log \\ell_{t,k}$, a TxK matrix of _log_ likelihoods.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    betas: TxK matrix with _normalized_ backward messages $\\tilde{\\beta}_{t,k}$\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    betas = ...\n",
        "    #\n",
        "    ##\n",
        "\n",
        "    return betas\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fmPQ3BYAlI-Y"
      },
      "source": [
        "### Problem 1c [Code]: Combine the forward and backward passes\n",
        "\n",
        "Compute the posterior marginal probabilities. We call these the `expected_states` because $q(z_t = k) = \\mathbb{E}_{q(z)}[\\mathbb{I}[z_t = k]]$. To compute them, combine the forward messages, backward messages, and the likelihoods, then normalize. Again, be careful when exponentiating the likelihoods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pvqAsTHhS9N",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class HMMPosterior:\n",
        "    expected_states: torch.Tensor\n",
        "    marginal_ll: float\n",
        "\n",
        "\n",
        "def forward_backward(initial_probs, transition_matrix, log_likes):\n",
        "    \"\"\"\n",
        "    Fun the forward and backward passes and then combine to compute the \n",
        "    posterior probabilities q(z_t=k).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    initial_probs: $\\pi$, the initial state probabilities. Length K, sums to 1.\n",
        "    transition_matrix: $P$, a KxK transition matrix. Rows sum to 1.\n",
        "    log_likes: $\\log \\ell_{t,k}$, a TxK matrix of _log_ likelihoods.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    posterior: an HMMPosterior object\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    expected_states = ...\n",
        "    marginal_ll = ...\n",
        "    #\n",
        "    ##\n",
        "    \n",
        "    # Package the results into a HMMPosterior\n",
        "    return HMMPosterior(expected_states=expected_states,\n",
        "                        marginal_ll=marginal_ll)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MWv_l-DIrxRm"
      },
      "source": [
        "### Time it on some more realistic sizes\n",
        "\n",
        "It should take about 3 seconds for a $T=36000$ time series with $K=50$ states. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67qw4unFi7LI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%timeit forward_backward(*helpers.random_args(36000, 50))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ci6teNka_OvN"
      },
      "source": [
        "## Part 2: Gaussian HMM\n",
        "\n",
        "First we'll implement a hidden Markov model (HMM) with Gaussian observations. This is the same model we studied in class,\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "p(x, z; \\Theta) &= \\mathrm{Cat}(z_1; \\pi) \\prod_{t=2}^{T} \\mathrm{Cat}(z_t; P_{z_{t-1}}) \\prod_{t=1}^T \\mathcal{N}(x_t; \\mu_{z_t}, \\Sigma_{z_t})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "with parameters $\\Theta = \\pi, P, \\{\\mu_k, \\Sigma_k\\}_{k=1}^K$. The observed datapoints are $x_t \\in \\mathbb{R}^{D}$ and the latent states are $z_t \\in \\{1,\\ldots, K\\}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Zb097iW1C5"
      },
      "source": [
        "### Problem 2a [Code]: Complete the following GaussianHMM class\n",
        "\n",
        "Finish the code below to implement a `GaussianHMM` object. Specifically, complete the following functions:\n",
        "- `sample`: to simulate from the joint distribution $p(z_{1:T}, x_{1:T})$.\n",
        "- `e_step`: to compute the posterior expectations and marginal likelihood using the `forward_backward` function you wrote in Part 1.\n",
        "- `m_step`: to update the parameters by maximizing the expected log joint probability under the posterior from `e_step`.\n",
        "- `fit`: to run the EM algorithm.\n",
        "\n",
        "Notes:\n",
        "- Recall that in Homework 4 you derived the M-step for a Gaussian mixture model with a normal-inverse Wishart prior distribution. You can reuse the same calculations for the M-step of the Gaussian HMM. Here, we are assuming an improper uniform prior on the parameters $(\\mu_k, \\Sigma_k)$, but you can think of that as a normal-inverse-Wishart prior with parameters $\\mu_0=0$, $\\kappa_0=0$, $\\Sigma_0=0$, and $\\nu_0=-(D+2)$. \n",
        "- For numerical stability, in the M-step you may need to add a small amount to the diagonal of $\\Sigma_k$ and explicitly make it symmetric; e.g. after solving for the optimal covariance do,\n",
        "```\n",
        "Sigma = 0.5 * (Sigma + Sigma.T) + 1e-4 * torch.eye(self.data_dim)\n",
        "```\n",
        "You can think of this as a very weak NIW prior.\n",
        "- We will **keep the initial distribution and transition matrix fixed** in this code!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAnR_Aqvn4iH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class GaussianHMM:\n",
        "    \"\"\"Simple implementation of a Gaussian HMM.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_states, data_dim):\n",
        "        self.num_states = num_states\n",
        "        self.data_dim = data_dim\n",
        "\n",
        "        # Initialize the HMM parameters\n",
        "        self.initial_probs = torch.ones(num_states) / num_states\n",
        "        self.transition_matrix = \\\n",
        "            0.9 * torch.eye(num_states) + \\\n",
        "            0.1 * torch.ones((num_states, num_states)) / num_states\n",
        "        self.emission_means = torch.randn(num_states, data_dim)\n",
        "        self.emission_covs = torch.eye(data_dim).repeat(num_states, 1, 1)\n",
        "\n",
        "    def sample(self, num_timesteps, seed=0):\n",
        "        \"\"\"Sample the HMM\n",
        "        \"\"\"\n",
        "        # Set random seed\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        # Initialize outputs\n",
        "        states = torch.full((num_timesteps,), -1, dtype=int)\n",
        "        data = torch.zeros((num_timesteps, self.data_dim))\n",
        "\n",
        "        ## \n",
        "        # YOUR CODE HERE\n",
        "        states[0] = ...                     # Sample the initial state\n",
        "        for t in range(num_timesteps):\n",
        "            data[t] = ...                   # Sample emission\n",
        "            if t < num_timesteps - 1:\n",
        "                states[t+1] = ...           # Sample next state\n",
        "        #\n",
        "        ##\n",
        "\n",
        "        return states, data\n",
        "\n",
        "    def e_step(self, data):\n",
        "        \"\"\"Run the forward-backward algorithm and return the posterior distribution\n",
        "        over latent states given the data and the current model parameters.\n",
        "        \"\"\"\n",
        "        ##\n",
        "        # YOUR CODE HERE\n",
        "        posterior = ...\n",
        "        #\n",
        "        ##\n",
        "        return posterior\n",
        "\n",
        "    def m_step(self, data, posterior):\n",
        "        \"\"\"Perform one m-step to update the emission means and covariance given the\n",
        "        data and the posteriors output by the forward-backward algorithm.\n",
        "        \n",
        "        NOTE: We will keep the initial distribution and transition matrix fixed!\n",
        "        \"\"\"\n",
        "        ##\n",
        "        # YOUR CODE HERE\n",
        "        self.emission_means = ...\n",
        "        self.emission_covs = ...\n",
        "        #\n",
        "        ##\n",
        "\n",
        "    def fit(self, data, num_iters=100):\n",
        "        \"\"\"Estimate the parameters of the HMM with expectation-maximization (EM).\n",
        "        \"\"\"\n",
        "        # Initialize the posterior randomly\n",
        "        expected_states = torch.rand(len(data), num_states)\n",
        "        expected_states /= expected_states.sum(axis=1, keepdims=True)\n",
        "        posterior = HMMPosterior(expected_states=expected_states,\n",
        "                                 marginal_ll=-torch.inf)\n",
        "        \n",
        "        # Track the marginal log likelihood of the data over EM iterations\n",
        "        lls = []\n",
        "\n",
        "        # Main loop of the EM algorithm\n",
        "        for itr in trange(num_iters):\n",
        "            ###\n",
        "            # YOUR CODE HERE\n",
        "\n",
        "            # E step: compute the posterior distribution given current parameters\n",
        "            posterior = ...\n",
        "\n",
        "            # Track the log likeliood\n",
        "            lls.append(...)\n",
        "\n",
        "            # M step: udate model parameters under the current posterior\n",
        "            ...\n",
        "            #\n",
        "            ##\n",
        "\n",
        "            \n",
        "        # convert lls to arrays and return\n",
        "        lls = torch.tensor(lls)\n",
        "        return lls, posterior\n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "deoZC_R81oNz"
      },
      "source": [
        "### Sample synthetic data from the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrSLgmm61qhj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Make a \"true\" HMM\n",
        "num_states = 5\n",
        "data_dim = 2\n",
        "true_hmm = GaussianHMM(num_states, data_dim)\n",
        "\n",
        "# Override the emission distribution\n",
        "true_hmm.emission_means = torch.column_stack([\n",
        "    torch.cos(torch.linspace(0, 2 * torch.pi, num_states+1))[:-1],\n",
        "    torch.sin(torch.linspace(0, 2 * torch.pi, num_states+1))[:-1]\n",
        "])\n",
        "true_hmm.emission_covs = 0.25**2 * torch.eye(data_dim).repeat(num_states, 1, 1)\n",
        "\n",
        "# Sample the model\n",
        "num_timesteps = 200\n",
        "states, emissions = true_hmm.sample(num_timesteps, seed=305+ord('c'))\n",
        "\n",
        "# Plot the data and the smoothed data\n",
        "lim = 1.05 * abs(emissions).max()\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(states[None,:],\n",
        "           aspect=\"auto\",\n",
        "           interpolation=\"none\",\n",
        "           cmap=helpers.cmap,\n",
        "           vmin=0,\n",
        "           vmax=len(helpers.colors)-1,\n",
        "           extent=(0, num_timesteps, -lim, (data_dim)*lim))\n",
        "\n",
        "means = true_hmm.emission_means[states]\n",
        "for d in range(data_dim):\n",
        "    plt.plot(emissions[:,d] + lim * d, '-k')\n",
        "    plt.plot(means[:,d] + lim * d, ':k')\n",
        "\n",
        "plt.xlim(0, num_timesteps)\n",
        "plt.xlabel(\"time\")\n",
        "plt.yticks(lim * torch.arange(data_dim), [\"$x_{}$\".format(d+1) for d in range(data_dim)])\n",
        "\n",
        "plt.title(\"Simulated data from an HMM\")\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zW7ZYBgxGS-p"
      },
      "source": [
        "### Fit the Gaussian HMM to synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_etTe3wFUPi8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Build the HMM and fit it with EM\n",
        "hmm = GaussianHMM(num_states, data_dim)\n",
        "lls, posterior = hmm.fit(emissions)\n",
        "\n",
        "# Plot the log likelihoods. They should go up.\n",
        "plt.plot(lls)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"marginal log lkhd\")\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGWdlGDEpMPK",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Plot the true and inferred states\n",
        "fig, axs = plt.subplots(2, 1, sharex=True)\n",
        "axs[0].imshow(states[None,:],\n",
        "              aspect=\"auto\",\n",
        "              interpolation=\"none\",\n",
        "              cmap=helpers.cmap,\n",
        "              vmin=0, vmax=len(helpers.colors)-1)\n",
        "axs[0].set_yticks([])\n",
        "axs[0].set_title(\"true states\")\n",
        "\n",
        "axs[1].imshow(posterior.expected_states.T,\n",
        "              aspect=\"auto\",\n",
        "              interpolation=\"none\",\n",
        "              cmap=\"Greys\",\n",
        "              vmin=0, vmax=1)\n",
        "axs[1].set_yticks(torch.arange(num_states))\n",
        "axs[1].set_ylabel(\"state\")\n",
        "axs[1].set_xlabel(\"time\")\n",
        "axs[1].set_title(\"expected states\")\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EEKMGAv8lSkK"
      },
      "source": [
        "### Problem 2b [Code]: Cross validation\n",
        "\n",
        "Fit HMMs with varying numbers of discrete states, $K$, and compare them on held-out test data. For each $K$, fit an HMM multiple times from different initial conditions to guard against local optima in the EM fits. Plot the held-out likelihoods as a function of $K$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkAAhdvH_s8P",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "##\n",
        "# Your code here\n",
        "\n",
        "#\n",
        "##"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SvziNmI38o5q"
      },
      "source": [
        "### Problem 2c [Short Answer]: Initialization\n",
        "\n",
        "The HMM doens't always find the true latent states. Sometimes it merges the red and blue states, for example. Running multiple restarts with random initializations sometimes works, but not always. Can you think of smarter initialization strategies? "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k33D9t3GA1Gl"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "od7nJS8yH9z4"
      },
      "source": [
        "## Part 3: Autoregressive HMMs\n",
        "\n",
        "Autoregressive hidden Markov models (ARHMMs) replace the Gaussian observations with an AR model:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "p(x, z \\mid \\Theta) &= \\mathrm{Cat}(z_1 \\mid \\pi) \\prod_{t=2}^{T} \\mathrm{Cat}(z_t \\mid P_{z_{t-1}}) \\prod_{t=1}^T p(x_t \\mid x_{1:t-1}, z_t)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The model is \"autoregressive\" because $x_t$ depends not only on $z_t$ but on $x_{1:t-1}$ as well. The precise form of this dependence varies; here we will consider linear Gaussian dependencies on only the most recent $L$ timesteps,:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "p(x_t \\mid x_{1:t-1}, z_t) &= \\mathcal{N}\\left(x_t \\mid \\sum_{l=1}^L A_{z_t,l} x_{t-l} + b_{z_t}, Q_{z_t} \\right)  \\qquad \\text{for } t > L\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "To complete the model, assume\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "p(x_t \\mid x_{1:t-1}, z_t) &= \\mathcal{N}\\left(x_t \\mid 0, I \\right)  \\qquad \\text{for } t \\leq L\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The new parameters are $\\Theta = \\pi, P, \\{\\{A_{k,l}\\}_{l=1}^L, b_{k}, Q_k\\}_{k=1}^K$, which include weights $A_{k,l} \\in \\mathbb{R}^{D \\times D}$ for each of the $K$ states and the $L$ lags, and a bias vector $b_k \\in \\mathbb{R}^D$.\n",
        "\n",
        "Note that we can write this as a simple **linear regression**,\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "p(x_t \\mid x_{1:t-1}, z_t) &= \\mathcal{N}\\left(x_t \\mid W_k \\phi_t , Q_{z_t} \\right)  \n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $\\phi_t = (x_{t-1}, \\ldots, x_{t-L}, 1) \\in \\mathbb{R}^{LD +1}$ is a vector of covariates (aka features) that includes the past $L$ time steps along with a 1 for the bias term.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "W_k = \\begin{bmatrix} A_{k,1}  & A_{k,2} & \\ldots & A_{kL} & b_k \\end{bmatrix}\n",
        "\\in \\mathbb{R}^{D \\times LD + 1}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "is a block matrix of the autoregressive weights and the bias. \n",
        "\n",
        "_Note that the covariates are fixed functions of the data so we can precompute them, if we know the number of lags $L$._ "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QPvxGpRCXiJL"
      },
      "source": [
        "### Problem 3a [Math]: Derive the natural parameters and sufficient statistics for a linear regression\n",
        "\n",
        "Expand the expected log likelihood of a linear regression model in terms of $W_k$ and $b_k$,\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbb{E}_{q(z)}\\left[ \\sum_{t=1}^T \\mathbb{I}[z_t=k] \\cdot \\log \\mathcal{N}(x_t \\mid W_k \\phi_t, Q_k) \\right].\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Write it as a sum of inner products between natural parameters (i.e. functions of $W_k$ and $Q_k$) and expected sufficient statistics (i.e. functions of $q$, $x$ and $\\phi$). "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r6fzT6z_A8tR"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "---\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "il1EtQius9Xq"
      },
      "source": [
        "### Problem 3b [Math]: Solve for the optimal linear regression parameters given expected sufficient statistics\n",
        "\n",
        "Solve for $W_k^\\star, Q_k^\\star$ that maximize the objective above in terms of the expected sufficient statistics.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c18dOwkdBLmE"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "98bDuGZWXpqk"
      },
      "source": [
        "### Problem 3c [Code]: Implement an Autoregressive HMM\n",
        "\n",
        "Now complete the code below to implement an AR-HMM. \n",
        "\n",
        "_Note: This code assumes $L=1$_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYEFQol4q4KD",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class AutoregressiveHMM:\n",
        "    \"\"\"Simple implementation of an Autoregressive HMM.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_states, data_dim):\n",
        "        self.num_states = num_states\n",
        "        self.data_dim = data_dim\n",
        "\n",
        "        # Initialize the HMM parameters\n",
        "        self.initial_probs = torch.ones(num_states) / num_states\n",
        "        self.initial_mean = torch.zeros(data_dim)\n",
        "        self.initial_cov = torch.eye(data_dim)\n",
        "        self.transition_matrix = \\\n",
        "            0.9 * torch.eye(num_states) + \\\n",
        "            0.1 * torch.ones((num_states, num_states)) / num_states\n",
        "        self.emission_dynamics = torch.randn(num_states, data_dim, data_dim)\n",
        "        self.emission_bias = torch.randn(num_states, data_dim)\n",
        "        self.emission_cov = torch.eye(data_dim).repeat(num_states, 1, 1)\n",
        "\n",
        "    def sample(self, num_timesteps, seed=0):\n",
        "        \"\"\"Sample the HMM\n",
        "        \"\"\"\n",
        "        # Set random seed\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        # Initialize outputs\n",
        "        states = torch.full((num_timesteps,), -1, dtype=int)\n",
        "        data = torch.zeros((num_timesteps, self.data_dim))\n",
        "\n",
        "        ## \n",
        "        # YOUR CODE HERE\n",
        "        states[0] = ...                     # Sample the initial state\n",
        "        data[0] = ...                       # Sample the initial emission\n",
        "        for t in range(1, num_timesteps):\n",
        "            states[t] = ...                 # Sample state\n",
        "            data[t] = ...                   # Sample emission\n",
        "        #\n",
        "        ##\n",
        "        return states, data\n",
        "\n",
        "    def e_step(self, data):\n",
        "        \"\"\"Perform one e-step to compute the posterior over the latent states\n",
        "        given the data.\n",
        "        \"\"\"\n",
        "        ###\n",
        "        # YOUR CODE HERE\n",
        "        posterior = ...\n",
        "        #\n",
        "        ##\n",
        "        return posterior\n",
        "\n",
        "    def m_step(self, data, posterior):\n",
        "        \"\"\"Perform one m-step to update the emission means and covariance given the\n",
        "        data and the posteriors output by the forward-backward algorithm.\n",
        "        \"\"\"\n",
        "        ##\n",
        "        # YOUR CODE HERE\n",
        "        self.emission_dynamics = ...\n",
        "        self.emission_bias = ...\n",
        "        self.emission_covs = ...\n",
        "        #\n",
        "        ##\n",
        "\n",
        "    def fit(self, data, num_iters=100):\n",
        "        \"\"\"Estimate the parameters of the HMM with expectation-maximization (EM).\n",
        "        \"\"\"\n",
        "        # Initialize the posterior randomly\n",
        "        expected_states = F.softmax(torch.randn(len(data), num_states), dim=0)\n",
        "        posterior = HMMPosterior(expected_states=expected_states,\n",
        "                                 marginal_ll=-torch.inf)\n",
        "        \n",
        "        # Track the marginal log likelihood of the data over EM iterations\n",
        "        lls = []\n",
        "\n",
        "        # Main loop of the EM algorithm\n",
        "        for itr in trange(num_iters):\n",
        "            ###\n",
        "            # YOUR CODE HERE\n",
        "            # E step: compute the posterior given the current parameters\n",
        "            posterior = ...\n",
        "\n",
        "            # Track the log likeliood\n",
        "            lls.append(...)\n",
        "\n",
        "            # M step: update model parameters under the current posterior\n",
        "            ...\n",
        "            #\n",
        "            ##\n",
        "\n",
        "        # convert lls to arrays and return\n",
        "        lls = torch.tensor(lls)\n",
        "        return lls, posterior        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ahVjBKouG5Hk"
      },
      "source": [
        "### Sample synthetic data from the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6ZQVD_jG7oF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Make observation distributions\n",
        "num_states = 5\n",
        "data_dim = 2\n",
        "\n",
        "# Initialize the transition matrix to proceed in a cycle\n",
        "transition_probs = (torch.arange(num_states)**10).type(torch.float)\n",
        "transition_probs /= transition_probs.sum()\n",
        "transition_matrix = torch.zeros((num_states, num_states))\n",
        "for k, p in enumerate(transition_probs.flip(0)):\n",
        "    transition_matrix += torch.roll(p * torch.eye(num_states), k, dims=1)\n",
        "\n",
        "# Initialize the AR dynamics to spiral toward points\n",
        "rotation_matrix = \\\n",
        "    lambda theta: torch.tensor([[torch.cos(theta), -torch.sin(theta)],\n",
        "                                [torch.sin(theta),  torch.cos(theta)]])\n",
        "theta = torch.tensor(-torch.pi / 25)\n",
        "dynamics = 0.8 * rotation_matrix(theta).repeat(num_states, 1, 1)\n",
        "bias = torch.column_stack([torch.cos(torch.linspace(0, 2*torch.pi, num_states+1)[:-1]), \n",
        "                        torch.sin(torch.linspace(0, 2*torch.pi, num_states+1)[:-1])])\n",
        "covs = torch.tile(0.001 * torch.eye(data_dim), (num_states, 1, 1))\n",
        "\n",
        "# Compute the stationary points\n",
        "stationary_points = torch.linalg.solve(torch.eye(data_dim) - dynamics, bias)\n",
        "\n",
        "# Construct an ARHMM and overwrite the emission parameters\n",
        "true_arhmm = AutoregressiveHMM(num_states, data_dim)\n",
        "true_arhmm.transition_matrix = transition_matrix\n",
        "true_arhmm.emission_dynamics = dynamics\n",
        "true_arhmm.emission_bias = bias\n",
        "true_arhmm.emission_cov = covs\n",
        "\n",
        "# Plot the true ARHMM dynamics for each of the 5 states\n",
        "helpers.plot_dynamics(true_arhmm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfFPXEm3CQPV",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Sample from the true ARHMM\n",
        "states, data = true_arhmm.sample(10000, seed=305+ord('c'))\n",
        "\n",
        "# Plot the data\n",
        "for k in range(num_states):\n",
        "    plt.plot(*data[states==k].T, 'o', color=helpers.colors[k],\n",
        "         alpha=0.75, markersize=3)\n",
        "    \n",
        "plt.plot(*data.T, '-k', lw=0.5, alpha=0.2)\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$x_2$\")\n",
        "plt.gca().set_aspect(\"equal\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E1gYK5iOTWwz"
      },
      "source": [
        "### Fit an ARHMM to the synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f47Y9yRVQEwZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Construct another ARHMM and fit it with EM\n",
        "arhmm = AutoregressiveHMM(num_states, data_dim)\n",
        "lls, posterior = arhmm.fit(data, num_iters=25)\n",
        "\n",
        "# Plot the log likelihoods. They should go up.\n",
        "plt.plot(lls)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"marginal log lkhd\")\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxsgahdxQEwa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Plot the true and inferred states\n",
        "fig, axs = plt.subplots(2, 1, sharex=True)\n",
        "axs[0].imshow(states[None,:],\n",
        "              aspect=\"auto\",\n",
        "              interpolation=\"none\",\n",
        "              cmap=helpers.cmap,\n",
        "              vmin=0, vmax=len(helpers.colors)-1)\n",
        "axs[0].set_xlim(0, 1000)\n",
        "axs[0].set_yticks([])\n",
        "axs[0].set_title(\"true states\")\n",
        "\n",
        "axs[1].imshow(posterior.expected_states.T,\n",
        "              aspect=\"auto\",\n",
        "              interpolation=\"none\",\n",
        "              cmap=\"Greys\",\n",
        "              vmin=0, vmax=1)\n",
        "axs[1].set_xlim(0, 1000)\n",
        "axs[1].set_yticks(torch.arange(num_states))\n",
        "axs[1].set_ylabel(\"state\")\n",
        "axs[1].set_xlabel(\"time\")\n",
        "axs[1].set_title(\"expected states\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQFeJF8kSxuf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Plot the learned dynamics\n",
        "helpers.plot_dynamics(arhmm)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ryRKihRYN0pE"
      },
      "source": [
        "As with the Gaussian HMM, you may find that the ARHMM doesn't perfectly learn the true underlying states. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fmOwyIuJSqyi"
      },
      "source": [
        "## Part 4: Fit the ARHMM to mouse videos\n",
        "\n",
        "\n",
        "Now we'll load in some real data from depth video recordings of freely moving mice. This data is from the Datta Lab at Harvard Medical School. The references are given at the top of this notebook.\n",
        "\n",
        "The video frames, even after cropping, are still 80x80 pixels. That's a 3600 dimensional observation. In practice, the frames can be adequately reconstructed with far fewer principal components. As little as ten PCs does a pretty good job of capturing the mouse's posture.\n",
        "\n",
        "The Datta lab has already computed the principal components and included them in the NWB. We'll extract them, along with other relevant information like the centroid position and heading angle of the mouse, which we'll use for making \"crowd\" movies below. Finally, they also included labels from MoSeq, an autoregressive (AR) HMM. You'll build an ARHMM in Part 3 of the lab and infer similar discrete latent state sequences yourself!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVgfZzT22oOg",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Load one session of data\n",
        "data_dim = 10\n",
        "train_dataset, test_dataset = helpers.load_dataset(indices=[0], num_pcs=data_dim)\n",
        "train_data = train_dataset[0]\n",
        "test_data = test_dataset[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i2FQoYmRFqY7"
      },
      "source": [
        "You should now have a `train_dataset` and a `test_dataset` loaded in memory. Each dataset is a list of dictionaries, one for each mouse. Each dictionary contains a few keys, most important of which is the `data` key, containing the standardized principal component time series, as shown above. For the test dataset, we also included the `frames` key, which has the original 80x80 images. We'll use these to create the movies of each inferred state.\n",
        "\n",
        "**Note:** Keeping the data in memory is costly but convenient.  You shouldn't run out of memory in this lab, but if you ever did, a better solution might be to write the preprocessed data (e.g. with the standardized PC trajectories) back to the NWB files and reload those files as necessary during fitting."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0E-I8ikk2Fw"
      },
      "source": [
        "### Plot a slice of data \n",
        "In the background, we're showing the labels that were given to us from MoSeq, an autoregressive hidden Markov model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrRyMF9_-hLs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "helpers.plot_data_and_states(\n",
        "    train_data, train_data[\"labels\"],\n",
        "    title=\"data and given discrete states\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tTUEkd48TmLV"
      },
      "source": [
        "### Fit it!\n",
        "\n",
        "With my implementation, this takes about 5 minutes to run on Colab. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8R9_B16bRM7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Build the HMM\n",
        "num_states = 25\n",
        "data_dim = 10\n",
        "arhmm = AutoregressiveHMM(num_states, data_dim)\n",
        "\n",
        "# Fit it!\n",
        "lls, posterior = arhmm.fit(torch.tensor(train_data[\"data\"]), \n",
        "                           num_iters=50)\n",
        "\n",
        "plt.plot(lls, label=\"train\")\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"marginal log lkhd\")\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zqzJJDWUPCcz"
      },
      "source": [
        "### Plot the data and the inferred states\n",
        "\n",
        "\n",
        "We'll make the same plot as above (in the warm-up) but using our inferred states instead. Hopefully, the states seem to switch along with changes in the data. \n",
        "\n",
        "**Note**: We're showing the state with the highest marginal probability, $z_t^\\star = \\mathrm{arg} \\, \\mathrm{max}_k \\; q(z_t = k)$. This is different from the most likely state path, $z_{1:T}^\\star = \\mathrm{arg}\\,\\mathrm{max} \\; q(z)$. We could compute the latter with the Viterbi algorithm, which is similar to the forward-backward algorithm you implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfaK78a7lz7U",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "arhmm_states = posterior.expected_states.argmax(1)\n",
        "helpers.plot_data_and_states(train_data, arhmm_states)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UVPESoqtPFA1"
      },
      "source": [
        "### Plot the state usage histogram\n",
        "\n",
        "\n",
        "The state usage histogram shows how often each discrete state was used under the posterior distribution. You'll probably see a long tail of states with non-trivial usage (hundreds of frames), all the way out to state 50. That suggests the model is using all its available capacity, and we could probably crank the number of states up even further for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENEZ70G3HTEV",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Sort states by usage\n",
        "arhmm_states = posterior.expected_states.argmax(1)\n",
        "arhmm_usage = torch.bincount(arhmm_states, minlength=num_states)\n",
        "arhmm_order = torch.argsort(arhmm_usage).flip(0)\n",
        "\n",
        "plt.bar(torch.arange(num_states), arhmm_usage[arhmm_order])\n",
        "plt.xlabel(\"state index [ordered]\")\n",
        "plt.ylabel(\"num frames\")\n",
        "plt.title(\"histogram of inferred state usage\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "haiNm-SRPIeD"
      },
      "source": [
        "### Plot some \"crowd\" movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_Gen3zHcBQu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "test_posterior = arhmm.e_step(torch.tensor(test_data[\"data\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTaoUpdIsajB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "helpers.play(helpers.make_crowd_movie(\n",
        "    int(arhmm_order[0]), [test_data], [test_posterior]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR4TyPgAsEJ9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "helpers.play(helpers.make_crowd_movie(\n",
        "    int(arhmm_order[1]), [test_data], [test_posterior]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oprWlKZ9LoFl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "helpers.play(helpers.make_crowd_movie(\n",
        "    int(arhmm_order[2]), [test_data], [test_posterior]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9TN-4XrLrqX",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "helpers.play(helpers.make_crowd_movie(\n",
        "    int(arhmm_order[3]), [test_data], [test_posterior]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE4XcUSdLvPP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "helpers.play(helpers.make_crowd_movie(\n",
        "    int(arhmm_order[4]), [test_data], [test_posterior]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwAqWXynC9Aj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "helpers.play(helpers.make_crowd_movie(\n",
        "    int(arhmm_order[20]), [test_data], [test_posterior]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iLSFWpR8mscG"
      },
      "source": [
        "### Download crowd movies for all states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd0GA1FbL0gh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Make \"crowd\" movies for each state and save them to disk\n",
        "# Then you can download them and play them offline\n",
        "for i in trange(num_states):\n",
        "    helpers.play(helpers.make_crowd_movie(\n",
        "        int(arhmm_order[i]), [test_data], [test_posterior]),\n",
        "        filename=\"arhmm_crowd_{}.mp4\".format(i), show=False)\n",
        "\n",
        "# Zip the movies up    \n",
        "!zip arhmm_crowd_movies.zip arhmm_crowd_*.mp4\n",
        "\n",
        "# Download the files as a zip\n",
        "files.download(\"arhmm_crowd_movies.zip\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Akd_RPa3oD5M"
      },
      "source": [
        "### Problem 4a [Short Answer]: Discussion\n",
        "\n",
        "Now that you've completed the analysis, discuss your findings in one or two paragraphs. Some questions to consider (though you need not answer all) are:\n",
        "- Did any interesting states pop out in your crowd movies? \n",
        "- Are the less frequently used states interesting or are they just noise? \n",
        "- It took a few minutes to fit data from a single mouse with ~50,000 frames of video. In practice, we have data from dozens of mice and millions of frames of video. What approaches might you take to speed up the fitting procedure?\n",
        "- Aside from runtime, what other challenges might you encounter when fitting the same model to multiple mice? What could you do to address those challenges?\n",
        "- The ARHMM finds reasonable looking discrete states (\"syllables\") but it's surely not a perfect model. What changes could you make to better model mouse behavior?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-wk3ADoOMrFK"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w9697GqPmYx_"
      },
      "source": [
        "**Formatting:** check that your code does not exceed 80 characters in line width. If you're working in Colab, you can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
        "\n",
        "Download your notebook in .ipynb format and use the following commands to convert it to PDF:\n",
        "```\n",
        "jupyter nbconvert --to pdf hw7_yourname.ipynb\n",
        "```\n",
        "\n",
        "**Dependencies:**\n",
        "\n",
        "- `nbconvert`: If you're using Anaconda for package management, \n",
        "```\n",
        "conda install -c anaconda nbconvert\n",
        "```\n",
        "\n",
        "**Upload** your .pdf file to Gradescope. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "STAT 305C Assignment 7: Autoregressive Hidden Markov Models.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
