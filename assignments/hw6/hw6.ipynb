{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 6: Neural Networks and VAEs\n",
        "\n",
        "\n",
        "STATS305C, Stanford University, Spring 2022\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slinderman/stats305c/blob/master/assignments/hw6/hw6.ipynb)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Name:** \n",
        "\n",
        "**Collaborators:** \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "DIaL2XtgC5_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this homework assignment, we will explore automatic differentiation, neural networks, and amortized variational inference. \n"
      ],
      "metadata": {
        "id": "5ZQgpVFL_pz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal, Bernoulli, Uniform\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "torch.manual_seed(305)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.cm import Blues\n",
        "import seaborn as sns\n",
        "sns.set_context(\"notebook\")"
      ],
      "metadata": {
        "id": "cYTJS_5FHb6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: Optimizing a Quadratic Objective via Gradient Descent\n",
        "\n",
        "We'll start off by optimizing a simple objective using gradient descent. We will compute the required gradients using PyTorch's automatic differentation capabilities.\n",
        "\n",
        "Consider the function $f: \\mathbb{R}^D \\to \\mathbb{R}$ given by:\n",
        "$$ f(\\mathbf{x}) = \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} $$\n",
        "where $\\mathbf{A} \\in \\mathbb{R}^{D \\times D}$ is a fixed positive definite matrix. It is obvious that a global minimizer of $f$ is $\\mathbf{x}^* = 0$. We will try to recover this known global minimizer using gradient descent. We note that although the objective is seemingly simple, a stochastic version of this objective has been used as a model for neural network loss surfaces (e.g. see [this paper](https://arxiv.org/abs/1803.02021)).\n",
        "\n",
        "We will simplify the objective further by assuming that $\\mathbf{A}$ is diagonal, so $\\mathbf{A} = \\text{diag}(\\mathbf{a})$ for some $\\mathbf{a} \\in \\mathbb{R}_{++}^D$."
      ],
      "metadata": {
        "id": "N1ulvTBkDkKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1a: Gradient Descent with a well-conditioned Objective\n",
        "\n",
        "Recall the gradient descent update rule for a fixed step size $\\alpha$ is:\n",
        "$$\\mathbf{x}^{(k + 1)} \\gets \\mathbf{x}^{(k)} - \\alpha \\nabla f(\\mathbf{x}^{(k)})$$\n",
        "\n",
        "Using PyTorch's automatic differentiation system, complete the function below implementing an optimization loop to minimize the objective."
      ],
      "metadata": {
        "id": "bijXAhrOnhHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, a):\n",
        "    \"\"\" Evaluates the objective.\n",
        "\n",
        "    Args:\n",
        "        x: (D,) tensor \n",
        "        a: (D,) tensor of positive values\n",
        "    Returns:\n",
        "        value: (,) tensor\n",
        "    \"\"\"\n",
        "    return (a * x**2).sum()\n",
        "\n",
        "def run_optimization(x, a, optimizer, num_iters):\n",
        "    \"\"\" Runs an optimization algorithm on the objective for num_iters.\n",
        "\n",
        "    Args:\n",
        "        x: Starting position for optimization. nn.Parameter of shape (D,)\n",
        "        a: parameter defining curvature of objective. tensor of shape (D,)\n",
        "        optimizer: a torch.optim Optimizer\n",
        "        num_iters: number of iterations to run optimization for\n",
        "\n",
        "    Returns:\n",
        "        xs: value of x at each iteration. tensor of shape (num_iters + 1, D)\n",
        "        losses: value of objective at each iterate. tensor of shape (num_iters + 1,)\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    # we have to make a copy of the data so that elements of list do not \n",
        "    # reference same location in memory\n",
        "    xs = [x.data.clone()]\n",
        "\n",
        "    for it in range(num_iters):\n",
        "\n",
        "        ###\n",
        "        # YOUR CODE HERE\n",
        "        # 1. zero out the gradient\n",
        "        # 2. compute the loss\n",
        "        # 3. compute the gradient of the loss\n",
        "        # 4. take one optimizer step (this will update x in place)\n",
        "        ...\n",
        "        loss = ...\n",
        "        ...\n",
        "        ##\n",
        "        losses.append(loss)\n",
        "        xs.append(x.data.clone()) \n",
        "\n",
        "    # Return the stacked losses and xs\n",
        "    losses = torch.tensor(losses)\n",
        "    xs = torch.vstack(xs)\n",
        "    return xs, losses"
      ],
      "metadata": {
        "id": "GlpA0d4_l0xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now run gradient descent on the objective in $D = 2$ dimensions, with $\\mathbf{a} = (1, 1)$. We will use 50 iterations with a learning rate of $\\alpha = 0.8$, starting from $\\mathbf{x}_0 = (10, 10)$. \n",
        "\n",
        "Note that we initialize `x` using `nn.Parameter`, which tells PyTorch we need to compute gradients with respect to `x`. Note that PyTorch's optimizers expect an iterable of `Parameters` to be passed as the first argument, so you must pass in `[x]` rather than `x` itself."
      ],
      "metadata": {
        "id": "J7Fb1YgbTcMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D = 2\n",
        "x = nn.Parameter(10 * torch.ones(D))\n",
        "a = 0.2 * torch.ones(D)\n",
        "\n",
        "### \n",
        "# YOUR CODE HERE\n",
        "optimizer = ...\n",
        "xs, losses = run_optimization(...)\n",
        "##"
      ],
      "metadata": {
        "id": "C1i0h0hCNbUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the loss curve. As you can see, the objective approaches the optimal value of zero quite quickly."
      ],
      "metadata": {
        "id": "K9ztkmzAVHqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Gradient Descent Iteration\")\n",
        "plt.ylabel(\"Objective Value\")"
      ],
      "metadata": {
        "id": "oAo4qKY8pObm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's visualize the trajectory of the gradient descent iterates $\\mathbf{x}^{(k)}$."
      ],
      "metadata": {
        "id": "XHhG6iRSVSPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_gradient_descent(xs, a, width=12, grid_size=200):\n",
        "    \"\"\" Visualizes gradient descent when iterates are two-dimensional.\"\"\"\n",
        "    \n",
        "    def batch_f(X):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "            X: (N, D) tensor\n",
        "        \"\"\"\n",
        "        return (X**2 * a).sum(dim=1)\n",
        "\n",
        "    grid_size = 200\n",
        "    x1s = np.linspace(-width, width, grid_size)\n",
        "    x2s = np.linspace(-width, width, grid_size)\n",
        "    X1, X2 = np.meshgrid(x1s, x2s)\n",
        "    points = np.transpose([np.tile(x1s, len(x2s)), np.repeat(x2s, len(x1s))])\n",
        "    points = torch.tensor(points, dtype=torch.float)\n",
        "\n",
        "    Z = batch_f(points).reshape(grid_size, grid_size)\n",
        "\n",
        "    fig = plt.figure(figsize = (10,7))\n",
        "    contours = plt.contour(X1, X2, Z, 20)\n",
        "\n",
        "    plt.plot(xs[:,0], xs[:,1])\n",
        "    plt.plot(xs[:,0], xs[:,1], '*', label = \"Cost function\")\n",
        "\n",
        "    plt.xlabel('$x_1$', fontsize=15)\n",
        "    plt.ylabel('$x_2$', fontsize=15)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kowxPmXkNcrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_gradient_descent(xs, a)"
      ],
      "metadata": {
        "id": "-vZjK4EatXYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1b: Gradient Descent with a Ill-Conditioned Objective\n",
        "\n",
        "Next, let's see how gradient descent performs on an ill-conditioned objective, where the amount of curvature in each direction varies. For our objective, we can control the amount of curvature using $\\mathbf{a}$. Repeat part (a), this time using $\\mathbf{a} = (0.05, 1.2)$ while holding all other hyperparameters constant."
      ],
      "metadata": {
        "id": "SDmlXl8OnmQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = nn.Parameter(10 * torch.ones(D))\n",
        "a = torch.tensor([0.05, 1.2])\n",
        "\n",
        "###\n",
        "# YOUR CODE HERE\n",
        "optimizer = ...\n",
        "xs, losses = run_optimization(...)\n",
        "##"
      ],
      "metadata": {
        "id": "E08kFw1SSCQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Gradient Descent Iteration\")\n",
        "plt.ylabel(\"Objective Value\")"
      ],
      "metadata": {
        "id": "irWgSJuWSJH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_gradient_descent(xs, a)"
      ],
      "metadata": {
        "id": "_EO10KBzSJTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain why the gradient descent iterates oscillate around the line $\\mathbf{x}_2 = 0$ and suggest one change that could be made to eliminate this behavior, without changing the objective itself."
      ],
      "metadata": {
        "id": "k6ZDrxVfVhAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "*Your answer here.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6pxYVqTmVslV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1c: Optimizing a High-Dimensional Objective\n",
        "\n",
        "Now let's tackle a more challenging, higher-dimensional problem. We'll use the same objective, but this time use $\\mathbf{a} \\in \\mathbb{R}^{10}$ where:\n",
        "$$a_i = 10^{-2 + \\frac{4(i - 1)}{9}}$$\n",
        "for $i \\in \\{1, \\dots, 10\\}$. This means the curvature of the dimensions ranges from $0.01$ to $100$.\n",
        "\n",
        "Experiment with different optimizers and hyperparameter settings on this problem, starting from the initial point $x^{(0)} = (10, \\dots, 10)$ and run your chosen optimizer for $1000$ iterations. You can see a complete list of optimizers PyTorch has implemented [here](https://pytorch.org/docs/stable/optim.html#algorithms). Find an optimizer/hyperparameter regime that achieves a final loss of less than $0.01$."
      ],
      "metadata": {
        "id": "uN_aH--DxVV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = nn.Parameter(10 * torch.ones(10))\n",
        "a = torch.tensor([10**i for i in np.linspace(-2, 2, num=10)])\n",
        "\n",
        "### \n",
        "# YOUR CODE HERE\n",
        "optimizer = ...\n",
        "xs, losses = run_optimization(...)\n",
        "##"
      ],
      "metadata": {
        "id": "0S3qaLjmxbSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Optimization Iteration\")\n",
        "plt.ylabel(\"Objective Value\")"
      ],
      "metadata": {
        "id": "NR5UYnym1imB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final loss: {:.10f}\".format(losses[-1].item()))"
      ],
      "metadata": {
        "id": "R0Q8YK8_0k4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Neural Network Classification\n",
        "\n",
        "Next, we will use a neural network to solve a classification problem for which the data is not linearly separable. We will implement the network as a PyTorch `nn.module` and train it using gradient descent, computing gradients using automatic differentation. \n",
        "\n",
        "First, we create and visualize a two-dimensional dataset where each point is labeled as positive (1) or negative (0). As seen below, the positive and negative points are not linearly separable.\n"
      ],
      "metadata": {
        "id": "RDzicQ7MFmDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(305)\n",
        "\n",
        "def make_dataset(num_points):\n",
        "    radius = 5\n",
        "\n",
        "    def sample_annulus(inner_radius, outer_radius, num_points):\n",
        "        r = Uniform(inner_radius, outer_radius).sample((num_points,))\n",
        "        angle = Uniform(0, 2 * math.pi).sample((num_points,))\n",
        "        x = r * torch.cos(angle)\n",
        "        y = r * torch.sin(angle)\n",
        "        data = torch.vstack([x, y]).T\n",
        "        return data\n",
        "    \n",
        "    # Generate positive examples (labeled 1)\n",
        "    data_1 = sample_annulus(0, 0.5 * radius, num_points // 2)\n",
        "    labels_1 = torch.ones(num_points // 2)\n",
        "        \n",
        "    # Generate negative examples (labeled 0).\n",
        "    data_0 = sample_annulus(0.7 * radius, radius, num_points // 2)\n",
        "    labels_0 = torch.zeros(num_points // 2)\n",
        "        \n",
        "    data = torch.vstack([data_0, data_1])\n",
        "    labels = torch.concat([labels_0, labels_1])\n",
        "\n",
        "    return data, labels\n",
        "    \n",
        "num_data = 500\n",
        "data, labels = make_dataset(num_data)\n",
        "\n",
        "# Note: red indicates a label of 1, blue indicates a label of 0\n",
        "plt.scatter(data[:num_data//2, 0], data[:num_data//2, 1], color='red') \n",
        "plt.scatter(data[num_data//2:, 0], data[num_data//2:, 1], color='blue') "
      ],
      "metadata": {
        "id": "9inQCD8vHSQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2a: The Maximum Likelihood Objective\n",
        "\n",
        "We will try to classify this data using a neural network. We posit the following statistical model for the labels $y \\in \\{0, 1\\}$ given features $\\mathbf{x} \\in \\mathbb{R}^2$:\n",
        "$$ y \\mid x \\sim \\text{Bern}(\\sigma(\\text{NN}_{\\boldsymbol{\\theta}}(\\mathbf{x})))$$\n",
        "Here, $\\text{NN}_{\\boldsymbol{\\theta}}: \\mathbb{R}^2 \\to \\mathbb{R}$ denotes a neural network with parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^P$ mapping datapoints $\\mathbf{x}$ to $\\text{logit}(\\mathbb{P}(y = 1 \\mid \\mathbf{x}))$. Recall the [logit](https://en.wikipedia.org/wiki/Logit) function is given by \n",
        "$$\\text{logit}(p) = \\log \\left( \\frac{p}{1-p} \\right)$$\n",
        "and its inverse is the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function): $$\\sigma(x) = \\frac{1}{1 + \\exp (-x)}$$\n",
        "\n",
        "We estimate the parameters $\\boldsymbol{\\theta}$ using maximum likelihood. Show that for a dataset $\\{(\\mathbf{x}_n, y_n)\\}_{n=1}^N$ the negative log-likelihood objective, rescaled by the number of datapoints $N$, may be written as:\n",
        "\n",
        "$$ L(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{n = 1}^N -y_n \\log \\sigma(\\text{NN}_{\\boldsymbol{\\theta}}(\\mathbf{x})) - (1 - y_n) \\log (1 - \n",
        " \\sigma(\\text{NN}_{\\boldsymbol{\\theta}}(\\mathbf{x}))) = \\frac{1}{N} \\sum_{n = 1}^N \\ell(y, \\text{NN}_{\\boldsymbol{\\theta}}(\\mathbf{x}_n))$$\n",
        "\n",
        "where $\\ell(y, x) = -y \\log \\sigma(x) - (1 - y) \\log(1 - \\sigma(x))$."
      ],
      "metadata": {
        "id": "TGPznxcUzIZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "*Your answer here.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "PU11hWQH5IKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2b: Define the Neural Network\n",
        "\n",
        "We will use a neural network with two hidden layers, the first of which has three hidden units and the second of which has five hidden units. The equations defining the output $z \\in \\mathbb{R}$ of our neural network given an input $\\mathbf{x} \\in \\mathbb{R}^2$ are:\n",
        "\\begin{align*}\n",
        "\\mathbf{h}_1 &= \\text{ReLU}(\\mathbf{W_1} \\mathbf{x} + \\mathbf{b_1}) \\\\\n",
        "\\mathbf{h}_2 &= \\text{ReLU}(\\mathbf{W_2} \\mathbf{h}_1 + \\mathbf{b_2}) \\\\\n",
        "z &= \\mathbf{w}_3^\\top \\mathbf{h}_2 + b_3\n",
        "\\end{align*}\n",
        "The parameters of the network are $\\boldsymbol{\\theta} = (\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, \\mathbf{b}_2, \\mathbf{w}_3, b_3)$ where $\\mathbf{W}_1 \\in \\mathbb{R}^{3 \\times 2}, \\mathbf{b}_1 \\in \\mathbb{R}^3, \\mathbf{W}_2 \\in \\mathbb{R}^{5 \\times 3}, \\mathbf{b}_2 \\in \\mathbb{R}^5, \\mathbf{w}_3 \\in \\mathbb{R}^5$, and $b_3 \\in \\mathbb{R}$. \n",
        "\n",
        "Implement this network as a PyTorch `nn.module`."
      ],
      "metadata": {
        "id": "VKr2P3bG5K04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ###\n",
        "        # YOUR CODE HERE\n",
        "        ##\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Implements the forward pass of the network.\n",
        "\n",
        "        Args:\n",
        "            x: torch.tensor of shape (N, 2)\n",
        "        Returns:\n",
        "            logits: torch.tensor of shape (N,) containing the logits\n",
        "        \"\"\"\n",
        "        ###\n",
        "        # YOUR CODE HERE\n",
        "        logits = ...\n",
        "        ##\n",
        "        return logits"
      ],
      "metadata": {
        "id": "sVbjj3UH7bQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the predictions of an untrained network. As we can see, the network does not succeed at classifying the points without training\n",
        "\n"
      ],
      "metadata": {
        "id": "SJFhBXvM8onn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions(net):\n",
        "    num_points = 200\n",
        "    x1s = np.linspace(-6.0, 6.0, num_points)\n",
        "    x2s = np.linspace(-6.0, 6.0, num_points)\n",
        "    X1, X2 = np.meshgrid(x1s, x2s)\n",
        "\n",
        "    points = np.transpose([np.tile(x1s, len(x2s)), np.repeat(x2s, len(x1s))])\n",
        "    points = torch.tensor(points, dtype=torch.float)\n",
        "    with torch.no_grad():\n",
        "        probs = torch.sigmoid(net(points)).reshape(num_points, num_points)\n",
        "\n",
        "    plt.pcolormesh(X1, X2, probs, cmap=plt.cm.get_cmap('YlGn'), vmin=0, vmax=1)\n",
        "    plt.colorbar()\n",
        "    plt.scatter(data[:num_data//2, 0], data[:num_data//2, 1], color='red') \n",
        "    plt.scatter(data[num_data//2:, 0], data[num_data//2:, 1], color='blue') \n",
        "    plt.title(\"Output Probabilities\")\n",
        "\n",
        "torch.manual_seed(305)\n",
        "model = SimpleNet()\n",
        "visualize_predictions(model)"
      ],
      "metadata": {
        "id": "SA8Xsixv8yyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2c: Train the Network\n",
        "\n",
        "We will now find the parameters of our network by maximizing the likelihood, or equivalently minimizing the negative log-likelihood. We will use full-batch gradient descent. That is, we will use the gradient $\\nabla L(\\boldsymbol{\\theta})$ itself to update the parameters rather than a stochastic estimate of $\\nabla L(\\boldsymbol{\\theta})$.\n",
        "\n",
        "Use the SGD optimizer from `torch.optim` with a learning rate of $1$ and no momentum for 1000 iterations. Note that the function $\\ell$ from above is implemented in PyTorch as [`nn.BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)."
      ],
      "metadata": {
        "id": "3P415LgJ8dld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 1000\n",
        "losses = []\n",
        "\n",
        "###\n",
        "# YOUR CODE HERE\n",
        "loss_fn = ...\n",
        "optimizer = ...\n",
        "for it in tqdm(range(num_steps)):\n",
        "    ...\n",
        "    loss = ...\n",
        "    ...\n",
        "    losses.append(loss.item())\n",
        "##"
      ],
      "metadata": {
        "id": "UjtqMoDp8ZFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Gradient Descent Iteration\")\n",
        "plt.ylabel(\"Objective Value\")"
      ],
      "metadata": {
        "id": "3Za_MUL6f5Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the predictions of our trained network. We see that the network has learned to separate the positive and negative examples."
      ],
      "metadata": {
        "id": "h-m3AZ1XkWBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_predictions(model)"
      ],
      "metadata": {
        "id": "mSkaBFYKgLlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3: Amortized Variational Inference\n",
        "\n",
        "In this problem, we will train a variational autoencoder for the MNIST dataset of handwritten digits. First, let's download this dataset using PyTorch's `datasets` module and visualize some of the digits. We will use a binarized version of the dataset in which each pixel value is either 0 or 1."
      ],
      "metadata": {
        "id": "IKp_lRGVgtJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download MNIST dataset and create dataloaders. \n",
        "def binarize(imgs, integer=False):\n",
        "    threshold = 127 if integer else 0.5\n",
        "    imgs = imgs.clone()\n",
        "    imgs[imgs < threshold] = 0.\n",
        "    imgs[imgs >= threshold] = 1.\n",
        "    return imgs\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"data\", train=True, download=True,\n",
        "                         transform=transforms.ToTensor())\n",
        "\n",
        "test_dataset = datasets.MNIST(root='data', train=False, download=True,\n",
        "                             transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(\"Number of points in dataset: {0}\".format(train_dataset.data.shape[0]))\n",
        "print(\"Number of batches per epoch: {0}\".format(len(train_loader)))"
      ],
      "metadata": {
        "id": "dUdWNkjXZYvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize some digits in the dataset.\n",
        "imgs, _ = next(iter(train_loader))\n",
        "imgs = binarize(imgs)\n",
        "fig, ax = plt.subplots(1, 6, figsize=(14, 14))\n",
        "fig.tight_layout()\n",
        "for i, ax in enumerate(ax.flat):\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.imshow(imgs[i].squeeze(), alpha=0.8, cmap='gray')"
      ],
      "metadata": {
        "id": "jtPVlqxvbMco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3a: Decoder Network\n",
        "\n",
        "We represent a $28 \\times 28$ image $\\mathbf{x}$ as a flattened $784$ dimensional vector of binary values, i.e. $\\mathbf{x} \\in \\{0, 1\\}^{784}$. We specify our generative model as:\n",
        "$$ \\mathbf{z} \\sim \\mathcal{N}(0, I), \\quad \\mathbf{x} \\mid \\mathbf{z} \\sim \\text{Bern}(\\sigma(D_{\\boldsymbol{\\theta}}(\\mathbf{z}))$$\n",
        "Here, $D_{\\boldsymbol{\\theta}}: \\mathbb{R}^2 \\to \\mathbb{R}^{784}$ is a neural network with parameters $\\boldsymbol{\\theta}$ and $\\mathbf{z} \\in \\mathbb{R}^{2}$ is a two-dimensional latent variable. We use only two dimensions so that the latent space can be easily visualized later, but using a higher dimensional latent variable would give a more flexible generative model. \n",
        "\n",
        "We will parametrize $D_{\\boldsymbol{\\theta}}$ as a fully connected neural network with two hidden layers and ReLU activations. We use 256 units in the first hidden layer and 512 in the second. Note that as in Problem 2, the network maps to the logits of the Bernoulli distribution and not the probabilities themselves. Implement this decoder network in PyTorch below."
      ],
      "metadata": {
        "id": "Gz0h_qvdkl47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define decoder architecture\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\" Neural network defining p(x | z) \"\"\"\n",
        "\n",
        "    def __init__(self, data_dim, latent_dim, hidden_dims=[256, 512]):\n",
        "        super().__init__()\n",
        "        self.data_dim = data_dim\n",
        "\n",
        "        ###\n",
        "        # YOUR CODE HERE\n",
        "        ## \n",
        "        \n",
        "    def forward(self, z):\n",
        "        \"\"\" Returns Bernoulli conditional distribution of p(x | z), parametrized\n",
        "        by logits.\n",
        "        Args:\n",
        "            z: (N, latent_dim) torch.tensor\n",
        "        Returns:\n",
        "            Bernoulli distribution with a batch of (N, data_dim) logits\n",
        "        \"\"\"\n",
        "        ###\n",
        "        # YOUR CODE HERE\n",
        "        ## \n",
        "        return Bernoulli(...)"
      ],
      "metadata": {
        "id": "YFih1GzaZcaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3b: Encoder Network\n",
        "\n",
        "We will estimate the parameters of the generative model by maximizing the Evidence Lower Bound (ELBO). As the exact posterior $p(\\mathbf{z} \\mid \\mathbf{x})$ is unknown, we will use an approximate, amortized posterior $q_{\\boldsymbol{\\phi}}(\\mathbf{z} \\mid \\mathbf{x}) = \\mathcal{N}(\\mathbf{z} \\mid \\mu_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\text{diag}(\\sigma^2_{\\boldsymbol{\\phi}}(\\mathbf{x})))$. We let $\\left(\\mu_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\log \\sigma^2_{\\boldsymbol{\\phi}}(\\mathbf{x}) \\right) = E_{\\boldsymbol{\\phi}}(\\mathbf{x})$ where $E_{\\boldsymbol{\\phi}}: \\mathbb{R}^{784} \\to \\mathbb{R}^2 \\times \\mathbb{R}^2$ is a neural network with parameters $\\boldsymbol{\\phi}$. \n",
        "\n",
        "As above, we parametrize $E_{\\boldsymbol{\\phi}}$ as a neural network with two layers of hidden units and ReLU activations. We use 512 hidden units in the first layer and 256 in the second. Then we let $\\mu_{\\boldsymbol{\\phi}}$ and $\\log \\sigma^2_{\\boldsymbol{\\phi}}$ be affine functions of the hidden layer activations. Implement the encoder $E_{\\boldsymbol{\\phi}}$ in the code below."
      ],
      "metadata": {
        "id": "JFLYAq5xmwjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define encoder architecture\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\" Neural network defining q(z | x). \"\"\"\n",
        "\n",
        "    def __init__(self, data_dim, latent_dim, hidden_dims=[512, 256]):\n",
        "        super().__init__()\n",
        "\n",
        "        ###\n",
        "        # YOUR CODE HERE\n",
        "        ##\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Returns Normal conditional distribution for q(z | x), with mean and\n",
        "        log-variance output by a neural network.\n",
        "\n",
        "        Args:\n",
        "            x: (N, data_dim) torch.tensor\n",
        "        Returns:\n",
        "            Normal distribution with a batch of (N, latent_dim) means and standard deviations\n",
        "        \"\"\"\n",
        "        ###\n",
        "        # YOUR CODE HERE\n",
        "        ##\n",
        "        return Normal(...)"
      ],
      "metadata": {
        "id": "vVPRAfWhZemu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ELBO Derivation [given]\n",
        "\n",
        "As a function of $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\phi}$, we can write the ELBO for a single datapoint $\\mathbf{x}$ as:\n",
        "$$\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\theta}, \\boldsymbol{\\phi}) = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z} \\mid \\mathbf{x})} \\left[ \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}, \\mathbf{z}) - \\log q_{\\boldsymbol{\\phi}}(\\mathbf{z} \\mid \\mathbf{x}) \\right]$$\n",
        "\n",
        "We can obtain a lower bound of the log-likelihood for an entire dataset $\\{\\mathbf{x}^{(n)} \\}_{n=1}^N$, rescaled by the number of datapoints $N$, as:\n",
        "$$\\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}) = \\frac{1}{N} \\sum_{n=1}^N \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z}^{(n)} \\mid \\mathbf{x}^{(n)})} \\left[ \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(n)}, \\mathbf{z}^{(n)}) - \\log q_{\\boldsymbol{\\phi}}(\\mathbf{z}^{(n)} \\mid \\mathbf{x}^{(n)}) \\right]$$\n",
        "\n",
        "We can rewrite the per-datapoint ELBO as:\n",
        "\\begin{align*}\n",
        "\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\theta}, \\boldsymbol{\\phi}) \n",
        "&= \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z} \\mid \\mathbf{x})} \\left[ \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x} \\mid \\mathbf{z}) \\right] - \\text{KL}\\left( q_{\\boldsymbol{\\phi}}(\\mathbf{z} \\mid \\mathbf{x}) \\mid\\mid p(\\mathbf{z})\\right) \\\\\n",
        "&= \\mathbb{E}_{\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})} \\left[ \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x} \\mid \\mu_{\\boldsymbol{\\phi}}(\\mathbf{x}) + \\boldsymbol{\\epsilon} \\odot \\sigma_{\\boldsymbol{\\phi}}(\\mathbf{x})) \\right] - \\text{KL}\\left( q_{\\boldsymbol{\\phi}}(\\mathbf{z} \\mid \\mathbf{x}) \\mid\\mid p(\\mathbf{z})\\right)\n",
        "\\end{align*}\n",
        "\n",
        "This allows us to obtain an unbiased estimate of the per-datapoint ELBO by first sampling $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$, then computing:\n",
        "$$\\hat{\\mathcal{L}}(\\mathbf{x}, \\boldsymbol{\\theta}, \\boldsymbol{\\phi}) = \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x} \\mid \\mu_{\\boldsymbol{\\phi}}(\\mathbf{x}) + \\boldsymbol{\\epsilon} \\odot \\sigma_{\\boldsymbol{\\phi}}(\\mathbf{x})) - \\text{KL}\\left( q_{\\boldsymbol{\\phi}}(\\mathbf{z} \\mid \\mathbf{x}) \\mid\\mid p(\\mathbf{z})\\right)$$\n",
        "\n",
        "This is known as the reparametrization trick, and it will allow us to straightforwardly use automatic differentiation to obtain the gradient of $\\hat{\\mathcal{L}}$ with respect to $\\boldsymbol{\\phi}$. \n",
        "\n",
        "Given a minibatch $\\{\\mathbf{x}^{(b)} \\}_{b=1}^B$ sampled uniformly from the entire dataset, we can simulate independent normal variates $\\boldsymbol{\\epsilon}^{(b)}$ to form an unbiased estimator of the ELBO for the entire dataset:\n",
        "$$\\hat{\\mathcal{L}}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}) = \\frac{1}{B} \\sum_{b = 1}^B \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(b)} \\mid \\mu_{\\boldsymbol{\\phi}}(\\mathbf{x}^{(b)}) + \\boldsymbol{\\epsilon} \\odot \\sigma_{\\boldsymbol{\\phi}}(\\mathbf{x}^{(b)})) - \\text{KL}\\left( q_{\\boldsymbol{\\phi}}(\\mathbf{z}^{(b)} \\mid \\mathbf{x}^{(b)}) \\mid\\mid p(\\mathbf{z}^{(b)})\\right) $$"
      ],
      "metadata": {
        "id": "ALGAlabhpyjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3c: Implement the ELBO\n",
        "\n",
        "Using our derivations above, implement the estimator of the ELBO $\\hat{\\mathcal{L}}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi})$. We assume sampling of the minibatch `x` is done outside of the function, but you must sample the noise variables $\\boldsymbol{\\epsilon}$ within the `elbo` function. You should use the `kl_divergence` function imported above to analytically compute the KL divergence between the Gaussian distributions $q_{\\boldsymbol{\\phi}}(\\mathbf{z} \\mid \\mathbf{x})$ and $p(\\mathbf{z})$. Make sure you use `rsample` on a `Distribution` object to use the reparametrization trick and not `sample`."
      ],
      "metadata": {
        "id": "l9MxjEL_vEyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def elbo(x, encoder, decoder):\n",
        "    \"\"\" Computes a stochastic estimate of the rescaled evidence lower bound\n",
        "\n",
        "    Args:\n",
        "        x: (N, data_dim) torch.tensor\n",
        "        encoder: an Encoder\n",
        "        decoder: a Decoder\n",
        "    Returns:\n",
        "        elbo: a (,) torch.tensor containing the estimate of the ELBO\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # YOUR CODE HERE\n",
        "    elbo = ...\n",
        "    ##\n",
        "    return elbo\n"
      ],
      "metadata": {
        "id": "oD8-PqawoVVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the Training Loop [given]\n",
        "\n",
        "Using our `Encoder` and `Decoder` definitions, as well as the `elbo` function, we have provided training code below. This code uses the [Adam](https://arxiv.org/abs/1412.6980) optimizer, a sophisticated optimization algorithm which uses the history of past gradients to rescale gradients before applying an update.\n",
        "\n",
        "We train for 20 epochs (an \"epoch\" refers to a complete pass through the dataset). Our implementation takes 10 minutes to run and achieves a training ELBO of $-135$ and a test ELBO of $-138$."
      ],
      "metadata": {
        "id": "VWImC7SvwALR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(data_dim=784, latent_dim=2)\n",
        "decoder = Decoder(data_dim=784, latent_dim=2)\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),\n",
        "                       lr=3e-4)\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    train_elbo = 0\n",
        "    for batch_idx, (x, _) in enumerate(train_loader):\n",
        "        x = binarize(x.reshape(x.shape[0], -1))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = -elbo(x, encoder, decoder)  \n",
        "        loss.backward()\n",
        "        train_elbo -= loss.item() * len(x)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tELBO: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(x), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), -loss.item()))\n",
        "            \n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    test_elbo = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in test_loader:\n",
        "            x = binarize(x.reshape(x.shape[0], -1))#.cuda()\n",
        "            test_elbo += elbo(x, encoder, decoder).item() * len(x)\n",
        "            \n",
        "    train_elbo /= len(train_loader.dataset)\n",
        "    test_elbo /= len(test_loader.dataset)\n",
        "    \n",
        "    print('====> Epoch: {} Average ELBO: {:.4f} Test ELBO: {:.4f}'.format(epoch,\n",
        "                                                                          train_elbo,\n",
        "                                                                          test_elbo))\n"
      ],
      "metadata": {
        "id": "TQixwXfsZiby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3e: Visualize samples from the trained model\n",
        "\n",
        "In addition to the ELBO, we can sample from the trained model to assess its performance. Use the code below to generate an $8 \\times 8$ grid of sampled digits from the model. Note that we follow the common practice of using the mean of $p_{\\boldsymbol{\\theta}}(\\mathbf{x} \\mid \\mathbf{z})$ rather than resampling from this distribution when visualizing samples. Critique these samples. What aspects of the data distribution does the model seem to have trouble learning?"
      ],
      "metadata": {
        "id": "hIbwsU3p3fQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sampled digits from our model\n",
        "decoder.eval()\n",
        "\n",
        "num_samples = 64\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(num_samples, 2)\n",
        "    expected_xs = decoder.forward(z).mean \n",
        "    expected_xs = expected_xs.reshape(-1, 28, 28).unsqueeze(1)\n",
        "\n",
        "# Plot the expected_xs as a grid of images\n",
        "expected_xs_grid = make_grid(expected_xs, nrow=8)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.axis('off')\n",
        "plt.imshow(expected_xs_grid.permute(1, 2, 0), vmin=0., vmax=1.)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wr7oSJPttGQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "*Your answer here.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Na2dd1eM4w6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3f: Visualize the Latent Embeddings\n",
        "\n",
        "Given $\\mathbf{x}$, we can interpret the mean of the approximate posterior $\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z} \\mid \\mathbf{x})}[\\mathbf{z}]$ as a lower dimensional representation of $\\mathbf{x}$. In the code below, we find the mean of the approximate posterior for each of the datapoints in the dataset and then visualize these means with a scatter plot. We color each point according to the label of the encoded digit. What do you notice? Are there classes with significant overlap, and are these classes which are visually similar? Is there a class which has clear separation from the others, and if so, why do you think this is? \n",
        "\n",
        "Note that we did not provide any information about the class label to either the generative model or the approximate posterior!"
      ],
      "metadata": {
        "id": "E5wovnHc8nzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the mean of the latents given the data\n",
        "encoder.eval()\n",
        "with torch.no_grad():\n",
        "    means = []\n",
        "    ys = []\n",
        "    for x, y in train_loader:        \n",
        "        x = binarize(x.reshape(x.shape[0], -1))\n",
        "        mean = encoder.forward(x).mean\n",
        "        means.append(mean)\n",
        "        ys.append(y)\n",
        "\n",
        "means = torch.vstack(means)\n",
        "ys = torch.hstack(ys)\n",
        "\n",
        "# Plot the first two dimensions of the latents\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "for i in range(10):\n",
        "    means_i = means[ys == i]\n",
        "    ax.scatter(means_i[:, 0], means_i[:, 1], label=str(i))\n",
        "\n",
        "ax.set_xlabel('$z_1$')\n",
        "ax.set_ylabel('$z_2$')\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "C3-bFZcRMS1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "*Your answer here.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4QJCBTyz-Hyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3g: Interpolation in the Latent Space\n",
        "\n",
        "Another desideratum for a latent variable model is smooth interpolation in the latent space. For example, if we linearly interpolate between a latent $\\mathbf{z}_{start}$ corresponding to a $7$ and a latent $\\mathbf{z}_{end}$ corresponding to a $1$, we should observe the decodings of the interpolations smoothly change from a $7$ to a $1$.\n",
        "\n",
        "In the code below, we sample $8$ different starting latent variables and $8$ different ending latent variables from the prior, linearly interpolate between them for $10$ points, then plot the decodings. Does our model smoothly change between decoded digits? Are there digit pairs it was more successful interpolating between?"
      ],
      "metadata": {
        "id": "fDjhJQzCDPIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpolate between 8 randomly chosen start and end points\n",
        "latent_starts = torch.randn(8, 2)\n",
        "latent_ends = torch.randn(8, 2)\n",
        "\n",
        "means = []\n",
        "for t in torch.linspace(0, 1, 10):\n",
        "    z = latent_starts + t * (latent_ends - latent_starts)\n",
        "    with torch.no_grad():\n",
        "        means.append(decoder.forward(z).mean.reshape(-1, 28, 28).unsqueeze(0))\n",
        "\n",
        "means_tensor = torch.vstack(means).permute(1, 0, 2, 3).reshape(-1, 28, 28).unsqueeze(1)\n",
        "sample_grid = make_grid(means_tensor, nrow=10)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.axis('off')\n",
        "plt.imshow(sample_grid.permute(1, 2, 0).cpu(), vmin=0., vmax=1.)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IAuVI8r2PQPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "*Your answer here.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vhfENy73CEjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4: Reflections"
      ],
      "metadata": {
        "id": "7GNUuqpkCKD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 4a\n",
        "\n",
        "Discuss one reason why we use amortized variational inference rather than optimizing per-datapoint latent variables $\\mu^{(n)}, (\\boldsymbol{\\sigma}^2)^{(n)}$ (so $p(\\mathbf{z^{(n)}} \\mid \\mathbf{x}^{(n)})= \\mathcal{N}(\\mathbf{z}^{(n)} \\mid \\mu^{(n)}, \\text{diag}((\\boldsymbol{\\sigma}^2)^{(n)})$)."
      ],
      "metadata": {
        "id": "RhJv4QlZCLds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "*Your answer here.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4zI7iSsUEJJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 4b\n",
        "\n",
        "Describe one way you could improve the variational autoencoder, either by changing the encoder or decoder network structure or by changing the  model itself, and why you think your proposed change would help.\n"
      ],
      "metadata": {
        "id": "xyFuwFYgC73l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "*Your answer here.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gWRfKdEyELki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 4c\n",
        "\n",
        "Suppose rather than using a Gaussian prior on $\\mathbf{z}$, we used $\\mathbf{z} \\overset{ind}{\\sim} \\text{Bern}(0.5)$. We can modify $E_{\\boldsymbol{\\phi}}$ to output logits for a multivariate Bernoulli distribution: $p(\\mathbf{z} \\mid \\mathbf{x}; \\boldsymbol{\\phi}) = \\text{Bern}(\\mathbf{z}; \\sigma(E_{\\boldsymbol{\\phi}}(\\mathbf{x})))$. Where would our optimization procedure break down in this case?"
      ],
      "metadata": {
        "id": "C1vdsrZ-DrnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "*Your answer here.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "MY_ES4kiENoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission Instructions\n",
        "\n",
        "\n",
        "**Formatting:** check that your code does not exceed 80 characters in line width. You can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
        "\n",
        "Download your notebook in .ipynb format and remove the Open in Colab button.  Then run the following command to convert to a PDF:\n",
        "```\n",
        "jupyter nbconvert --to pdf <yourname>_hw6.ipynb\n",
        "```\n",
        "\n",
        "\n",
        "**Installing nbconvert:**\n",
        "\n",
        "If you're using Anaconda for package management, \n",
        "```\n",
        "conda install -c anaconda nbconvert\n",
        "```\n",
        "\n",
        "**Upload** your .pdf files to Gradescope. "
      ],
      "metadata": {
        "id": "8RQzBPoTRgEV"
      }
    }
  ]
}