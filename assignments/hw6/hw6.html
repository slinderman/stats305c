

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>HW6: Neural Networks and VAEs &#8212; Applied Statistics III</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'assignments/hw6/hw6';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="HW7: Autoregressive HMMs" href="../hw7/hw7.html" />
    <link rel="prev" title="HW5: Poisson Matrix Factorization" href="../hw5/hw5.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    <p class="title logo__title">Applied Statistics III</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Assignments</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../hw0/hw0.html">HW0: PyTorch Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hw1/hw1.html">HW1: Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hw2/hw2.html">HW2: Gibbs Sampling and Metropolis-Hastings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hw3/hw3.html">HW3: Continuous Latent Variable Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hw4/hw4.html">HW4: Bayesian Mixture Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hw5/hw5.html">HW5: Poisson Matrix Factorization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">HW6: Neural Networks and VAEs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hw7/hw7.html">HW7: Autoregressive HMMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hw8/hw8.html">HW 8: Sigmoidal Gaussian Cox Processes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Notebooks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/01_bayes_normal.html">Bayesian Analysis of the Normal Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/02_mvn.html">The Multivariate Normal Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/03_hier_gauss.html">Hierarchical Gaussian Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/04_mcmc.html">Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/09_cavi_gmm.html">Coordinate Ascent Variational Inference for GMMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/09_cavi_nix.html">CAVI in a Simple Gaussian Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/10_cavi_lda.html">Latent Dirichlet Allocation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/11_advi_nix.html">Gradient-based VI in a Simple Gaussian Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/12_nns_vaes.html">Neural Networks and VAEs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/15_gps.html">Gaussian Processes</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../lectures/99_references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/slinderman/stats305c/blob/spring2023/assignments/hw6/hw6.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/assignments/hw6/hw6.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>HW6: Neural Networks and VAEs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1-optimizing-a-quadratic-objective-via-gradient-descent">Problem 1: Optimizing a Quadratic Objective via Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1a-gradient-descent-with-a-well-conditioned-objective">Problem 1a: Gradient Descent with a well-conditioned Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1b-gradient-descent-with-a-ill-conditioned-objective">Problem 1b: Gradient Descent with a Ill-Conditioned Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1c-optimizing-a-high-dimensional-objective">Problem 1c: Optimizing a High-Dimensional Objective</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2-neural-network-classification">Problem 2: Neural Network Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2a-the-maximum-likelihood-objective">Problem 2a: The Maximum Likelihood Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2b-define-the-neural-network">Problem 2b: Define the Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2c-train-the-network">Problem 2c: Train the Network</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3-amortized-variational-inference">Problem 3: Amortized Variational Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3a-decoder-network">Problem 3a: Decoder Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3b-encoder-network">Problem 3b: Encoder Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elbo-derivation-given">ELBO Derivation [given]</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3c-implement-the-elbo">Problem 3c: Implement the ELBO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-the-training-loop-given">Implement the Training Loop [given]</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3d-visualize-samples-from-the-trained-model">Problem 3d: Visualize samples from the trained model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3e-visualize-the-latent-embeddings">Problem 3e: Visualize the Latent Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3f-interpolation-in-the-latent-space">Problem 3f: Interpolation in the Latent Space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-4-reflections">Problem 4: Reflections</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-4a">Problem 4a</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-4b">Problem 4b</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-4c">Problem 4c</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#submission-instructions">Submission Instructions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="hw6-neural-networks-and-vaes">
<h1>HW6: Neural Networks and VAEs<a class="headerlink" href="#hw6-neural-networks-and-vaes" title="Permalink to this heading">#</a></h1>
<hr class="docutils" />
<p><strong>Name:</strong></p>
<p><strong>Collaborators:</strong></p>
<hr class="docutils" />
<p>In this homework assignment, we will explore automatic differentiation, neural networks, and amortized variational inference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">Bernoulli</span><span class="p">,</span> <span class="n">Uniform</span>
<span class="kn">from</span> <span class="nn">torch.distributions.kl</span> <span class="kn">import</span> <span class="n">kl_divergence</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">make_grid</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">305</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.cm</span> <span class="kn">import</span> <span class="n">Blues</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;notebook&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="problem-1-optimizing-a-quadratic-objective-via-gradient-descent">
<h2>Problem 1: Optimizing a Quadratic Objective via Gradient Descent<a class="headerlink" href="#problem-1-optimizing-a-quadratic-objective-via-gradient-descent" title="Permalink to this heading">#</a></h2>
<p>We’ll start off by optimizing a simple objective using gradient descent. We will compute the required gradients using PyTorch’s automatic differentation capabilities.</p>
<p>Consider the function <span class="math notranslate nohighlight">\(f: \mathbb{R}^D \to \mathbb{R}\)</span> given by:</p>
<div class="math notranslate nohighlight">
\[ f(\mathbf{x}) = \mathbf{x}^\top \mathbf{A} \mathbf{x} \]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{D \times D}\)</span> is a fixed positive definite matrix. It is obvious that a global minimizer of <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(\mathbf{x}^* = 0\)</span>. We will try to recover this known global minimizer using gradient descent. We note that although the objective is seemingly simple, a stochastic version of this objective has been used as a model for neural network loss surfaces (e.g. see <a class="reference external" href="https://arxiv.org/abs/1803.02021">this paper</a>).</p>
<p>We will simplify the objective further by assuming that <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is diagonal, so <span class="math notranslate nohighlight">\(\mathbf{A} = \text{diag}(\mathbf{a})\)</span> for some <span class="math notranslate nohighlight">\(\mathbf{a} \in \mathbb{R}_{++}^D\)</span>.</p>
<section id="problem-1a-gradient-descent-with-a-well-conditioned-objective">
<h3>Problem 1a: Gradient Descent with a well-conditioned Objective<a class="headerlink" href="#problem-1a-gradient-descent-with-a-well-conditioned-objective" title="Permalink to this heading">#</a></h3>
<p>Recall the gradient descent update rule for a fixed step size <span class="math notranslate nohighlight">\(\alpha\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}^{(k + 1)} \gets \mathbf{x}^{(k)} - \alpha \nabla f(\mathbf{x}^{(k)})\]</div>
<p>Using PyTorch’s automatic differentiation system, complete the function below implementing an optimization loop to minimize the objective.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Evaluates the objective.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: (D,) tensor </span>
<span class="sd">        a: (D,) tensor of positive values</span>
<span class="sd">    Returns:</span>
<span class="sd">        value: (,) tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">run_optimization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Runs an optimization algorithm on the objective for num_iters.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Starting position for optimization. nn.Parameter of shape (D,)</span>
<span class="sd">        a: parameter defining curvature of objective. tensor of shape (D,)</span>
<span class="sd">        optimizer: a torch.optim Optimizer</span>
<span class="sd">        num_iters: number of iterations to run optimization for</span>

<span class="sd">    Returns:</span>
<span class="sd">        xs: value of x at each iteration. tensor of shape (num_iters + 1, D)</span>
<span class="sd">        losses: value of objective at each iterate. tensor of shape (num_iters + 1,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># we have to make a copy of the data so that elements of list do not </span>
    <span class="c1"># reference same location in memory</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()]</span>

    <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>

        <span class="c1">###</span>
        <span class="c1"># YOUR CODE HERE</span>
        <span class="c1"># 1. zero out the gradient</span>
        <span class="c1"># 2. compute the loss</span>
        <span class="c1"># 3. compute the gradient of the loss</span>
        <span class="c1"># 4. take one optimizer step (this will update x in place)</span>
        <span class="o">...</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
        <span class="o">...</span>
        <span class="c1">##</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span> 

    <span class="c1"># Return the stacked losses and xs</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">xs</span><span class="p">,</span> <span class="n">losses</span>
</pre></div>
</div>
</div>
</div>
<p>We will now run gradient descent on the objective in <span class="math notranslate nohighlight">\(D = 2\)</span> dimensions, with <span class="math notranslate nohighlight">\(\mathbf{a} = (0.2, 0.2)\)</span>. We will use 50 iterations with a learning rate of <span class="math notranslate nohighlight">\(\alpha = 0.8\)</span>, starting from <span class="math notranslate nohighlight">\(\mathbf{x}_0 = (10, 10)\)</span>.</p>
<p>Note that we initialize <code class="docutils literal notranslate"><span class="pre">x</span></code> using <code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code>, which tells PyTorch we need to compute gradients with respect to <code class="docutils literal notranslate"><span class="pre">x</span></code>. Note that PyTorch’s optimizers expect an iterable of <code class="docutils literal notranslate"><span class="pre">Parameters</span></code> to be passed as the first argument, so you must pass in <code class="docutils literal notranslate"><span class="pre">[x]</span></code> rather than <code class="docutils literal notranslate"><span class="pre">x</span></code> itself.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">D</span><span class="p">))</span>
<span class="n">a</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>

<span class="c1">### </span>
<span class="c1"># YOUR CODE HERE</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="n">run_optimization</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="c1">##</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s plot the loss curve. As you can see, the objective approaches the optimal value of zero quite quickly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Gradient Descent Iteration&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Objective Value&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next, let’s visualize the trajectory of the gradient descent iterates <span class="math notranslate nohighlight">\(\mathbf{x}^{(k)}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">visualize_gradient_descent</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Visualizes gradient descent when iterates are two-dimensional.&quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="nf">batch_f</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args: </span>
<span class="sd">            X: (N, D) tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">grid_size</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="n">x1s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">grid_size</span><span class="p">)</span>
    <span class="n">x2s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">grid_size</span><span class="p">)</span>
    <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="n">x2s</span><span class="p">)</span>
    <span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2s</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">x2s</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1s</span><span class="p">))])</span>
    <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

    <span class="n">Z</span> <span class="o">=</span> <span class="n">batch_f</span><span class="p">(</span><span class="n">points</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">grid_size</span><span class="p">,</span> <span class="n">grid_size</span><span class="p">)</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
    <span class="n">contours</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">xs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">xs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Cost function&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_gradient_descent</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="problem-1b-gradient-descent-with-a-ill-conditioned-objective">
<h3>Problem 1b: Gradient Descent with a Ill-Conditioned Objective<a class="headerlink" href="#problem-1b-gradient-descent-with-a-ill-conditioned-objective" title="Permalink to this heading">#</a></h3>
<p>Next, let’s see how gradient descent performs on an ill-conditioned objective, where the amount of curvature in each direction varies. For our objective, we can control the amount of curvature using <span class="math notranslate nohighlight">\(\mathbf{a}\)</span>. Repeat part (a), this time using <span class="math notranslate nohighlight">\(\mathbf{a} = (0.05, 1.2)\)</span> while holding all other hyperparameters constant.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">D</span><span class="p">))</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>

<span class="c1">###</span>
<span class="c1"># YOUR CODE HERE</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="n">run_optimization</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="c1">##</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Gradient Descent Iteration&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Objective Value&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_gradient_descent</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Explain why the gradient descent iterates oscillate around the line <span class="math notranslate nohighlight">\(\mathbf{x}_2 = 0\)</span> and suggest one change that could be made to eliminate this behavior, without changing the objective itself.</p>
<hr class="docutils" />
<p><em>Your answer here.</em></p>
</section>
<hr class="docutils" />
<section id="problem-1c-optimizing-a-high-dimensional-objective">
<h3>Problem 1c: Optimizing a High-Dimensional Objective<a class="headerlink" href="#problem-1c-optimizing-a-high-dimensional-objective" title="Permalink to this heading">#</a></h3>
<p>Now let’s tackle a more challenging, higher-dimensional problem. We’ll use the same objective, but this time use <span class="math notranslate nohighlight">\(\mathbf{a} \in \mathbb{R}^{10}\)</span> where:</p>
<div class="math notranslate nohighlight">
\[a_i = 10^{-2 + \frac{4(i - 1)}{9}}\]</div>
<p>for <span class="math notranslate nohighlight">\(i \in \{1, \dots, 10\}\)</span>. This means the curvature of the dimensions ranges from <span class="math notranslate nohighlight">\(0.01\)</span> to <span class="math notranslate nohighlight">\(100\)</span>.</p>
<p>Experiment with different optimizers and hyperparameter settings on this problem, starting from the initial point <span class="math notranslate nohighlight">\(x^{(0)} = (10, \dots, 10)\)</span> and run your chosen optimizer for <span class="math notranslate nohighlight">\(1000\)</span> iterations. You can see a complete list of optimizers PyTorch has implemented <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#algorithms">here</a>. Find an optimizer/hyperparameter regime that achieves a final loss of less than <span class="math notranslate nohighlight">\(0.01\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">10</span><span class="p">)])</span>

<span class="c1">### </span>
<span class="c1"># YOUR CODE HERE</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="n">run_optimization</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="c1">##</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Optimization Iteration&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Objective Value&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final loss: </span><span class="si">{:.10f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="problem-2-neural-network-classification">
<h2>Problem 2: Neural Network Classification<a class="headerlink" href="#problem-2-neural-network-classification" title="Permalink to this heading">#</a></h2>
<p>Next, we will use a neural network to solve a classification problem for which the data is not linearly separable. We will implement the network as a PyTorch <code class="docutils literal notranslate"><span class="pre">nn.module</span></code> and train it using gradient descent, computing gradients using automatic differentation.</p>
<p>First, we create and visualize a two-dimensional dataset where each point is labeled as positive (1) or negative (0). As seen below, the positive and negative points are not linearly separable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">305</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">make_dataset</span><span class="p">(</span><span class="n">num_points</span><span class="p">):</span>
    <span class="n">radius</span> <span class="o">=</span> <span class="mi">5</span>

    <span class="k">def</span> <span class="nf">sample_annulus</span><span class="p">(</span><span class="n">inner_radius</span><span class="p">,</span> <span class="n">outer_radius</span><span class="p">,</span> <span class="n">num_points</span><span class="p">):</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">Uniform</span><span class="p">(</span><span class="n">inner_radius</span><span class="p">,</span> <span class="n">outer_radius</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_points</span><span class="p">,))</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_points</span><span class="p">,))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
        <span class="k">return</span> <span class="n">data</span>
    
    <span class="c1"># Generate positive examples (labeled 1)</span>
    <span class="n">data_1</span> <span class="o">=</span> <span class="n">sample_annulus</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">radius</span><span class="p">,</span> <span class="n">num_points</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">labels_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_points</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        
    <span class="c1"># Generate negative examples (labeled 0).</span>
    <span class="n">data_0</span> <span class="o">=</span> <span class="n">sample_annulus</span><span class="p">(</span><span class="mf">0.7</span> <span class="o">*</span> <span class="n">radius</span><span class="p">,</span> <span class="n">radius</span><span class="p">,</span> <span class="n">num_points</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">labels_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_points</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        
    <span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">data_0</span><span class="p">,</span> <span class="n">data_1</span><span class="p">])</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">labels_0</span><span class="p">,</span> <span class="n">labels_1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span>
    
<span class="n">num_data</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">make_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">)</span>

<span class="c1"># Note: red indicates a label of 1, blue indicates a label of 0</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="n">num_data</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:</span><span class="n">num_data</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">num_data</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">num_data</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<section id="problem-2a-the-maximum-likelihood-objective">
<h3>Problem 2a: The Maximum Likelihood Objective<a class="headerlink" href="#problem-2a-the-maximum-likelihood-objective" title="Permalink to this heading">#</a></h3>
<p>We will try to classify this data using a neural network. We posit the following statistical model for the labels <span class="math notranslate nohighlight">\(y \in \{0, 1\}\)</span> given features <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[ y \mid x \sim \text{Bern}(\sigma(\text{NN}_{\boldsymbol{\theta}}(\mathbf{x})))\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\text{NN}_{\boldsymbol{\theta}}: \mathbb{R}^2 \to \mathbb{R}\)</span> denotes a neural network with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \mathbb{R}^P\)</span> mapping datapoints <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to <span class="math notranslate nohighlight">\(\text{logit}(\mathbb{P}(y = 1 \mid \mathbf{x}))\)</span>. Recall the <a class="reference external" href="https://en.wikipedia.org/wiki/Logit">logit</a> function is given by</p>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \log \left( \frac{p}{1-p} \right)\]</div>
<p>and its inverse is the <a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a>:</p>
<div class="math notranslate nohighlight">
\[\sigma(x) = \frac{1}{1 + \exp (-x)}\]</div>
<p>We estimate the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> using maximum likelihood. Show that for a dataset <span class="math notranslate nohighlight">\(\{(\mathbf{x}_n, y_n)\}_{n=1}^N\)</span> the negative log-likelihood objective, rescaled by the number of datapoints <span class="math notranslate nohighlight">\(N\)</span>, may be written as:</p>
<div class="math notranslate nohighlight">
\[ L(\boldsymbol{\theta}) = \frac{1}{N} \sum_{n = 1}^N -y_n \log \sigma(\text{NN}_{\boldsymbol{\theta}}(\mathbf{x})) - (1 - y_n) \log (1 - 
 \sigma(\text{NN}_{\boldsymbol{\theta}}(\mathbf{x}))) = \frac{1}{N} \sum_{n = 1}^N \ell(y, \text{NN}_{\boldsymbol{\theta}}(\mathbf{x}_n))
\]</div>
<p>where <span class="math notranslate nohighlight">\(\ell(y, x) = -y \log \sigma(x) - (1 - y) \log(1 - \sigma(x))\)</span>.</p>
<hr class="docutils" />
<p><em>Your answer here.</em></p>
</section>
<hr class="docutils" />
<section id="problem-2b-define-the-neural-network">
<h3>Problem 2b: Define the Neural Network<a class="headerlink" href="#problem-2b-define-the-neural-network" title="Permalink to this heading">#</a></h3>
<p>We will use a neural network with two hidden layers, the first of which has three hidden units and the second of which has five hidden units. The equations defining the output <span class="math notranslate nohighlight">\(z \in \mathbb{R}\)</span> of our neural network given an input <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^2\)</span> are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{h}_1 &amp;= \text{ReLU}(\mathbf{W_1} \mathbf{x} + \mathbf{b_1}) \\
\mathbf{h}_2 &amp;= \text{ReLU}(\mathbf{W_2} \mathbf{h}_1 + \mathbf{b_2}) \\
z &amp;= \mathbf{w}_3^\top \mathbf{h}_2 + b_3
\end{align*}
\end{split}\]</div>
<p>The parameters of the network are <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = (\mathbf{W}_1, \mathbf{b}_1, \mathbf{W}_2, \mathbf{b}_2, \mathbf{w}_3, b_3)\)</span> where <span class="math notranslate nohighlight">\(\mathbf{W}_1 \in \mathbb{R}^{3 \times 2}, \mathbf{b}_1 \in \mathbb{R}^3, \mathbf{W}_2 \in \mathbb{R}^{5 \times 3}, \mathbf{b}_2 \in \mathbb{R}^5, \mathbf{w}_3 \in \mathbb{R}^5\)</span>, and <span class="math notranslate nohighlight">\(b_3 \in \mathbb{R}\)</span>.</p>
<p>Implement this network as a PyTorch <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> using <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> and <code class="docutils literal notranslate"><span class="pre">F.relu</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1">###</span>
        <span class="c1"># YOUR CODE HERE</span>
        <span class="c1">##</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Implements the forward pass of the network.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: torch.tensor of shape (N, 2)</span>
<span class="sd">        Returns:</span>
<span class="sd">            logits: torch.tensor of shape (N,) containing the logits</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">###</span>
        <span class="c1"># YOUR CODE HERE</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="o">...</span>
        <span class="c1">##</span>
        <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s visualize the predictions of an untrained network. As we can see, the network does not succeed at classifying the points without training</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">visualize_predictions</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
    <span class="n">num_points</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="n">x1s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
    <span class="n">x2s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
    <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="n">x2s</span><span class="p">)</span>

    <span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2s</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">x2s</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1s</span><span class="p">))])</span>
    <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">points</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;YlGn&#39;</span><span class="p">),</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="n">num_data</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:</span><span class="n">num_data</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">num_data</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">num_data</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Output Probabilities&quot;</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">305</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span>
<span class="n">visualize_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="problem-2c-train-the-network">
<h3>Problem 2c: Train the Network<a class="headerlink" href="#problem-2c-train-the-network" title="Permalink to this heading">#</a></h3>
<p>We will now find the parameters of our network by maximizing the likelihood, or equivalently minimizing the negative log-likelihood. We will use full-batch gradient descent. That is, we will use the gradient <span class="math notranslate nohighlight">\(\nabla L(\boldsymbol{\theta})\)</span> itself to update the parameters rather than a stochastic estimate of <span class="math notranslate nohighlight">\(\nabla L(\boldsymbol{\theta})\)</span>.</p>
<p>Use the SGD optimizer from <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> with a learning rate of <span class="math notranslate nohighlight">\(1\)</span> and no momentum for 1000 iterations. Note that the function <span class="math notranslate nohighlight">\(\ell\)</span> from above is implemented in PyTorch as <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"><code class="docutils literal notranslate"><span class="pre">nn.BCEWithLogitsLoss</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_steps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">###</span>
<span class="c1"># YOUR CODE HERE</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="o">...</span>
<span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">)):</span>
    <span class="o">...</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="o">...</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="c1">##</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Gradient Descent Iteration&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Objective Value&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s visualize the predictions of our trained network. We see that the network has learned to separate the positive and negative examples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="problem-3-amortized-variational-inference">
<h2>Problem 3: Amortized Variational Inference<a class="headerlink" href="#problem-3-amortized-variational-inference" title="Permalink to this heading">#</a></h2>
<p>In this problem, we will train a variational autoencoder for the MNIST dataset of handwritten digits. First, let’s download this dataset using PyTorch’s <code class="docutils literal notranslate"><span class="pre">datasets</span></code> module and visualize some of the digits. We will use a binarized version of the dataset in which each pixel value is either 0 or 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download MNIST dataset and create dataloaders. </span>
<span class="k">def</span> <span class="nf">binarize</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">integer</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="mi">127</span> <span class="k">if</span> <span class="n">integer</span> <span class="k">else</span> <span class="mf">0.5</span>
    <span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">imgs</span><span class="p">[</span><span class="n">imgs</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">imgs</span><span class="p">[</span><span class="n">imgs</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
    <span class="k">return</span> <span class="n">imgs</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of points in dataset: </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of batches per epoch: </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize some digits in the dataset.</span>
<span class="n">imgs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="n">binarize</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">imgs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="problem-3a-decoder-network">
<h3>Problem 3a: Decoder Network<a class="headerlink" href="#problem-3a-decoder-network" title="Permalink to this heading">#</a></h3>
<p>We represent a <span class="math notranslate nohighlight">\(28 \times 28\)</span> image <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> as a flattened <span class="math notranslate nohighlight">\(784\)</span> dimensional vector of binary values, i.e. <span class="math notranslate nohighlight">\(\mathbf{x} \in \{0, 1\}^{784}\)</span>. We specify our generative model as:</p>
<div class="math notranslate nohighlight">
\[ \mathbf{z} \sim \mathcal{N}(0, I), \quad \mathbf{x} \mid \mathbf{z} \sim \text{Bern}(\sigma(D_{\boldsymbol{\theta}}(\mathbf{z}))\]</div>
<p>Here, <span class="math notranslate nohighlight">\(D_{\boldsymbol{\theta}}: \mathbb{R}^2 \to \mathbb{R}^{784}\)</span> is a neural network with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^{2}\)</span> is a two-dimensional latent variable. We use only two dimensions so that the latent space can be easily visualized later, but using a higher dimensional latent variable would give a more flexible generative model.</p>
<p>We will parametrize <span class="math notranslate nohighlight">\(D_{\boldsymbol{\theta}}\)</span> as a fully connected neural network with two hidden layers and ReLU activations. We use 256 units in the first hidden layer and 512 in the second. Note that as in Problem 2, the network maps to the logits of the Bernoulli distribution and not the probabilities themselves. Implement this decoder network in PyTorch below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define decoder architecture</span>
<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Neural network defining p(x | z) &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_dim</span> <span class="o">=</span> <span class="n">data_dim</span>

        <span class="c1">###</span>
        <span class="c1"># YOUR CODE HERE</span>
        <span class="c1">## </span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns Bernoulli conditional distribution of p(x | z), parametrized</span>
<span class="sd">        by logits.</span>
<span class="sd">        Args:</span>
<span class="sd">            z: (N, latent_dim) torch.tensor</span>
<span class="sd">        Returns:</span>
<span class="sd">            Bernoulli distribution with a batch of (N, data_dim) logits</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">###</span>
        <span class="c1"># YOUR CODE HERE</span>
        <span class="c1">## </span>
        <span class="k">return</span> <span class="n">Bernoulli</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="problem-3b-encoder-network">
<h3>Problem 3b: Encoder Network<a class="headerlink" href="#problem-3b-encoder-network" title="Permalink to this heading">#</a></h3>
<p>We will estimate the parameters of the generative model by maximizing the Evidence Lower Bound (ELBO). As the exact posterior <span class="math notranslate nohighlight">\(p(\mathbf{z} \mid \mathbf{x})\)</span> is unknown, we will use an approximate, amortized posterior <span class="math notranslate nohighlight">\(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\mathbf{z} \mid \mu_{\boldsymbol{\phi}}(\mathbf{x}), \text{diag}(\sigma^2_{\boldsymbol{\phi}}(\mathbf{x})))\)</span>. We let <span class="math notranslate nohighlight">\(\left(\mu_{\boldsymbol{\phi}}(\mathbf{x}), \log \sigma^2_{\boldsymbol{\phi}}(\mathbf{x}) \right) = E_{\boldsymbol{\phi}}(\mathbf{x})\)</span> where <span class="math notranslate nohighlight">\(E_{\boldsymbol{\phi}}: \mathbb{R}^{784} \to \mathbb{R}^2 \times \mathbb{R}^2\)</span> is a neural network with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span>.</p>
<p>As above, we parametrize <span class="math notranslate nohighlight">\(E_{\boldsymbol{\phi}}\)</span> as a neural network with two layers of hidden units and ReLU activations. We use 512 hidden units in the first layer and 256 in the second. Then we let <span class="math notranslate nohighlight">\(\mu_{\boldsymbol{\phi}}\)</span> and <span class="math notranslate nohighlight">\(\log \sigma^2_{\boldsymbol{\phi}}\)</span> be affine functions of the hidden layer activations. Implement the encoder <span class="math notranslate nohighlight">\(E_{\boldsymbol{\phi}}\)</span> in the code below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define encoder architecture</span>
<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Neural network defining q(z | x). &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1">###</span>
        <span class="c1"># YOUR CODE HERE</span>
        <span class="c1">##</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns Normal conditional distribution for q(z | x), with mean and</span>
<span class="sd">        log-variance output by a neural network.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: (N, data_dim) torch.tensor</span>
<span class="sd">        Returns:</span>
<span class="sd">            Normal distribution with a batch of (N, latent_dim) means and standard deviations</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">###</span>
        <span class="c1"># YOUR CODE HERE</span>
        <span class="c1">##</span>
        <span class="k">return</span> <span class="n">Normal</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="elbo-derivation-given">
<h3>ELBO Derivation [given]<a class="headerlink" href="#elbo-derivation-given" title="Permalink to this heading">#</a></h3>
<p>As a function of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span>, we can write the ELBO for a single datapoint <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\phi}) = \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})} \left[ \log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z}) - \log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \right]\]</div>
<p>We can obtain a lower bound of the log-likelihood for an entire dataset <span class="math notranslate nohighlight">\(\{\mathbf{x}^{(n)} \}_{n=1}^N\)</span>, rescaled by the number of datapoints <span class="math notranslate nohighlight">\(N\)</span>, as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi}) = \frac{1}{N} \sum_{n=1}^N \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}^{(n)} \mid \mathbf{x}^{(n)})} \left[ \log p_{\boldsymbol{\theta}}(\mathbf{x}^{(n)}, \mathbf{z}^{(n)}) - \log q_{\boldsymbol{\phi}}(\mathbf{z}^{(n)} \mid \mathbf{x}^{(n)}) \right]\]</div>
<p>We can rewrite the per-datapoint ELBO as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{L}(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\phi}) 
&amp;= \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})} \left[ \log p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z}) \right] - \text{KL}\left( q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \mid\mid p(\mathbf{z})\right) \\
&amp;= \mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})} \left[ \log p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mu_{\boldsymbol{\phi}}(\mathbf{x}) + \boldsymbol{\epsilon} \odot \sigma_{\boldsymbol{\phi}}(\mathbf{x})) \right] - \text{KL}\left( q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \mid\mid p(\mathbf{z})\right)
\end{align*}
\end{split}\]</div>
<p>This allows us to obtain an unbiased estimate of the per-datapoint ELBO by first sampling <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span>, then computing:</p>
<div class="math notranslate nohighlight">
\[\hat{\mathcal{L}}(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\phi}) = \log p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mu_{\boldsymbol{\phi}}(\mathbf{x}) + \boldsymbol{\epsilon} \odot \sigma_{\boldsymbol{\phi}}(\mathbf{x})) - \text{KL}\left( q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \mid\mid p(\mathbf{z})\right)\]</div>
<p>This is known as the reparametrization trick, and it will allow us to straightforwardly use automatic differentiation to obtain the gradient of <span class="math notranslate nohighlight">\(\hat{\mathcal{L}}\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span>.</p>
<p>Given a minibatch <span class="math notranslate nohighlight">\(\{\mathbf{x}^{(b)} \}_{b=1}^B\)</span> sampled uniformly from the entire dataset, we can simulate independent normal variates <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}^{(b)}\)</span> to form an unbiased estimator of the ELBO for the entire dataset:</p>
<div class="math notranslate nohighlight">
\[\hat{\mathcal{L}}(\boldsymbol{\theta}, \boldsymbol{\phi}) = \frac{1}{B} \sum_{b = 1}^B \log p_{\boldsymbol{\theta}}(\mathbf{x}^{(b)} \mid \mu_{\boldsymbol{\phi}}(\mathbf{x}^{(b)}) + \boldsymbol{\epsilon} \odot \sigma_{\boldsymbol{\phi}}(\mathbf{x}^{(b)})) - \text{KL}\left( q_{\boldsymbol{\phi}}(\mathbf{z}^{(b)} \mid \mathbf{x}^{(b)}) \mid\mid p(\mathbf{z}^{(b)})\right) \]</div>
</section>
<section id="problem-3c-implement-the-elbo">
<h3>Problem 3c: Implement the ELBO<a class="headerlink" href="#problem-3c-implement-the-elbo" title="Permalink to this heading">#</a></h3>
<p>Using our derivations above, implement the estimator of the ELBO <span class="math notranslate nohighlight">\(\hat{\mathcal{L}}(\boldsymbol{\theta}, \boldsymbol{\phi})\)</span>. We assume sampling of the minibatch <code class="docutils literal notranslate"><span class="pre">x</span></code> is done outside of the function, but you must sample the noise variables <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> within the <code class="docutils literal notranslate"><span class="pre">elbo</span></code> function. You should use the <code class="docutils literal notranslate"><span class="pre">kl_divergence</span></code> function imported above to analytically compute the KL divergence between the Gaussian distributions <span class="math notranslate nohighlight">\(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\)</span> and <span class="math notranslate nohighlight">\(p(\mathbf{z})\)</span>. Make sure you use <code class="docutils literal notranslate"><span class="pre">rsample</span></code> on a <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> object to use the reparametrization trick and not <code class="docutils literal notranslate"><span class="pre">sample</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">elbo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Computes a stochastic estimate of the rescaled evidence lower bound</span>

<span class="sd">    Args:</span>
<span class="sd">        x: (N, data_dim) torch.tensor</span>
<span class="sd">        encoder: an Encoder</span>
<span class="sd">        decoder: a Decoder</span>
<span class="sd">    Returns:</span>
<span class="sd">        elbo: a (,) torch.tensor containing the estimate of the ELBO</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">###</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="n">elbo</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1">##</span>
    <span class="k">return</span> <span class="n">elbo</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="implement-the-training-loop-given">
<h3>Implement the Training Loop [given]<a class="headerlink" href="#implement-the-training-loop-given" title="Permalink to this heading">#</a></h3>
<p>Using our <code class="docutils literal notranslate"><span class="pre">Encoder</span></code> and <code class="docutils literal notranslate"><span class="pre">Decoder</span></code> definitions, as well as the <code class="docutils literal notranslate"><span class="pre">elbo</span></code> function, we have provided training code below. This code uses the <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam</a> optimizer, a sophisticated optimization algorithm which uses the history of past gradients to rescale gradients before applying an update.</p>
<p>We train for 20 epochs (an “epoch” refers to a complete pass through the dataset). Our implementation takes 10 minutes to run and achieves a training ELBO of <span class="math notranslate nohighlight">\(-135\)</span> and a test ELBO of <span class="math notranslate nohighlight">\(-138\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">data_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">data_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span>
                       <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">20</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">encoder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">decoder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">train_elbo</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">binarize</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">elbo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">)</span>  
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">train_elbo</span> <span class="o">-=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Epoch: </span><span class="si">{}</span><span class="s1"> [</span><span class="si">{}</span><span class="s1">/</span><span class="si">{}</span><span class="s1"> (</span><span class="si">{:.0f}</span><span class="s1">%)]</span><span class="se">\t</span><span class="s1">ELBO: </span><span class="si">{:.6f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span>
                <span class="mf">100.</span> <span class="o">*</span> <span class="n">batch_idx</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> <span class="o">-</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
            
    <span class="n">encoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">decoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">test_elbo</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">binarize</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="c1">#.cuda()</span>
            <span class="n">test_elbo</span> <span class="o">+=</span> <span class="n">elbo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
    <span class="n">train_elbo</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">test_elbo</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;====&gt; Epoch: </span><span class="si">{}</span><span class="s1"> Average ELBO: </span><span class="si">{:.4f}</span><span class="s1"> Test ELBO: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span>
                                                                          <span class="n">train_elbo</span><span class="p">,</span>
                                                                          <span class="n">test_elbo</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="problem-3d-visualize-samples-from-the-trained-model">
<h3>Problem 3d: Visualize samples from the trained model<a class="headerlink" href="#problem-3d-visualize-samples-from-the-trained-model" title="Permalink to this heading">#</a></h3>
<p>In addition to the ELBO, we can sample from the trained model to assess its performance. Use the code below to generate an <span class="math notranslate nohighlight">\(8 \times 8\)</span> grid of sampled digits from the model. Note that we follow the common practice of using the mean of <span class="math notranslate nohighlight">\(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\)</span> rather than resampling from this distribution when visualizing samples. Critique these samples. What aspects of the data distribution does the model seem to have trouble learning?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize sampled digits from our model</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">64</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">expected_xs</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span> 
    <span class="n">expected_xs</span> <span class="o">=</span> <span class="n">expected_xs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Plot the expected_xs as a grid of images</span>
<span class="n">expected_xs_grid</span> <span class="o">=</span> <span class="n">make_grid</span><span class="p">(</span><span class="n">expected_xs</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">expected_xs_grid</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">vmin</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><em>Your answer here.</em></p>
</section>
<hr class="docutils" />
<section id="problem-3e-visualize-the-latent-embeddings">
<h3>Problem 3e: Visualize the Latent Embeddings<a class="headerlink" href="#problem-3e-visualize-the-latent-embeddings" title="Permalink to this heading">#</a></h3>
<p>Given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, we can interpret the mean of the approximate posterior <span class="math notranslate nohighlight">\(\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}[\mathbf{z}]\)</span> as a lower dimensional representation of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. In the code below, we find the mean of the approximate posterior for each of the datapoints in the dataset and then visualize these means with a scatter plot. We color each point according to the label of the encoded digit. What do you notice? Are there classes with significant overlap, and are these classes which are visually similar? Is there a class which has clear separation from the others, and if so, why do you think this is?</p>
<p>Note that we did not provide any information about the class label to either the generative model or the approximate posterior!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute the mean of the latents given the data</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">means</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>        
        <span class="n">x</span> <span class="o">=</span> <span class="n">binarize</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span>
        <span class="n">means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">means</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">means</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>

<span class="c1"># Plot the first two dimensions of the latents</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">means_i</span> <span class="o">=</span> <span class="n">means</span><span class="p">[</span><span class="n">ys</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">means_i</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">means_i</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$z_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$z_2$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><em>Your answer here.</em></p>
</section>
<hr class="docutils" />
<section id="problem-3f-interpolation-in-the-latent-space">
<h3>Problem 3f: Interpolation in the Latent Space<a class="headerlink" href="#problem-3f-interpolation-in-the-latent-space" title="Permalink to this heading">#</a></h3>
<p>Another desideratum for a latent variable model is smooth interpolation in the latent space. For example, if we linearly interpolate between a latent <span class="math notranslate nohighlight">\(\mathbf{z}_{start}\)</span> corresponding to a <span class="math notranslate nohighlight">\(7\)</span> and a latent <span class="math notranslate nohighlight">\(\mathbf{z}_{end}\)</span> corresponding to a <span class="math notranslate nohighlight">\(1\)</span>, we should observe the decodings of the interpolations smoothly change from a <span class="math notranslate nohighlight">\(7\)</span> to a <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>In the code below, we sample <span class="math notranslate nohighlight">\(8\)</span> different starting latent variables and <span class="math notranslate nohighlight">\(8\)</span> different ending latent variables from the prior, linearly interpolate between them for <span class="math notranslate nohighlight">\(10\)</span> points, then plot the decodings. Does our model smoothly change between decoded digits? Are there digit pairs it was more successful interpolating between?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Interpolate between 8 randomly chosen start and end points</span>
<span class="n">latent_starts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">latent_ends</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">latent_starts</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="p">(</span><span class="n">latent_ends</span> <span class="o">-</span> <span class="n">latent_starts</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="n">means_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">means</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sample_grid</span> <span class="o">=</span> <span class="n">make_grid</span><span class="p">(</span><span class="n">means_tensor</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">sample_grid</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">vmin</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p><em>Your answer here.</em></p>
</section>
</section>
<hr class="docutils" />
<section id="problem-4-reflections">
<h2>Problem 4: Reflections<a class="headerlink" href="#problem-4-reflections" title="Permalink to this heading">#</a></h2>
<section id="problem-4a">
<h3>Problem 4a<a class="headerlink" href="#problem-4a" title="Permalink to this heading">#</a></h3>
<p>Discuss one reason why we use amortized variational inference rather than optimizing per-datapoint latent variables <span class="math notranslate nohighlight">\(\mu^{(n)}, (\boldsymbol{\sigma}^2)^{(n)}\)</span> (so <span class="math notranslate nohighlight">\(p(\mathbf{z^{(n)}} \mid \mathbf{x}^{(n)})= \mathcal{N}(\mathbf{z}^{(n)} \mid \mu^{(n)}, \text{diag}((\boldsymbol{\sigma}^2)^{(n)})\)</span>).</p>
<hr class="docutils" />
<p><em>Your answer here.</em></p>
</section>
<hr class="docutils" />
<section id="problem-4b">
<h3>Problem 4b<a class="headerlink" href="#problem-4b" title="Permalink to this heading">#</a></h3>
<p>Describe one way you could improve the variational autoencoder, either by changing the encoder or decoder network structure or by changing the  model itself, and why you think your proposed change would help.</p>
<hr class="docutils" />
<p><em>Your answer here.</em></p>
</section>
<hr class="docutils" />
<section id="problem-4c">
<h3>Problem 4c<a class="headerlink" href="#problem-4c" title="Permalink to this heading">#</a></h3>
<p>Suppose rather than using a Gaussian prior on <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, we used <span class="math notranslate nohighlight">\(\mathbf{z} \overset{ind}{\sim} \text{Bern}(0.5)\)</span>. We can modify <span class="math notranslate nohighlight">\(E_{\boldsymbol{\phi}}\)</span> to output logits for a multivariate Bernoulli distribution: <span class="math notranslate nohighlight">\(p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\phi}) = \text{Bern}(\mathbf{z}; \sigma(E_{\boldsymbol{\phi}}(\mathbf{x})))\)</span>. Where would our optimization procedure break down in this case?</p>
<hr class="docutils" />
<p><em>Your answer here.</em></p>
</section>
</section>
<hr class="docutils" />
<section id="submission-instructions">
<h2>Submission Instructions<a class="headerlink" href="#submission-instructions" title="Permalink to this heading">#</a></h2>
<p><strong>Formatting:</strong> check that your code does not exceed 80 characters in line width. You can set <em>Tools → Settings → Editor → Vertical ruler column</em> to 80 to see when you’ve exceeded the limit.</p>
<p>Download your notebook in .ipynb format and remove the Open in Colab button.  Then run the following command to convert to a PDF:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jupyter</span> <span class="n">nbconvert</span> <span class="o">--</span><span class="n">to</span> <span class="n">pdf</span> <span class="o">&lt;</span><span class="n">yourname</span><span class="o">&gt;</span><span class="n">_hw6</span><span class="o">.</span><span class="n">ipynb</span>
</pre></div>
</div>
<p><strong>Installing nbconvert:</strong></p>
<p>If you’re using Anaconda for package management,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">anaconda</span> <span class="n">nbconvert</span>
</pre></div>
</div>
<p><strong>Upload</strong> your .pdf files to Gradescope.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./assignments/hw6"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../hw5/hw5.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">HW5: Poisson Matrix Factorization</p>
      </div>
    </a>
    <a class="right-next"
       href="../hw7/hw7.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">HW7: Autoregressive HMMs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1-optimizing-a-quadratic-objective-via-gradient-descent">Problem 1: Optimizing a Quadratic Objective via Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1a-gradient-descent-with-a-well-conditioned-objective">Problem 1a: Gradient Descent with a well-conditioned Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1b-gradient-descent-with-a-ill-conditioned-objective">Problem 1b: Gradient Descent with a Ill-Conditioned Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1c-optimizing-a-high-dimensional-objective">Problem 1c: Optimizing a High-Dimensional Objective</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2-neural-network-classification">Problem 2: Neural Network Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2a-the-maximum-likelihood-objective">Problem 2a: The Maximum Likelihood Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2b-define-the-neural-network">Problem 2b: Define the Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2c-train-the-network">Problem 2c: Train the Network</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3-amortized-variational-inference">Problem 3: Amortized Variational Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3a-decoder-network">Problem 3a: Decoder Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3b-encoder-network">Problem 3b: Encoder Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elbo-derivation-given">ELBO Derivation [given]</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3c-implement-the-elbo">Problem 3c: Implement the ELBO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-the-training-loop-given">Implement the Training Loop [given]</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3d-visualize-samples-from-the-trained-model">Problem 3d: Visualize samples from the trained model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3e-visualize-the-latent-embeddings">Problem 3e: Visualize the Latent Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3f-interpolation-in-the-latent-space">Problem 3f: Interpolation in the Latent Space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-4-reflections">Problem 4: Reflections</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-4a">Problem 4a</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-4b">Problem 4b</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-4c">Problem 4c</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#submission-instructions">Submission Instructions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Scott Linderman
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>