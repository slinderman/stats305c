{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZzZt5v9lp7P"
      },
      "source": [
        "# HW8: Sigmoidal Gaussian Cox Processes\n",
        "\n",
        "---\n",
        "\n",
        "**Name:**\n",
        "\n",
        "**Names of any collaborators:**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NGQPIfCtlLR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.distributions import (\n",
        "    Bernoulli, Distribution, Exponential, \n",
        "    MultivariateNormal, Normal, Poisson, Uniform)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xpxO2j5eTSqh"
      },
      "source": [
        "## Helpers\n",
        "\n",
        "First we define a few helper functions and classes.\n",
        "- `probit` computes the probit function (i.e. the standard normal CDF)\n",
        "\n",
        "- `OneSidedTruncatedNormal` is a PyTorch `Distribution` for sampling from a truncated normal distribution of the form,\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "p(x; \\mu, \\sigma^2, a) &\\propto \\mathcal{N}(x; \\mu, \\sigma^2) \\, \\mathbb{I}[x > a].\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "It uses the inverse CDF sampling method when $\\frac{a - \\mu}{\\sigma} < 2$ and a rejection sampling method from [Robert (2009)](https://arxiv.org/pdf/0907.4010.pdf) otherwise. For the latter case, rejection sampling is more numerically stable and nearly as efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6P2-UWo76A0"
      },
      "outputs": [],
      "source": [
        "def probit(u):\n",
        "    return Normal(0, 1, validate_args=False).cdf(u)\n",
        "\n",
        "class OneSidedTruncatedNormal(Distribution):\n",
        "    \"\"\"\n",
        "    Super simple implementation of a one-sided truncated normal distribution.\n",
        "    \n",
        "    ..math:\n",
        "        p(x; \\mu, \\sigma^2, a) \\propto N(x; \\mu, \\sigma^2) I[x > a]\n",
        "\n",
        "    where $\\mu$ is the location, $\\sigma$ is the scale, and $a$ is the lower bound.\n",
        "    \"\"\"\n",
        "    def __init__(self, loc, scale, lower_bound):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        loc: the location of the truncated normal distribution\n",
        "        scale: the scale of the truncated normal distribution\n",
        "        lower_bound: the lower bound of the truncated normal distribution\n",
        "        \"\"\"\n",
        "        to_tensor = lambda x: x if isinstance(x, torch.Tensor) else torch.tensor(x)\n",
        "        self.loc = to_tensor(loc)\n",
        "        self.scale = to_tensor(scale)\n",
        "        self.lower_bound = to_tensor(lower_bound)\n",
        "\n",
        "        # Compute the batch shape and broadcast\n",
        "        self._batch_shape = torch.broadcast_shapes(\n",
        "            self.loc.shape, self.scale.shape, self.lower_bound.shape)\n",
        "        self.loc = self.loc * torch.ones(self._batch_shape)\n",
        "        self.scale = self.scale * torch.ones(self._batch_shape)\n",
        "        self.lower_bound = self.lower_bound * torch.ones(self._batch_shape)\n",
        "\n",
        "        # Convert params into cdf coordinates\n",
        "        self._u_lb = Normal(0., 1.).cdf((self.lower_bound - self.loc) / self.scale)\n",
        "\n",
        "    def sample(self, sample_shape=()):\n",
        "        \"\"\"\n",
        "        Draw samples from the truncated normal distribution.\n",
        "\n",
        "        NOTE: This can be unstable when self._u_lb is close to 1... \n",
        "        In those cases we should really use a rejection sampling algorithm.\n",
        "        C.f. https://arxiv.org/pdf/0907.4010.pdf\n",
        "        \"\"\"\n",
        "        if sample_shape != ():\n",
        "            raise NotImplementedError(\n",
        "                \"We haven't supported sampling many at once. \"\n",
        "                \"If you need to do that, broadcast the constructor args.\")\n",
        "        \n",
        "        # Use the inverse CDF sampling algorithm only if the lower bound is small\n",
        "        do_icdf = (self.lower_bound - self.loc) / self.scale < 2.0\n",
        "        samples = torch.full(self._batch_shape, torch.nan)\n",
        "\n",
        "        samples[do_icdf] = OneSidedTruncatedNormal._inverse_cdf_sample(\n",
        "            self.loc[do_icdf], self.scale[do_icdf], self.lower_bound[do_icdf])\n",
        "        \n",
        "        samples[~do_icdf] = OneSidedTruncatedNormal._rejection_sample(\n",
        "            self.loc[~do_icdf], self.scale[~do_icdf], self.lower_bound[~do_icdf])\n",
        "        \n",
        "        assert torch.all(torch.isfinite(samples))\n",
        "        return samples\n",
        "\n",
        "    @staticmethod\n",
        "    def _inverse_cdf_sample(loc, scale, lower_bound):\n",
        "        u_lb = Normal(loc, scale).cdf(lower_bound)\n",
        "        u = Uniform(u_lb, 1-1e-4).sample()\n",
        "        return Normal(loc, scale).icdf(u)\n",
        "\n",
        "    @staticmethod\n",
        "    def _rejection_sample(loc, scale, lower_bound, max_steps=20):\n",
        "        \"\"\"Inverse CDF sampling is unstable when (lower_bound - loc) / scale >> 1.\n",
        "        In that case, use a rejection sampling algorithm instead:\n",
        "        https://arxiv.org/pdf/0907.4010.pdf\n",
        "\n",
        "        This algorithm draws samples from N_+(0, 1, a) where a is the lower bound.\n",
        "        \"\"\"\n",
        "        def _propose_and_accept(z_lb):\n",
        "            alpha = 0.5 * (z_lb + torch.sqrt(z_lb**2 + 4))\n",
        "            proposal = z_lb + Exponential(alpha).sample()\n",
        "            threshold = torch.exp(-0.5 * (proposal - alpha)**2)\n",
        "            accept = Uniform(0, 1).sample(z_lb.shape) <= threshold\n",
        "            return proposal, accept\n",
        "\n",
        "        # Compute the standardized lower bound\n",
        "        z_lb = (lower_bound - loc) / scale\n",
        "\n",
        "        # Propose from an exponential distribution\n",
        "        samples = torch.full(z_lb.shape, torch.nan)\n",
        "        valid = torch.zeros(z_lb.shape, dtype=bool)\n",
        "\n",
        "        count = 0\n",
        "        while torch.any(~valid):\n",
        "            count += 1\n",
        "            if count == max_steps:\n",
        "                raise Exception(\"Maximum number of rejection sampling steps reached!\")\n",
        "\n",
        "            # only update the indices that are invalid\n",
        "            inds = torch.nonzero(~valid, as_tuple=True)\n",
        "            proposal, accept = _propose_and_accept(z_lb[inds])\n",
        "            samples[inds] = torch.where(accept, proposal, samples[inds])\n",
        "            valid[inds] = torch.where(accept, True, valid[inds])\n",
        "\n",
        "        # Rescale samples and return\n",
        "        return samples * scale + loc        \n",
        "\n",
        "    def log_prob(self, value):\n",
        "        lp = Normal(self.loc, self.scale).log_prob(value) - torch.log1p(-self.u_lb) \n",
        "        lp = torch.where(lp, value < self.lower_bound, -torch.inf, lp)\n",
        "        return lp\n",
        "\n",
        "## Test\n",
        "# plt.figure()\n",
        "# tn = OneSidedTruncatedNormal(0 * torch.ones(10000), \n",
        "#                              2. * torch.ones(10000), \n",
        "#                              0. * torch.ones(10000))\n",
        "# plt.hist(tn.sample(), 25)\n",
        "# plt.xlabel(\"x\")\n",
        "# plt.ylabel(\"p(x)\")\n",
        "\n",
        "# plt.figure()\n",
        "# tn = OneSidedTruncatedNormal(-6 * torch.ones(10000), \n",
        "#                              2. * torch.ones(10000), \n",
        "#                              0. * torch.ones(10000))\n",
        "# plt.hist(tn.sample(), 25)\n",
        "# plt.xlabel(\"x\")\n",
        "# plt.ylabel(\"p(x)\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W4TbDvm8UBhT"
      },
      "source": [
        "## Part 1: Gaussian processes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EvmkbuXFUQVq"
      },
      "source": [
        "### Problem 1a [Code]: Write a function to sample a 1D Gaussian process\n",
        "\n",
        "_Hint: For numerical stability, you may have to add a small amount (like $10^{-4}$) to the diagonal of the Gram matrix to ensure positive definiteness._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4ZzA6NtUdvn"
      },
      "outputs": [],
      "source": [
        "def sample_gp(xs, mean_func, kernel, sample_shape=()):\n",
        "    \"\"\"\n",
        "    Sample a one-dimensional Gaussian process.\n",
        "\n",
        "    Args:\n",
        "        xs: shape (N,) tensor specifying the inputs at which to sample the GP\n",
        "        mean_func: function that takes in an (N,) tensor of xs and outputs an \n",
        "            (N,) tensor of the means E[f(x)] for each x\n",
        "        kernel: function that takes in (N,) tensor of xs and (M,) tensor of x's \n",
        "            and outputs a (N, M) tensor of the kernel evaluated at each pair \n",
        "            (x, x'). E.g. if the input tensors are the same, this function\n",
        "             computes the Gram matrix.\n",
        "        sample_shape: [optional] tuple specifying number of samples\n",
        "\n",
        "    Returns:\n",
        "        fs: tensor of shape (sample_shape + (N,)) with independent samples of \n",
        "            the GP.\n",
        "    \"\"\"\n",
        "    assert xs.ndim == 1\n",
        "    \n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    fs = ...\n",
        "    #\n",
        "    ##\n",
        "    return fs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7WtoNQUTXKMd"
      },
      "source": [
        "#### Test that your code outputs sensible samples\n",
        "This code uses a mean function of zero and a squared exponential kernel with length scale $\\ell = 5$ and variance $\\sigma^2 = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A33hZgNAVjnf"
      },
      "outputs": [],
      "source": [
        "mean_func = lambda xs: torch.zeros_like(xs)\n",
        "kernel = lambda x1s, x2s: 1 * torch.exp(\n",
        "    -0.5 * ((x1s[:, None] - x2s[None, :]) / 5)**2)\n",
        "T = 100\n",
        "xs = torch.linspace(0, T, T+1)\n",
        "fs_samples = sample_gp(xs, mean_func, kernel, sample_shape=(5,))\n",
        "\n",
        "# Plot the samples\n",
        "for fs in fs_samples:\n",
        "    plt.plot(xs, fs)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.title(\"Samples from the GP prior\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uR3u_mPQXml1"
      },
      "source": [
        "### Problem 1b [Code]: Write a function to compute the 1D GP posterior predictive distribution\n",
        "\n",
        "Given observations $\\{x_n, f(x_n)\\}_{n=1}^N$ with $x_n \\in \\mathbb{R}$ and $f(x_n) \\in \\mathbb{R}$, compute the posterior predictive distribution of $\\{f(x_m)\\}_{m=1}^M$ at new points $\\{x_m\\}_{m=1}^M$.\n",
        "\n",
        "_Hint: like above, you may need to add a small amount to the diagonal of the Gram matrices._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrVJE5CcYL0D"
      },
      "outputs": [],
      "source": [
        "def compute_gp_predictions(xs, fs, new_xs, mean_func, kernel):\n",
        "    \"\"\"\n",
        "    Compute the posterior predictive distribution of a \n",
        "\n",
        "    Args:\n",
        "        xs: shape (N,) tensor specifying the observed inputs \n",
        "        fs: shape (N,) tensor specifying the observed outputs \n",
        "        new_xs: shape (M,) tensor specifying the inputs at which to evaluate the \n",
        "            posterior predictive distribution.\n",
        "        mean_func: function that takes in an (N,) tensor of xs and outputs an \n",
        "            (N,) tensor of the means E[f(x)] for each x\n",
        "        kernel: function that takes in (N,) tensor of xs and (M,) tensor of x's \n",
        "            and outputs a (N, M) tensor of the kernel evaluated at each pair \n",
        "            (x, x'). E.g. if the input tensors are the same, this function \n",
        "            computes the Gram matrix.\n",
        "    \n",
        "    Returns:\n",
        "        pred_mean: shape (M,) tensor with posterior predictive mean\n",
        "        pred_cov: shape (M,M) tensor with posterior predictive covariance\n",
        "    \"\"\" \n",
        "    assert xs.ndim == 1\n",
        "    assert fs.ndim == 1\n",
        "    assert new_xs.ndim == 1\n",
        "\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    pred_mean = ...\n",
        "    pred_cov = ...\n",
        "    #\n",
        "    ##\n",
        "\n",
        "    # Answer might not be perfectly symmetric due to numerical precision \n",
        "    # limits. Symmetrize to be safe. \n",
        "    pred_cov = 0.5 * (pred_cov + pred_cov.T)\n",
        "    return pred_mean, pred_cov\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7J9Vb3LjZVV8"
      },
      "source": [
        "#### Test that your code outputs reasonable predictions\n",
        "\n",
        "Run the following cell to produce a plot of the GP posterior predictive distribution over the function $f(x)$ at a dense grid of test points (`new_xs`) given observations (`xs` and `ys`).\n",
        "\n",
        "You can tweak the kernel while debugging or answer Problem 1c, but please reset to length scale $\\ell = 5$ and variance $\\sigma^2 4$ before submitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4sbHuFXZYDA"
      },
      "outputs": [],
      "source": [
        "T = 100\n",
        "mean_func = lambda xs: torch.zeros_like(xs)\n",
        "kernel = lambda x1s, x2s: 1 * torch.exp(\n",
        "    -0.5 * ((x1s[:, None] - x2s[None, :]) / 5)**2)\n",
        "\n",
        "xs = torch.tensor([10, 25, 35, 45, 60, 90], dtype=torch.float32)\n",
        "fs = torch.tensor([-1.0, 1.0, 1.5, -0.5, 0.0, -0.5], dtype=torch.float32)\n",
        "new_xs = torch.linspace(0, T, T+1)\n",
        "\n",
        "pred_mean, pred_cov = compute_gp_predictions(xs, fs, new_xs, mean_func, kernel)\n",
        "pred_std = torch.sqrt(torch.diag(pred_cov))\n",
        "\n",
        "# Plot the predictive mean and the marginal predictive variance\n",
        "plt.plot(new_xs, pred_mean, '-r', lw=3, label=\"predictive mean\")\n",
        "for i in range(1, 3):\n",
        "    label = \"posterior credible intervals\" if i == 1 else None\n",
        "    plt.fill_between(new_xs, \n",
        "                    pred_mean - i * pred_std, \n",
        "                    pred_mean + i * pred_std,\n",
        "                    color='r', alpha=0.1,\n",
        "                    label=label)\n",
        "\n",
        "plt.plot(xs, fs, 'ko', markersize=8, label=\"observations\")\n",
        "\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.title(\"Posterior predictive distribution under the GP\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z_Wmjd74b59I"
      },
      "source": [
        "### Problem 1c [Short answer] Playing with kernel hyperparameters\n",
        "\n",
        "Describe how your predictions change when you vary the length scale or the variance of the squared exponential kernel. \n",
        "\n",
        "How do you think your answers would change if you instead used a Matern kernel with the same length scale and variance, but set $\\nu = 1/2$?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfi0PIbFdWDM"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M4iBP8-Ev_J8"
      },
      "source": [
        "### Problem 1d [Code]: GP Probit Classification\n",
        "\n",
        "Now we will write a simple Gibbs sampling algorithm for GP classification using a probit mean function.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f &\\sim \\mathrm{GP}(\\mu(\\cdot), \\, K(\\cdot, \\cdot)) \\\\\n",
        "y_n \\mid f, x_n &\\sim \\mathrm{Bern}(g(f(x_n)))\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $g(u) = \\Pr(z \\leq u)$ with $z \\sim \\mathcal{N}(0, 1))$.\n",
        "\n",
        "First, we've written some code that generates synthetic data. \n",
        "\n",
        "_Note: this code relies on your solutions to Problems 1a and 1b!_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bqaYi4g2421"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(305+ord('c'))\n",
        "\n",
        "mean_func = lambda xs: torch.zeros_like(xs)\n",
        "kernel = lambda x1s, x2s: 4 * torch.exp(\n",
        "    -0.5 * ((x1s[:, None] - x2s[None, :]) / 5)**2)\n",
        "\n",
        "# Sample the GP at a random set of N points in [0, 100]\n",
        "T = 100\n",
        "N = 40\n",
        "xs = Uniform(0, T).sample((N,))\n",
        "fs_true = sample_gp(xs, mean_func, kernel)\n",
        "\n",
        "# Sample observations from Bernoulli \n",
        "ys = Bernoulli(probit(fs_true)).sample()\n",
        "\n",
        "# For visualization, compute the predictive mean at a grid of points\n",
        "grid = torch.linspace(0, T, T+1)\n",
        "f_grid_true, _ = compute_gp_predictions(xs, fs_true, grid, mean_func, kernel)\n",
        "\n",
        "# Plot the probit of the true GP and the binary observations\n",
        "plt.plot(grid, probit(f_grid_true), '-k', lw=3, label=\"g(f(x))\")\n",
        "plt.plot(xs[ys==1], torch.ones_like(xs[ys==1]), 'r+', label=\"y=1\", mew=3)\n",
        "plt.plot(xs[ys==0], torch.zeros_like(xs[ys==0]), 'bo', label=\"y=0\", mew=3)\n",
        "plt.plot([0,T], [0, 0], '-k', lw=1)\n",
        "plt.plot([0,T], [1, 1], '-k', lw=1)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"g(f(x))\")\n",
        "plt.xlim(0, T)\n",
        "plt.ylim(-0.05, 1.05)\n",
        "plt.legend(loc=\"lower right\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xAHd7WMD5o0o"
      },
      "source": [
        "Now you will write code to perform Gibbs sampling in this model.\n",
        "\n",
        "We will use the augmentation scheme described in class. As we derived in class, the model above is equivalent to,\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f &\\sim \\mathrm{GP}(\\mu(\\cdot), \\, K(\\cdot, \\cdot)) \\\\\n",
        "z_n &\\sim \\mathcal{N}(f(x_n), 1) \\\\\n",
        "y_n &= \\mathbb{I}[z_n > 0].\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Using this augmented model, we can perform Bayesian inference by Gibbs sampling.\n",
        "\n",
        "Remember that, technically, $f$ is a function that has values at a continuum of points. Thankfully, we don't have to instantiate $f$ everywhere. For the Gibbs sampler, it suffices to instantiate $f$ only at the input points $f_n = f(x_n)$. That means the state of our Gibbs sampler will consist of the tuples $\\{(f_n, z_n)\\}_{n=1}^N$, and we will iteratively sample the following conditional distributions, \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "z_n &\\sim p(z_n \\mid f_n, y_n) \\\\\n",
        "\\mathbf{f} = (f_1, \\ldots, f_N)^\\top &\\sim p(\\mathbf{f} \\mid \\{x_n, z_n\\}_{n=1}^N)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Write code to implement each of these Gibbs updates. You may use the `OneSidedTruncatedNormal` distribution implemented at the top of this notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjczL5Zay9z-"
      },
      "outputs": [],
      "source": [
        "def gibbs_sample_zs(fs, ys):\n",
        "    \"\"\"\n",
        "    Perform a Gibbs step to sample (z_1, \\ldots, z_N) from their conditional\n",
        "    distribution given the function value at that point f_n = f(x_n) and the\n",
        "    binary observation y_n.\n",
        "\n",
        "    Args:\n",
        "        fs: shape (N,) tensor of function evaluations at each input x_n\n",
        "        ys: shape (N,) tensor of binary observations y_n\n",
        "\n",
        "    Returns:\n",
        "        zs: shape (N,) tensor of augmentation variables z_n sampled from\n",
        "            their conditional distribution.\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    zs = ...\n",
        "    ##\n",
        "    return zs\n",
        "\n",
        "def gibbs_sample_fs(xs, zs, mean_func, kernel):\n",
        "    \"\"\"\n",
        "    Sample Gaussian process values (f_1, ..., f_N) given inputs (x_1, ... x_N)\n",
        "    and augmentation variables (z_1, ..., z_N). After augmentation, this reduces\n",
        "    to GP Regression (see Lecture 15, Slide 17).\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    fs = ...\n",
        "    ##\n",
        "    return fs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jd_8mIA1yGm"
      },
      "source": [
        "#### Test your Gibbs sampler\n",
        "We have written a Gibbs loop to test your sampler on the synthetic data generated above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObW3GY0f2DBR"
      },
      "outputs": [],
      "source": [
        "def gibbs(xs, ys, mean_func, kernel, num_samples=1000):\n",
        "    \"\"\"Simple function to iteratively update z and f.\n",
        "    \"\"\"\n",
        "    assert xs.ndim == 1\n",
        "    assert ys.ndim == 1\n",
        "\n",
        "    # Initialize the sampler with f_n = 0 and z_n = 0\n",
        "    # (zs will immediately be overwritten anyway)\n",
        "    fs = torch.zeros_like(xs)\n",
        "    zs = torch.zeros_like(xs)\n",
        "\n",
        "    samples = []\n",
        "    for itr in trange(num_samples):\n",
        "        zs = gibbs_sample_zs(fs, ys)\n",
        "        fs = gibbs_sample_fs(xs, zs, mean_func, kernel)\n",
        "        samples.append((zs, fs))\n",
        "\n",
        "    zs, fs = list(zip(*samples))\n",
        "    return torch.row_stack(zs), torch.row_stack(fs)\n",
        "\n",
        "# Run the Gibbs sampler\n",
        "z_samples, f_samples = gibbs(xs, ys, mean_func, kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TKQRNtZ8SlI"
      },
      "outputs": [],
      "source": [
        "# Compute GP predictions on the grid\n",
        "f_grid_samples = torch.row_stack([\n",
        "    compute_gp_predictions(xs, fs, grid, mean_func, kernel)[0]\n",
        "    for fs in f_samples\n",
        "])\n",
        "\n",
        "# Plot the results\n",
        "burnin = 100\n",
        "prob_samples = probit(f_grid_samples)\n",
        "\n",
        "# Compute the posterior median probability at each point on the grid\n",
        "med_prob = torch.quantile(prob_samples[burnin:], q=.50, dim=0)\n",
        "plt.plot(grid, med_prob, '-r', lw=2, label=\"post. median g(f(x))\")\n",
        "\n",
        "# Compute and plot posterior quantiles for each point on the grid\n",
        "for lb, ub in [(.25, .75), (0.05, 0.95), (.025, .975)]:\n",
        "    prob_lb = torch.quantile(prob_samples[burnin:], q=lb, dim=0)\n",
        "    prob_ub = torch.quantile(prob_samples[burnin:], q=ub, dim=0)\n",
        "    plt.fill_between(grid, prob_lb, prob_ub, color='r', alpha=0.1)\n",
        "\n",
        "# Plot the true function and binary observations\n",
        "plt.plot(grid, probit(f_grid_true), '-k', lw=2, label=\"true g(f(x))\")\n",
        "plt.plot(xs[ys==1], torch.ones_like(xs[ys==1]), 'r+', label=\"y=1\", mew=3)\n",
        "plt.plot(xs[ys==0], torch.zeros_like(xs[ys==0]), 'bo', label=\"y=0\", mew=3)\n",
        "\n",
        "# Plot the bounds\n",
        "plt.plot([0,T], [0, 0], '-k', lw=1)\n",
        "plt.plot([0,T], [1, 1], '-k', lw=1)\n",
        "\n",
        "# Labels and stuff\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"g(f(x))\")\n",
        "plt.xlim(0, T)\n",
        "plt.ylim(-0.05, 1.05)\n",
        "plt.legend(loc=\"lower right\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tOOJEw4RbzdG"
      },
      "source": [
        "## Part 2: Poisson processes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SlUc41Y5ehhJ"
      },
      "source": [
        "### Problem 2a [Code]: Write a function to sample a homogeneous Poisson process \n",
        "\n",
        "There are many ways to do this, like we saw in Lecture 16. Use the top-down method for simplicity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPcXJW4MeoYI"
      },
      "outputs": [],
      "source": [
        "def sample_homog_pp(T, intensity):\n",
        "    \"\"\"\n",
        "    Sample a homogenous Poisson process on [0, T] with intensity lambda.\n",
        "\n",
        "    Args:\n",
        "        T: scalar length of interval\n",
        "        intensity: scalar homogeneous intensity\n",
        "\n",
        "    Returns:\n",
        "        xs: (N,) tensor of times in [0, T] where N is random\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    xs = ...\n",
        "    #\n",
        "    ##\n",
        "    return xs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BL3HTjm0gQVp"
      },
      "source": [
        "#### Plot one of your samples\n",
        "\n",
        "Run the cell below to plot one of your samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2sl4bLHewRa"
      },
      "outputs": [],
      "source": [
        "def plot_pp(xs, times, rates, height=0.05):\n",
        "    \"\"\"Helper function to plot Poisson process.\n",
        "    \"\"\"\n",
        "    N = len(xs)\n",
        "    print(\"number of points: \", N)\n",
        "    print(\"expected number of points: \", torch.trapz(rates, times))\n",
        "\n",
        "    plt.plot(times, rates, '-k', lw=1)\n",
        "    plt.fill_between(times, torch.zeros_like(rates), rates, \n",
        "                     color='k', alpha=0.1)\n",
        "    for x in xs:\n",
        "        plt.plot([x, x], [0, height], '-k', lw=2)\n",
        "        plt.plot([x], [height], 'ko', ms=8)\n",
        "    \n",
        "    plt.xlim(times[0], times[-1])\n",
        "    plt.ylim(0, 1.1 * rates.max())\n",
        "    plt.xlabel(\"time\")\n",
        "    plt.ylabel(\"rate\")\n",
        "\n",
        "# Sample a homogenous Poisson process on [0, 100] with intensity .1\n",
        "torch.manual_seed(305 + ord('c'))\n",
        "T = 100\n",
        "intensity = 0.1\n",
        "xs = sample_homog_pp(T, intensity)\n",
        "plot_pp(xs, torch.tensor([0, T]), torch.tensor([intensity, intensity]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lTtkW5CVgaTz"
      },
      "source": [
        "### Problem 2b [Code]: Sample an inhomogeneous Poisson process by thinning\n",
        "\n",
        "Write a function to sample an inhomogeneous Poisson process via thinning. Assume the intensity function is upper bounded by a constant $\\lambda_{\\mathsf{max}}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMubzrhmhAy2"
      },
      "outputs": [],
      "source": [
        "def sample_pp_thinning(T, intensity_func, max_intensity):\n",
        "    \"\"\"Sample a Poisson process via thinning.\n",
        "\n",
        "    Args:\n",
        "        T: length of time interval\n",
        "\n",
        "        intensity_func: function that takes in a tensor of times in [0,T] and\n",
        "            outputs a tensor of intensities evaluated at those times and\n",
        "            in the range [0, max_intensity].\n",
        "\n",
        "        max_intensity: upper bound on the intensity function.\n",
        "\n",
        "    Returns:\n",
        "        xs: (N,) tensor of times in [0, T] distributed according to the \n",
        "            inhomogeneous Poisson process.\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    xs = ...\n",
        "    #\n",
        "    ##\n",
        "    return xs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sfw3ERMch6HA"
      },
      "source": [
        "#### Test your function\n",
        "\n",
        "Sample from an inhomogeneous Poisson process an exponentiated sinusoidal intensity function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWCWraBsiQMD"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(305 + ord('c'))\n",
        "T = 100\n",
        "intensity_func = lambda ts: 0.25 * torch.exp(\n",
        "    2 * torch.sin(2 * torch.pi * ts / 20.0))\n",
        "max_intensity = 0.25 * torch.exp(torch.tensor(2))\n",
        "xs = sample_pp_thinning(T, intensity_func, max_intensity)\n",
        "\n",
        "ts = torch.linspace(0, T, T+1)\n",
        "intensities = intensity_func(ts)\n",
        "plot_pp(xs, ts, intensities, height=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aTeWSxpOjwtt"
      },
      "source": [
        "## Part 3: Sigmoidal Gaussian Cox Processes\n",
        "\n",
        "A sigmoidal Gaussian Cox Process (SGCP, [Adams et al, 2009](https://homepages.inf.ed.ac.uk/imurray2/pub/09poisson/adams-murray-mackay-2009b.pdf)) is a doubly stochastic point process with intensity,\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\lambda(x) = g(f(x))\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f &\\sim \\mathrm{GP}(\\mu(\\cdot), K(\\cdot, \\cdot))\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "and $g: \\mathbb{R} \\mapsto \\mathbb{R}_+$ is a sigmoidal function. Adams et al took $g$ to be a scaled logistic function, but we will consider a scaled **probit function** instead. That is, assume,\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "g(u) = \\lambda_{\\mathsf{max}}  \\cdot \\Pr(z \\leq u) \\quad \\text{where} \\quad z \\sim \\mathcal{N}(0, 1).\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "In this part of the assignment, you will write code to perform Gibbs sampling in an SGCP."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0cHBckpHl2ZL"
      },
      "source": [
        "### Problem 3a [Code]: Write a function to sample an SCGP\n",
        "\n",
        "You may use the functions you wrote for Parts 1 and 2 as well as the `probit` helper function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9imyWFyt3zW"
      },
      "outputs": [],
      "source": [
        "def sample_scgp(T, grid, mean_func, kernel, max_intensity):\n",
        "    \"\"\"\n",
        "    Sample a sigmoidal Gaussian Cox process.\n",
        "\n",
        "    Args:\n",
        "        T: the length of the interval\n",
        "        grid: grid of (M,) times at which to return the value of the sampled GP\n",
        "        mean_func: function that takes in an (N,) tensor of xs and outputs an \n",
        "            (N,) tensor of the means E[f(x)] for each x\n",
        "        kernel: function that takes in (N,) tensor of xs and (M,) tensor of x's \n",
        "            and outputs a (N, M) tensor of the kernel evaluated at each pair \n",
        "            (x, x'). E.g. if the input tensors are the same, this function \n",
        "            computes the Gram matrix.\n",
        "        max_intensity: the maximum intensity (\\lambda_max)\n",
        "\n",
        "    Returns:\n",
        "        grid_intensity: the sampled intensity evaluated at each time in grid\n",
        "        xs: a set of points drawn from the SCGP\n",
        "    \"\"\"\n",
        "    ## \n",
        "    # YOUR CODE BELOW\n",
        "    \n",
        "    # 1. Sample a homogenous Poisson process. CA\n",
        "    ts = ...\n",
        "    # 2. Sample the GP from its predictive distribution at \n",
        "    #    the points drawn from the Poisson process\n",
        "    fts = ...\n",
        "    # 3. Accept or reject points randomly to get xs\n",
        "    xs = ...\n",
        "    # 4. Sample the GP on the grid given (ts, fts)\n",
        "    f_grid = ...\n",
        "    # 5. Evaluate the intensity on the grid\n",
        "    grid_intensity = ...    \n",
        "    #\n",
        "    ##\n",
        "    return grid_intensity, xs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP8UDwKexCDl"
      },
      "outputs": [],
      "source": [
        "# Initialize params\n",
        "torch.manual_seed(305+ord('c'))\n",
        "mean_func = lambda xs: torch.zeros_like(xs)\n",
        "kernel = lambda x1s, x2s: 1 * torch.exp(\n",
        "    -0.5 * ((x1s[:, None] - x2s[None, :]) / 5)**2)\n",
        "max_intensity = 2.0\n",
        "\n",
        "# Initialize domain\n",
        "T = 100\n",
        "grid = torch.linspace(0, T, T+1)\n",
        "grid_intensity, xs = sample_scgp(T, grid, mean_func, kernel, max_intensity)\n",
        "\n",
        "# Plot the sample\n",
        "plot_pp(xs, grid, grid_intensity, height=1)\n",
        "plt.ylim(0, max_intensity + .05)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mxYz6H1OqIev"
      },
      "source": [
        "### Problem 3b [Code]: Comment the Gibbs sampling code below \n",
        "\n",
        "From Parts 1 and 2, you already have all the functions you need to implement a Gibbs sampler for the sigmoidal Guassian Cox process! \n",
        "\n",
        "Note that in the `sample_scgp` function you rejected (thinned) a bunch of points randomly by sampling from a Bernoulli distribution with probability $g(f(x))$ where $g$ was the probit function. We can think of those accept/reject outcomes as binary latent variables. If we knew the locations of the rejected points, then inferring the GP would reduce to a GP Probit Classifcation problem, just like we implemented in Problem 1d.  \n",
        "\n",
        "This motivates the following Gibbs sampling algorithm. The state of the Gibbs sampler will consist of the following latent variables:\n",
        "- $(r_1, \\ldots, r_M)$ the set of points that were rejected by the Poisson thinning algorithm. \n",
        "- $(z_1, \\ldots, z_{N+M})$ the augmentation variables for the GP Classification problem at both the observed points _and_ the rejected points. \n",
        "- $(f_1, \\ldots, f_{N+M})$ the GP function values at both the observed points _and_ the rejected points.\n",
        "\n",
        "The trick is, at each iteration of the Gibbs sampler we will generate a new set of rejected points $(r_1, \\ldots, r_M)$. The conditional distribution of these points is itself a Poisson process!\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\{r_m\\}_{m=1}^M \\mid f &\\sim \\mathrm{PP}(\\lambda_{\\mathsf{max}} - g(f(x))).\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "How do you sample that conditional distribution? Poisson thinning again!\n",
        "\n",
        "Rather than implementing this yourself, we've written code to do so. Your assignment is to **comment the code below (function headers and line comments) to explain what it does.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZA2s46ry_f0"
      },
      "outputs": [],
      "source": [
        "# COMMENT THE FUNCTIONS AND LINES BELOW\n",
        "\n",
        "def gibbs_update_rs(xs, fxs, rs, frs, \n",
        "                    mean_func, kernel, max_intensity):\n",
        "    \"\"\"\n",
        "    TODO\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    ts = sample_homog_pp(T, max_intensity)\n",
        "    # TODO\n",
        "    mu, Sigma = compute_gp_predictions(\n",
        "        torch.cat([xs, rs]), torch.cat([fxs, frs]), ts, \n",
        "        mean_func, kernel)\n",
        "    # TODO\n",
        "    fts = MultivariateNormal(mu, Sigma).sample()\n",
        "    # TODO\n",
        "    lambda_ts = max_intensity * (1 - probit(fts))\n",
        "    # TODO\n",
        "    accept = Uniform(0, max_intensity).sample((len(ts),)) < lambda_ts\n",
        "    return ts[accept], fts[accept]\n",
        "\n",
        "def gibbs(xs, T, grid, mean_func, kernel, max_intensity, num_samples=2000):\n",
        "    \"\"\"\n",
        "    TODO\n",
        "    \"\"\"\n",
        "    assert xs.ndim == 1\n",
        "    N = len(xs)\n",
        "\n",
        "    # TODO\n",
        "    fxs = torch.zeros_like(xs)\n",
        "    rs = torch.tensor([])\n",
        "    frs = torch.zeros_like(rs)\n",
        "    M = len(rs)\n",
        "\n",
        "    f_grid_samples = []\n",
        "    for itr in trange(num_samples):\n",
        "        # TODO\n",
        "        rs, frs = gibbs_update_rs(xs, fxs, rs, frs, \n",
        "                                  mean_func, kernel, max_intensity)\n",
        "        M = len(rs)\n",
        "\n",
        "        # TODO\n",
        "        zs = gibbs_sample_zs(torch.cat([fxs, frs]), \n",
        "                             torch.cat([torch.ones(N), torch.zeros(M)]))\n",
        "\n",
        "        # TODO\n",
        "        fs = gibbs_sample_fs(torch.cat([xs, rs]), zs, mean_func, kernel)\n",
        "        fxs, frs = fs[:N], fs[N:]\n",
        "\n",
        "        # TODO\n",
        "        f_grid, _ = compute_gp_predictions(torch.cat([xs, rs]), \n",
        "                                           torch.cat([fxs, frs]), \n",
        "                                           grid, mean_func, kernel)\n",
        "        f_grid_samples.append(f_grid)\n",
        "        \n",
        "    f_grid_samples = torch.row_stack(f_grid_samples)\n",
        "    return f_grid_samples, rs\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X530Bta9A60G"
      },
      "source": [
        "#### Now let's see if it works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AOYdKhpIPCO"
      },
      "outputs": [],
      "source": [
        "# Run the Gibbs sampler. It may take a minute.\n",
        "torch.manual_seed(305+ord('c'))\n",
        "f_grid_samples, rs = gibbs(xs, T, grid, mean_func, kernel, max_intensity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1XHjKoJH_xi"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "burnin = 0\n",
        "intensity_samples = max_intensity * probit(f_grid_samples)\n",
        "\n",
        "# Compute the posterior median probability at each point on the grid\n",
        "med_prob = torch.quantile(intensity_samples[burnin:], q=.50, dim=0)\n",
        "plt.plot(grid, med_prob, '-r', lw=2, label=\"post. median g(f(x))\")\n",
        "\n",
        "# Compute and plot posterior quantiles for each point on the grid\n",
        "for lb, ub in [(.25, .75), (0.05, 0.95), (.025, .975)]:\n",
        "    intensity_lb = torch.quantile(intensity_samples[burnin:], q=lb, dim=0)\n",
        "    intensity_ub = torch.quantile(intensity_samples[burnin:], q=ub, dim=0)\n",
        "    plt.fill_between(grid, intensity_lb, intensity_ub, color='r', alpha=0.1)\n",
        "\n",
        "# Plot the true function and binary observations\n",
        "plt.plot(grid, grid_intensity, '-k', lw=2, label=\"true g(f(x))\")\n",
        "for x in xs:\n",
        "    plt.plot([x, x], [0, 1], '-ko', lw=1, ms=4)\n",
        "\n",
        "# Plot the bounds\n",
        "plt.plot([0,T], [0, 0], '-k', lw=1)\n",
        "plt.plot([0,T], max_intensity * torch.ones(2), ':k', lw=1)\n",
        "\n",
        "# Labels and stuff\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"\\lambda(x)\")\n",
        "plt.xlim(0, T)\n",
        "plt.ylim(-0.05, max_intensity + .05)\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.title(\"Posterior distribution of $\\lambda(x)$\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D-S341PsoI27"
      },
      "source": [
        "### Problem 3c [Short Answer]: Computational efficiency\n",
        "\n",
        "Answer the following questions:\n",
        "- You may have noticed that the number of iterations per second jumps around a bit during the course of sampling. What could cause that to happen?\n",
        "\n",
        "- Suppose that we set the max intensity to $\\lambda_{\\mathsf{max}} = 10$. How would that affect the run time of the Gibbs sampler and why?\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q2mKcMJ--tA_"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kCY8o1JV9oEi"
      },
      "source": [
        "### Bonus [math]: Gibbs updates for $\\lambda_{\\mathsf{max}}$\n",
        "\n",
        "Derive a closed form Gibbs update for $\\lambda_{\\mathsf{max}}$ given the remaining variables (including the latent variables like the rejected spikes, etc.)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9XcMzH_Q-8RQ"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pDJ8feGX-7b-"
      },
      "source": [
        "**Formatting:** check that your code does not exceed 80 characters in line width. If you're working in Colab, you can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
        "\n",
        "Download your notebook in .ipynb format and use the following commands to convert it to PDF:\n",
        "```\n",
        "jupyter nbconvert --to pdf hw8_yourname.ipynb\n",
        "```\n",
        "\n",
        "**Dependencies:**\n",
        "\n",
        "- `nbconvert`: If you're using Anaconda for package management, \n",
        "```\n",
        "conda install -c anaconda nbconvert\n",
        "```\n",
        "\n",
        "**Upload** your .pdf files to Gradescope. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xpxO2j5eTSqh"
      ],
      "name": "STAT 305C Assignment 8: Sigmoidal Gaussian Cox Process.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
