{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHdhDx3SfbNa"
      },
      "source": [
        "# Assignment 1: Bayesian Linear Regression\n",
        "STATS305C, Stanford University, Spring 2022\n",
        "\n",
        "**Your name:**\n",
        "\n",
        "**Collaborators:**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-_CGkTvS9dH"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "import torch\n",
        "from torch.distributions import Normal, Gamma, \\\n",
        "    TransformedDistribution, MultivariateNormal\n",
        "from torch.distributions.transforms import PowerTransform\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_context(\"notebook\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6k2BC4ZeFpS"
      },
      "source": [
        "## Bayesian Linear Regression\n",
        "\n",
        "STATS 305A was all about linear regression. In this assignment, you'll revisit that classic model from a Bayesian perspective.\n",
        "\n",
        "Let $\\{\\mathbf{x}_n, y_n\\}_{n=1}^N$ denote a dataset with covariates $\\mathbf{x}_n \\in \\mathbb{R}^D$ and scalar outcomes $y_n \\in \\mathbb{R}$. Let $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$ denote the design matrix where each row is a vector of covariates and $\\mathbf{y} \\in \\mathbb{R}^N$ denote the vector of outcomes.\n",
        "\n",
        "We will model the outcomes as conditionally independent Gaussian random variables given the covariates and the parameters,\n",
        "\\begin{align}\n",
        "p(\\mathbf{y} \\mid \\mathbf{w}, \\sigma^2, \\mathbf{X})\n",
        "&= \\prod_{n=1}^N \\mathcal{N}(y_n \\mid \\mathbf{w}^\\top \\mathbf{x}_n, \\sigma^2),\n",
        "\\end{align}\n",
        "where $\\mathbf{w} \\in \\mathbb{R}^D$ are the _weights_ and $\\sigma^2 \\in \\mathbb{R}_+$ is the conditional variance. \n",
        "\n",
        "In a _Bayesian_ linear regression, we place a prior on the parameters,\n",
        "\\begin{align}\n",
        "p(\\mathbf{w}, \\sigma^2) &=\n",
        "\\chi^{-2}(\\sigma^2 \\mid \\nu_0, \\sigma_0^2) \\,\n",
        "\\mathcal{N}(\\mathbf{w} \\mid \\boldsymbol{\\mu}_0, \\sigma^2 \\boldsymbol{\\Lambda}_0^{-1}),\n",
        "\\end{align}\n",
        "where $\\nu_0 \\in \\mathbb{R}_+$ sets the degrees of freedom, $\\sigma_0^2$ is the prior mean of the variance parameter, $\\boldsymbol{\\mu}_0$ is the prior mean of the weights, and $\\boldsymbol{\\Lambda}_0 \\in \\mathcal{S}_D$ is a positive definite $D \\times D$ precision matrix.\n",
        "\n",
        "## PyTorch \n",
        "\n",
        "As in the notebooks from class, you will use PyTorch to complete the coding portions of this assignment. If you are unfamiliar with PyTorch, [this](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html) webpage provides an introductory tutorial to PyTorch tensors. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: Derive the Posterior [Math]\n",
        "\n",
        "Derive the posterior distribution $p(\\mathbf{w}, \\sigma^2 \\mid \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta})$ where $\\boldsymbol{\\eta} = (\\nu_0, \\sigma_0^2, \\boldsymbol{\\mu}_0, \\boldsymbol{\\Lambda}_0)$. It should be of the same form as the prior,\n",
        "\n",
        "\\begin{align}\n",
        "p(\\mathbf{w}, \\sigma^2 \\mid \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta}) \n",
        "&= \\chi^{-2}(\\sigma^2 \\mid \\nu_N, \\sigma_N^2) \\mathcal{N}(\\mathbf{w} \\mid \\boldsymbol{\\mu}_N, \\sigma^2 \\boldsymbol{\\Lambda}_N^{-1})\n",
        "\\end{align}\n",
        "\n",
        "for some $\\nu_N$, $\\sigma_N^2$, $\\boldsymbol{\\mu}_N$, and $\\boldsymbol{\\Lambda}_N$. \n"
      ],
      "metadata": {
        "id": "SCoqcOs8RnS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JDrwyQmdR44o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: The Posterior Mean [Math]\n",
        "a. What does the posterior mean $\\mathbb{E}[\\mathbf{w} \\mid \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta}]$ equal in the uninformative limit where $\\boldsymbol{\\Lambda}_0 \\to 0$ and $\\nu_0 \\to 0$?\n",
        "\n",
        "b. What does the posterior mean $\\mathbb{E}[\\sigma^2 \\mid \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta}]$ equal in the uninformative limit where $\\boldsymbol{\\Lambda}_0 \\to 0$ and $\\nu_0 \\to 0$? Write your answer in terms of the _hat matrix_ $\\mathbf{H} = \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top$. "
      ],
      "metadata": {
        "id": "tYbTI5tcocJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "RJDQjtrhoubw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthetic Data\n",
        "\n",
        "We'll do some simple analysis of a synthetic dataset with $N =20$ data points. Each data point has covariates $\\mathbf{x}_n = (1, x_n) \\in \\mathbb{R}^2$ and scalar outcomes $y_n \\in \\mathbb{R}$. It looks like this:"
      ],
      "metadata": {
        "id": "52LZXHq9yW4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://raw.githubusercontent.com/slinderman/stats305c/main/assignments/hw1/hw1.pt\n",
        "\n",
        "# Load the data.\n",
        "# X = [[1, x_1]\n",
        "#      [1, x_2]\n",
        "#         ...\n",
        "#      [1, x_N]]\n",
        "#\n",
        "# y = [y_1, ..., y_N]\n",
        "X, y = torch.load(\"hw1.pt\")\n",
        "\n",
        "plt.plot(X[:, 1], y, 'ko')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")"
      ],
      "metadata": {
        "id": "9rZ3PvD2yuki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the outcomes were simulated from a linear regression with Gaussian noise according to some true parameters (not given). You will compute and visualize the posterior distribution over the weights and variance given the data."
      ],
      "metadata": {
        "id": "HiagNPRRzXKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3: Compute the posterior [Code]\n",
        "\n",
        "Write a function to compute the posterior parameters given data and hyperparameters.\n",
        "\n",
        "*Hints*: You may find the following commands in PyTorch useful:\n",
        "- If ```a``` is a tensor, ```a.shape``` is a tuple containing the shape of ```a```.\n",
        "- If ```a``` is a tensor, ```a.T``` returns the transpose of ```a```.\n",
        "- ```torch.linalg.solve```\n",
        "- ```*``` denotes element-wise multiplication while ```@``` denotes standard matrix-matrix or matrix-vector multiplication."
      ],
      "metadata": {
        "id": "SI4tX9AGquoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_posterior(X, y, nu_0, sigmasq_0, mu_0, Lambda_0):\n",
        "    \"\"\"\n",
        "    Compute the posterior parameters nu_N, sigmasq_N, mu_N, and Lambda_N \n",
        "    given covariates X, outcomes y, and hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        X:          (N, D) tensor of covariates\n",
        "        y:          (N,) tensor of outcomes\n",
        "        nu_0:       prior degrees of freedom\n",
        "        sigmasq_0:  prior mean of the variance parameter\n",
        "        mu_0:       prior mean of the weights\n",
        "        Lambda_0:   prior precision of the weights\n",
        "\n",
        "    Returns:\n",
        "        nu_N:       posterior degrees of freedom\n",
        "        sigmasq_N:  posterior mean of the variance parameter\n",
        "        mu_N:       posterior mean of the weights\n",
        "        Lambda_N:   posterior precision of the weights\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    ##\n",
        "    \n",
        "    return nu_N, sigmasq_N, mu_N, Lambda_N"
      ],
      "metadata": {
        "id": "Ty4VpJVV67Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please run the following code to print your answers:"
      ],
      "metadata": {
        "id": "vXnTwuLHyAfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test:\n",
        "hypers = dict(\n",
        "    nu_0=torch.tensor(1.0),\n",
        "    sigmasq_0=torch.tensor(1.0),\n",
        "    mu_0=torch.zeros(2),\n",
        "    Lambda_0=0.1 * torch.eye(2)\n",
        ")\n",
        "\n",
        "nu_N, sigmasq_N, mu_N, Lambda_N = compute_posterior(X, y, **hypers)\n",
        "print(\"nu_N:       \\n\", nu_N)\n",
        "print(\"\")\n",
        "print(\"sigmasq_N:  \\n\", sigmasq_N)\n",
        "print(\"\")\n",
        "print(\"mu_N:       \\n\", mu_N)\n",
        "print(\"\")\n",
        "print(\"Lambda_N:   \\n\", Lambda_N)"
      ],
      "metadata": {
        "id": "8bTSbgTNu5AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4: Plot the posterior density of the variance [Code]\n",
        "\n",
        "Plot $p(\\sigma^2 \\mid X, y, \\eta)$ vs $\\sigma^2$ over the interval $[10^{-3}, 2]$. \n",
        "\n",
        "You may use the `ScaledInvChiSq` distribution object below, which we copied from the demo for Lecture 1.\n",
        "\n",
        "_Hint_: In Python, you can use `dir(object)` to list the attributes and functions that an object supports. \n",
        "\n",
        "_Hint_: To learn more about PyTorch distributions, see the [docs](https://pytorch.org/docs/stable/distributions.html)."
      ],
      "metadata": {
        "id": "qCAPSDTFz1KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledInvChiSq(TransformedDistribution):\n",
        "    \n",
        "    def __init__(self, dof, scale):\n",
        "        \"\"\"\n",
        "        Implementation of the scaled inverse \\chi^2 distribution,\n",
        "        \n",
        "        ..math:\n",
        "            \\chi^{-2}(\\nu_0, \\sigma_0^2)\n",
        "\n",
        "        It is equivalent to an inverse gamma distribution, which we implement\n",
        "        as a transformation of a Gamma distribution. Thus, this class inherits\n",
        "        functions like `log_prob` from its parent.\n",
        "\n",
        "        Args:\n",
        "            dof:   degrees of freedom parameter\n",
        "            scale: mean of the $\\chi^{-2}$ distribution.\n",
        "        \"\"\"\n",
        "        base = Gamma(dof / 2, dof * scale / 2)\n",
        "        transforms = [PowerTransform(-1)]\n",
        "        TransformedDistribution.__init__(self, base, transforms)\n",
        "        self.dof = dof\n",
        "        self.scale = scale        "
      ],
      "metadata": {
        "id": "E1ayM_2pz6z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "# YOUR CODE HERE\n",
        "#\n",
        "##"
      ],
      "metadata": {
        "id": "VoO2dZZxBaVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 5: Plot posterior samples of the regression function. [Code]\n",
        "Draw 50 samples from the posterior marginal distribution over the weights $\\mathbf{w} \\in \\mathbb{R}^2$. For each sample, compute the expected value of $y$ on a grid of points $x$ evenly spaced between $[-3, 3]$. Remember that our covariates were defined as $\\mathbf{x} = (1, x)$ so that for each sample of the weights you get a line for $\\mathbb{E}[y \\mid x, \\mathbf{w}]$ as a function of $x$. Plot these 50 lines on top of each other to get a sense of the posterior uncertainty in the regression function. (You may want to plot each line with some transparency, like `alpha=0.1`.) Overlay the observed data points. \n",
        "\n",
        "*Hint*: You may find ```torch.inverse``` useful."
      ],
      "metadata": {
        "id": "epW71f9x03sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "# YOUR CODE HERE\n",
        "#\n",
        "##"
      ],
      "metadata": {
        "id": "ECdb34A401Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus: Posterior Predictive Distribution [Math]\n",
        "Derive the posterior predictive distribution of the outcome at a new input $\\mathbf{x}_{N+1}$. That is, compute,\n",
        "\\begin{align}\n",
        "p(y_{N+1} \\mid x_{N+1}, \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta})\n",
        "\\end{align}\n",
        "integrating over the posterior distribution on the weights $\\mathbf{w}$ and variance $\\sigma^2$.\n"
      ],
      "metadata": {
        "id": "gcl9zCRs4YHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VxHvY2tk5I_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission Instructions\n",
        "\n",
        "\n",
        "**Formatting:** check that your code does not exceed 80 characters in line width. You can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
        "\n",
        "Download your notebook in .ipynb format and use the following commands to convert it to PDF.  Then run the following command to convert to a PDF:\n",
        "```\n",
        "jupyter nbconvert --to pdf <yourname>_hw1.ipynb\n",
        "```\n",
        "\n",
        "\n",
        "**Installing nbconvert:**\n",
        "\n",
        "If you're using Anaconda for package management, \n",
        "```\n",
        "conda install -c anaconda nbconvert\n",
        "```\n",
        "\n",
        "**Upload** your .ipynb and .pdf files to Gradescope. "
      ],
      "metadata": {
        "id": "hM9vMLLP9J1n"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "b6k2BC4ZeFpS"
      ],
      "name": "STATS305C Assignment 1: Bayesian Linear Regression",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}