{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHdhDx3SfbNa"
      },
      "source": [
        "# HW1: Bayesian Linear Regression\n",
        "STATS305C, Stanford University, Spring 2023\n",
        "\n",
        "**Your name:**\n",
        "\n",
        "**Collaborators:**\n",
        "\n",
        "**Hours spent:**\n",
        "\n",
        "Please let us know how many hours in total you spent on this assignment so we can calibrate for future assignments. Your feedback is always welcome!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-_CGkTvS9dH"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "import torch\n",
        "from torch.distributions import Normal, Gamma, \\\n",
        "    TransformedDistribution, MultivariateNormal\n",
        "from torch.distributions.transforms import PowerTransform\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_context(\"notebook\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b6k2BC4ZeFpS"
      },
      "source": [
        "## Bayesian Linear Regression\n",
        "\n",
        "STATS 305A was all about linear regression. In this assignment, you'll revisit that classic model from a Bayesian perspective.\n",
        "\n",
        "Let $\\{\\mathbf{x}_n, y_n\\}_{n=1}^N$ denote a dataset with covariates $\\mathbf{x}_n \\in \\mathbb{R}^D$ and scalar outcomes $y_n \\in \\mathbb{R}$. Let $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$ denote the design matrix where each row is a vector of covariates and $\\mathbf{y} \\in \\mathbb{R}^N$ denote the vector of outcomes.\n",
        "\n",
        "We will model the outcomes as conditionally independent Gaussian random variables given the covariates and the parameters,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(\\mathbf{y} \\mid \\mathbf{w}, \\sigma^2, \\mathbf{X})\n",
        "&= \\prod_{n=1}^N \\mathcal{N}(y_n \\mid \\mathbf{w}^\\top \\mathbf{x}_n, \\sigma^2),\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{w} \\in \\mathbb{R}^D$ are the _weights_ and $\\sigma^2 \\in \\mathbb{R}_+$ is the conditional variance.\n",
        "\n",
        "In a _Bayesian_ linear regression, we place a prior on the parameters,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(\\mathbf{w}, \\sigma^2 | \\boldsymbol{\\eta}) &=\n",
        "\\chi^{-2}(\\sigma^2 \\mid \\nu_0, \\sigma_0^2) \\,\n",
        "\\mathcal{N}(\\mathbf{w} \\mid \\boldsymbol{\\mu}_0, \\sigma^2 \\boldsymbol{\\Lambda}_0^{-1}),\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\nu_0 \\in \\mathbb{R}_+$ sets the degrees of freedom, $\\sigma_0^2$ is the prior mean of the variance parameter, $\\boldsymbol{\\mu}_0$ is the prior mean of the weights, and $\\boldsymbol{\\Lambda}_0 \\in \\mathcal{S}_D$ is a positive definite $D \\times D$ precision matrix.\n",
        "We collect these *hyperparameters* into the vector $\\boldsymbol{\\eta} = (\\nu_0, \\sigma_0^2, \\boldsymbol{\\mu}_0, \\boldsymbol{\\Lambda}_0)$.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## PyTorch \n",
        "\n",
        "As in the notebooks from class, you will use PyTorch to complete the coding portions of this assignment. If you are unfamiliar with PyTorch, [this](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html) webpage provides an introductory tutorial to PyTorch tensors. You should also make sure you can solve the problems in [homework 0](../hw0/hw0.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCoqcOs8RnS1"
      },
      "source": [
        "## Problem 1: Derive the Posterior [Math]\n",
        "\n",
        "Derive the posterior distribution $p(\\mathbf{w}, \\sigma^2 \\mid \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta})$ where $\\boldsymbol{\\eta} = (\\nu_0, \\sigma_0^2, \\boldsymbol{\\mu}_0, \\boldsymbol{\\Lambda}_0)$. It should be of the same form as the prior,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(\\mathbf{w}, \\sigma^2 \\mid \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta}) \n",
        "&= \\chi^{-2}(\\sigma^2 \\mid \\nu_N, \\sigma_N^2) \\mathcal{N}(\\mathbf{w} \\mid \\boldsymbol{\\mu}_N, \\sigma^2 \\boldsymbol{\\Lambda}_N^{-1})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "for some $\\nu_N$, $\\sigma_N^2$, $\\boldsymbol{\\mu}_N$, and $\\boldsymbol{\\Lambda}_N$. \n",
        "\n",
        "*Hint:* Remember that the \"standard procedure\" for deriving the posterior distribution is to write down the joint distribution (on both parameters and data), and then to only foucs on the terms you care about (the parameters). But, in this setting, you have to be very careful to keep both $\\mathbf{w}$ and $\\sigma^2$, because we are asking for the joint posterior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDrwyQmdR44o"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYbTI5tcocJM"
      },
      "source": [
        "## Problem 2: The Posterior Mean [Math]\n",
        "a. What does the posterior mean $\\mathbb{E}[\\mathbf{w} \\mid \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta}]$ equal in the uninformative limit where $\\boldsymbol{\\Lambda}_0 \\to 0$ and $\\nu_0 \\to 0$?\n",
        "\n",
        "b. What does the posterior mean $\\mathbb{E}[\\sigma^2 \\mid \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta}]$ equal in the uninformative limit where $\\boldsymbol{\\Lambda}_0 \\to 0$ and $\\nu_0 \\to 0$? Write your answer in terms of the _hat matrix_ $\\mathbf{H} = \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJDQjtrhoubw"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52LZXHq9yW4o"
      },
      "source": [
        "## Synthetic Data\n",
        "\n",
        "We'll do some simple analysis of a synthetic dataset with $N =20$ data points. Each data point has covariates $\\mathbf{x}_n = (1, x_n) \\in \\mathbb{R}^2$ and scalar outcomes $y_n \\in \\mathbb{R}$. It looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rZ3PvD2yuki"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://raw.githubusercontent.com/slinderman/stats305c/main/assignments/hw1/hw1.pt\n",
        "\n",
        "# Load the data.\n",
        "# X = [[1, x_1]\n",
        "#      [1, x_2]\n",
        "#         ...\n",
        "#      [1, x_N]]\n",
        "#\n",
        "# y = [y_1, ..., y_N]\n",
        "X, y = torch.load(\"hw1.pt\")\n",
        "\n",
        "plt.plot(X[:, 1], y, 'ko')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiagNPRRzXKD"
      },
      "source": [
        "Here, the outcomes were simulated from a linear regression with Gaussian noise according to some true parameters (not given). You will compute and visualize the posterior distribution over the weights and variance given the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI4tX9AGquoL"
      },
      "source": [
        "## Problem 3: Compute the posterior [Code]\n",
        "\n",
        "Write a function to compute the posterior parameters given data and hyperparameters.\n",
        "\n",
        "*Hints*: You may find the following commands in PyTorch useful:\n",
        "- If ```a``` is a tensor, ```a.shape``` is a tuple containing the shape of ```a```.\n",
        "- If ```a``` is a tensor, ```a.T``` returns the transpose of ```a```.\n",
        "- ```torch.linalg.solve```\n",
        "- ```*``` denotes element-wise multiplication while ```@``` denotes standard matrix-matrix or matrix-vector multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty4VpJVV67Kn"
      },
      "outputs": [],
      "source": [
        "def compute_posterior(X, y, nu_0, sigmasq_0, mu_0, Lambda_0):\n",
        "    \"\"\"\n",
        "    Compute the posterior parameters nu_N, sigmasq_N, mu_N, and Lambda_N \n",
        "    given covariates X, outcomes y, and hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        X:          (N, D) tensor of covariates\n",
        "        y:          (N,) tensor of outcomes\n",
        "        nu_0:       prior degrees of freedom\n",
        "        sigmasq_0:  prior mean of the variance parameter\n",
        "        mu_0:       prior mean of the weights\n",
        "        Lambda_0:   prior precision of the weights\n",
        "\n",
        "    Returns:\n",
        "        nu_N:       posterior degrees of freedom\n",
        "        sigmasq_N:  posterior scale of the variance parameter\n",
        "        mu_N:       posterior mean of the weights\n",
        "        Lambda_N:   posterior precision of the weights\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    ##\n",
        "    \n",
        "    return nu_N, sigmasq_N, mu_N, Lambda_N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXnTwuLHyAfR"
      },
      "source": [
        "Please run the following code to print your answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bTSbgTNu5AI"
      },
      "outputs": [],
      "source": [
        "# Test:\n",
        "hypers = dict(\n",
        "    nu_0=torch.tensor(1.0),\n",
        "    sigmasq_0=torch.tensor(1.0),\n",
        "    mu_0=torch.zeros(2),\n",
        "    Lambda_0=0.1 * torch.eye(2)\n",
        ")\n",
        "\n",
        "nu_N, sigmasq_N, mu_N, Lambda_N = compute_posterior(X, y, **hypers)\n",
        "print(\"nu_N:       \\n\", nu_N)\n",
        "print(\"\")\n",
        "print(\"sigmasq_N:  \\n\", sigmasq_N)\n",
        "print(\"\")\n",
        "print(\"mu_N:       \\n\", mu_N)\n",
        "print(\"\")\n",
        "print(\"Lambda_N:   \\n\", Lambda_N)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCAPSDTFz1KK"
      },
      "source": [
        "## Problem 4: Plot the posterior density of the variance [Code]\n",
        "\n",
        "Plot $p(\\sigma^2 \\mid X, y, \\eta)$ vs $\\sigma^2$ over the interval $[10^{-3}, 2]$, where $X$ and $y$ continue to be the synthetic data we downloaded and used in Problem 3.\n",
        "\n",
        "You may use the `ScaledInvChiSq` distribution object below, which we copied from the demo for Lecture 1.\n",
        "\n",
        "_Hint_: In Python, you can use `dir(object)` to list the attributes and functions that an object supports. \n",
        "\n",
        "_Hint_: To learn more about PyTorch distributions, see the [docs](https://pytorch.org/docs/stable/distributions.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1ayM_2pz6z_"
      },
      "outputs": [],
      "source": [
        "class ScaledInvChiSq(TransformedDistribution):\n",
        "    \n",
        "    def __init__(self, dof, scale):\n",
        "        \"\"\"\n",
        "        Implementation of the scaled inverse \\chi^2 distribution,\n",
        "        \n",
        "        ..math:\n",
        "            \\chi^{-2}(\\nu_0, \\sigma_0^2)\n",
        "\n",
        "        It is equivalent to an inverse gamma distribution, which we implement\n",
        "        as a transformation of a Gamma distribution. Thus, this class inherits\n",
        "        functions like `log_prob` from its parent.\n",
        "\n",
        "        Args:\n",
        "            dof:   degrees of freedom parameter\n",
        "            scale: scale of the $\\chi^{-2}$ distribution.\n",
        "        \"\"\"\n",
        "        base = Gamma(dof / 2, dof * scale / 2)\n",
        "        transforms = [PowerTransform(-1)]\n",
        "        TransformedDistribution.__init__(self, base, transforms)\n",
        "        self.dof = dof\n",
        "        self.scale = scale        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoO2dZZxBaVt"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# YOUR CODE HERE\n",
        "#\n",
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epW71f9x03sn"
      },
      "source": [
        "## Problem 5: Plot posterior samples of the regression function. [Code]\n",
        "Draw 50 samples from the posterior marginal distribution over the weights $\\mathbf{w} \\in \\mathbb{R}^2$. For each sample, compute the expected value of $y$ on a grid of points $x$ evenly spaced between $[-3, 3]$. Remember that our covariates were defined as $\\mathbf{x} = (1, x)$ so that for each sample of the weights you get a line for $\\mathbb{E}[y \\mid x, \\mathbf{w}]$ as a function of $x$. Plot these 50 lines on top of each other to get a sense of the posterior uncertainty in the regression function. (You may want to plot each line with some transparency, like `alpha=0.1`.) Overlay the observed data points. \n",
        "\n",
        "*Hint*: You may find ```torch.inverse``` useful.\n",
        "\n",
        "*Hint*: Remember that in the generative model we have posited, the distribution of $\\mathbf{w}$ depends on $\\sigma^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECdb34A401Kh"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# YOUR CODE HERE\n",
        "#\n",
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcl9zCRs4YHt"
      },
      "source": [
        "## Problem 6: Posterior Predictive Distribution [Math]\n",
        "The subparts of this problem will walk you through deriving the posterior predictive distribution of the outcome at a new input $\\mathbf{x}_{N+1}$. That is, computing,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(y_{N+1} \\mid x_{N+1}, \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "integrating over the posterior distribution on the weights $\\mathbf{w}$ and variance $\\sigma^2$.\n",
        "Remember that you found this posterior distribution in Problem 1, but for the purpose of this question it's enough to leave it in the form\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(\\mathbf{w}, \\sigma^2 \\mid \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta}) \n",
        "&= \\chi^{-2}(\\sigma^2 \\mid \\nu_N, \\sigma_N^2) \\mathcal{N}(\\mathbf{w} \\mid \\boldsymbol{\\mu}_N, \\sigma^2 \\boldsymbol{\\Lambda}_N^{-1}),\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "i.e. you don't need to plug in the values for for some $\\nu_N$, $\\sigma_N^2$, $\\boldsymbol{\\mu}_N$, and $\\boldsymbol{\\Lambda}_N$ that you found in Problem 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a11OeZxrqOBz"
      },
      "source": [
        "### Problem 6a\n",
        "\n",
        "Using the product rule of probability, write out the joint distribution of the posterior over the parameters and the observation of the next data point $y_{N+1}$:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  p(y_{N + 1}, \\mathbf{w}, \\sigma^2 | x_{N + 1}, \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta}).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "You can (and please do) replace any densities from a known family with the notation $\\text{symbol for the family}( \\text{variable name} | \\text{parameters})$. (We follow this notation in how we write out the posterior above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxHvY2tk5I_Q"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u-kOJ9TTss0S"
      },
      "source": [
        "### Problem 6b\n",
        "\n",
        "Now using the sum rule of probability, compute the posterior predictive distribution\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  p(y_{N + 1}, | x_{N + 1}, \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta}) = \\int  p(y_{N + 1}, \\mathbf{w}, \\sigma^2 | x_{N + 1}, \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\eta}) d\\mathbf{w} d\\sigma^2.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "*Hint:* You can do this integral without taking any integrals! Thinks about conjugate families and how the Student T distribution came up in lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVH5ZfdDtfjC"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hM9vMLLP9J1n"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "\n",
        "**Formatting:** check that your code does not exceed 80 characters in line width. You can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
        "\n",
        "Download your notebook in .ipynb format and use the following commands to convert it to PDF.  Then run the following command to convert to a PDF:\n",
        "```\n",
        "jupyter nbconvert --to pdf <yourlastname>_hw1.ipynb\n",
        "```\n",
        "(Note that for the above code to work, you need to rename your file `<yourlastname>_hw1.ipynb`)\n",
        "\n",
        "Possible causes for errors:\n",
        "  * the \"Open in colab\" button. Just delete the code that creates this button (go to the top cell and delete it)\n",
        "  * Latex errors: many latex errors aren't visible in the notebook. Try binary search: comment out half of the latex at a time, until you find the bugs\n",
        "\n",
        "Getting this HW into PDF form isn't meant to be a burden. One quick and easy approach is to open it as a Jupyter notebook, print, save to pdf. Just make sure your latex math answers aren't cut off so I can grade them.\n",
        "\n",
        "Please post on Ed or come to OH if there are any other problems submitting the HW.\n",
        "\n",
        "**Installing nbconvert:**\n",
        "\n",
        "If you're using Anaconda for package management, \n",
        "```\n",
        "conda install -c anaconda nbconvert\n",
        "```\n",
        "\n",
        "**Upload** your .pdf file to Gradescope. Please tag your questions!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "b6k2BC4ZeFpS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
