{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STATS305C Assignment 2: Gibbs Sampling and Metropolis-Hastings",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Gibbs Sampling and Metropolis-Hastings\n",
        "\n",
        "\n",
        "STATS305C, Stanford University, Spring 2022\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slinderman/stats305c/blob/master/assignments/hw2/hw2.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Name:** \n",
        "\n",
        "**Collaborators:** \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "DIaL2XtgC5_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this homework assignment, we will investigate a hierarchical Bayesian model of polling data. To perform inference, we will implement a hybrid Gibbs/Metropolis-Hastings sampler.\n",
        "\n",
        "### Background: Polling Data\n",
        "\n",
        "The 2004 election between George W. Bush and John Kerry was closely contested. Both campaigns focused heavily on the swing state of Ohio, worth 20 votes in the electoral college. CNN/USA Today/Gallup conducted several Ohio polls in the months leading up to the election on November 4th, and obtained the following raw results:\n",
        "\n",
        "|   | Kerry | Bush | Total\n",
        "| :- | :- | :- | :- |\n",
        "| Sept. 4-7 | 284 | 344 | 628 |\n",
        "| Sept. 25-28 | 312 | 325 | 637 |\n",
        "| Oct. 17-20 | 346 | 339 | 685 |\n",
        "| Oct. 28-31 | 556 | 511 | 1067 |\n",
        "\n",
        "The polling data can be found [here](http://www.cnn.com/ELECTION/2004/special/president/showdown/OH/) (although there are some dead links on the page).\n",
        "\n",
        "We will let $\\{x_i\\}_{i = 1}^4$ denote the number of votes for Kerry in each poll, and let $\\{n_i\\}_{i = 1}^4$ denote the total number surveyed in each poll. We will represent these quantities in code using integer PyTorch tensors."
      ],
      "metadata": {
        "id": "5ZQgpVFL_pz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.distributions import Gamma, Beta, Binomial, Normal\n",
        "torch.manual_seed(305)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.cm import Blues\n",
        "import seaborn as sns\n",
        "sns.set_context(\"notebook\")"
      ],
      "metadata": {
        "id": "cYTJS_5FHb6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = torch.tensor([284, 312, 346, 556], dtype=int)\n",
        "ns = torch.tensor([628, 637, 685, 1067], dtype=int)"
      ],
      "metadata": {
        "id": "PBRYlwtVHiT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Background: Hierarchical Model\n",
        "\n",
        "It is reasonable to model each voter's support as a draw from a Bernoulli distribution, with some poll-specific probability $\\rho_i$ of supporting Kerry. We believe each poll has a different probability since the polls are conducted at different times and may reach different subpopulations of voters (e.g. if one is conducted over telephone and another is an internet survey). However, these $\\rho_i$ are likely highly correlated due to a base level of support for Kerry in the general population. We will therefore use the following model: \n",
        "\n",
        "\\begin{align}\n",
        "\\rho_i &\\sim \\mathrm{Beta}(\\alpha, \\beta) \\quad  \\text{for } i =1, \\ldots, 4 \\\\ \n",
        "x_i \\mid \\rho_i &\\sim \\mathrm{Bin}(\\rho_i, n_i) \\quad  \\text{for } i =1, \\ldots, 4\n",
        "\\end{align}\n",
        "Note that the [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) is a conjugate prior for the [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution). (See also, Bishop Ch. 2.1.)\n",
        "\n",
        "Following a Bayesian approach, we introduce priors over the parameters $\\alpha$ and $\\beta$. Unfortunately, the parameters of the beta distribution do not have a simple conjugate prior. Instead, we will assume $\\alpha$ and $\\beta$ follow gamma distributions, resulting in the following Bayesian hierarchical model.\n",
        "\\begin{align}\n",
        "\\alpha &\\sim \\mathrm{Ga}(c_\\alpha, d_\\alpha) \\\\\n",
        "\\beta &\\sim \\mathrm{Ga}(c_\\beta, d_\\beta) \\\\\n",
        "\\rho_i \\mid \\alpha, \\beta &\\sim \\mathrm{Beta}(\\alpha, \\beta) \\quad  \\text{for } i =1, \\ldots, 4 \\\\ \n",
        "x_i \\mid \\rho_i &\\sim \\mathrm{Bin}(\\rho_i, n_i) \\quad  \\text{for } i =1, \\ldots, 4\n",
        "\\end{align}\n",
        "\n",
        "Here, $c_\\alpha, d_\\alpha, c_\\beta, d_\\beta$ are fixed hyperparameters for the \"hyper-priors\" on $\\alpha$ and $\\beta$ and we use the shape-rate parametrization of the gamma distribution. \n",
        "\n",
        "How should we interpret these hyperparameters? Recall that in a beta-Bernoulli model we can interpret the parameters of the beta as pseudo-observations. That is, we can view $\\alpha$ and $\\beta$ as results from a prior poll that resulted in $\\alpha$ voters for Kerry and $\\beta$ voters for Bush. Thus, we can interpret the mean of $\\alpha$ as previously observed supporters for Kerry and its standard deviation as a measure of uncertainty in this number of previous voters.\n",
        "\n",
        "Since the mean of $\\alpha$ is $\\frac{c_\\alpha}{d_\\alpha}$ and the standard deviation is $\\sqrt{\\frac{c_\\alpha}{d_\\alpha^2}}$, and similarly for $\\beta$, we can adjust the hyperparameters to accord with our prior belief on Kerry/Bush's level of support."
      ],
      "metadata": {
        "id": "U3kGh6YlHZry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1 [Math]: Derive the complete conditional for $\\rho$ \n",
        "\n",
        "In order to implement a Gibbs sampler, we must be able to sample from the complete conditional of each of the unobserved variables in our model. We will sample the variables $\\rho = (\\rho_1, \\dots, \\rho_4)$ simultaneously as part of a [*block Gibbs update*](https://en.wikipedia.org/wiki/Gibbs_sampling#Blocked_Gibbs_sampler). \n",
        "\n",
        "#### Part (a): Demonstrate conditional independence of $\\{\\rho_i\\}$\n",
        "\n",
        "First, show that we have:\n",
        "\n",
        "$$p(\\rho \\mid \\alpha, \\beta, \\{x_i\\}) = \\prod_{i=1}^4 p(\\rho_i \\mid \\alpha, \\beta, x_i)$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N1ulvTBkDkKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "_Your answer here_.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Tb_nEkBnFO2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part (b): \n",
        "\n",
        "Fixing $i$, derive the conditional density of $\\rho_i$ given $\\alpha, \\beta, x_i$, that is, compute:\n",
        "$$p(\\rho_i \\mid \\alpha, \\beta, x_i)$$\n",
        "\n",
        "Hint: Remember that this conditional density is proportional to the joint density of all the variables. The density should simplify nicely to a known density."
      ],
      "metadata": {
        "id": "Xwf0i5OrTCIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0HNQGLH1LJna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2: Write code to sample from the complete conditional for $\\rho$\n",
        "\n",
        "Write a function which generates a sample from the conditional distribution for $\\rho$ given $\\alpha$, $\\beta$, and $\\{x_i\\}$."
      ],
      "metadata": {
        "id": "RDzicQ7MFmDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Beta\n",
        "\n",
        "def sample_rhos(alpha, beta, xs):\n",
        "    \"\"\"\n",
        "    Returns a sample from the complete conditional of \\{\\rho_i\\} given alpha, beta, \\{x_i\\}.\n",
        "    Args:\n",
        "        alpha: scalar value > 0\n",
        "        beta: scalar value > 0\n",
        "        xs: torch.tensor of integers of length 4\n",
        "    Returns:\n",
        "        rhos: torch.tensor of length 4 with values in [0,1]\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    ##\n",
        "    return rhos"
      ],
      "metadata": {
        "id": "9inQCD8vHSQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3: Derive and implement the complete conditional for $\\alpha$\n",
        "\n",
        "#### Part (a): Derive the complete conditional\n",
        "\n",
        "Next, we need to derive the complete conditional for the global variables $\\alpha$ and $\\beta$. We will first find the complete conditional of $\\alpha$. As usual, you should use that the conditional density is proportional to the joint density to find this conditional. However, unlike earlier, the conditional density is not of a simple, known form. Instead, you should find an expression for the conditional density up to an unknown normalizing constant, i.e. an expression that the conditional density is proportional to, up to a constant."
      ],
      "metadata": {
        "id": "5lTWHpzSLA5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "_Your answer here_.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Xtaw_lxzLxxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part (b): Write code evaluating the unnormalized log probability\n",
        "\n",
        "By taking the logarithm of your answer above, we have an expression for the log conditional density, up to an additive constant. Implement this expression below in code.\n",
        "\n",
        "Hint: You may find the Pytorch function ```lgamma``` useful."
      ],
      "metadata": {
        "id": "tQ9xsgfAwUUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def alpha_log_cond(alpha, beta, rhos, c_alpha, d_alpha):\n",
        "    \"\"\"\n",
        "    Returns log p(\\alpha \\mid \\beta, \\{\\rho_i\\}, \\{x_i\\})\n",
        "    Args:\n",
        "        alpha: scalar value > 0\n",
        "        beta: scalar value > 0\n",
        "        rhos: torch.tensor of length 4 with values in [0,1]\n",
        "        c_alpha: scalar value > 0\n",
        "        d_alpha: scalar value > 0\n",
        "    Returns:\n",
        "        log_cond: scalar, conditional log probability of alpha\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    ##\n",
        "    return log_cond"
      ],
      "metadata": {
        "id": "sSB9IQ9qwfuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 4: Derive and implement the complete conditional for $\\beta$\n",
        "\n",
        "#### Part (a): Derive the complete conditional\n",
        "\n",
        "Write down an expression for the complete conditional density of $\\beta$, up to a normalizing constant. Your expression should be very similar to the expression for $\\alpha$ in problem 3(a). You do not need to show your work."
      ],
      "metadata": {
        "id": "HjStlHHuMF2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "8tGHYVCyL3kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part (b): Write code evaluating the unnormalized log probability\n",
        "\n",
        "As in 3(b), write a function which evaluates the log complete conditional for $\\beta$, up to an additive constant. This should be very similar to your function from 3(b)."
      ],
      "metadata": {
        "id": "Z9OPARzswj9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beta_log_cond(alpha, beta, rhos, c_beta, d_beta):\n",
        "    \"\"\"\n",
        "    Returns log p(\\beta \\mid \\alpha, \\{\\rho_i\\}, \\{x_i\\})\n",
        "    Args:\n",
        "        alpha: scalar value > 0\n",
        "        beta: scalar value > 0\n",
        "        rhos: torch.tensor of length 4 with values in [0,1]\n",
        "        c_beta: scalar value > 0\n",
        "        d_beta: scalar value > 0\n",
        "    Returns:\n",
        "        log_cond: scalar, conditional log probability of beta\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    ##\n",
        "    return log_cond"
      ],
      "metadata": {
        "id": "rymdw53iwpaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 5: Write code to sample from the complete conditionals for $\\alpha$ and $\\beta$\n",
        "\n",
        "Because we only know the complete conditionals for $\\alpha$ and $\\beta$ up to a normalizing constant, we will use the Metropolis-Hastings algorithm to sample from them. We will use a normal distribution centered at our current point as the proposal distribution. We will use the same standard deviation for the proposals of $\\alpha$ and $\\beta$. Thus, the proposal distributions will be:\n",
        "\\begin{align}\n",
        "q(\\alpha \\mid \\alpha') &= \\mathcal{N}(\\alpha ; \\alpha', l^2) \\\\\n",
        "q(\\beta \\mid \\beta') &= \\mathcal{N}(\\beta ; \\beta', l^2)\n",
        "\\end{align}\n",
        "for some $l > 0$.\n",
        "\n",
        "Note that we require $\\alpha > 0$ and $\\beta > 0$, so we may make a proposal which lies outside the support of the complete conditional. You should convince yourself that there is zero probability of accepting such a proposal in the Metropolis algorithm. \n",
        "\n",
        "Implement a step of the Metropolis-Hastings algorithm for both $\\alpha$ and $\\beta$ in the function below. You will have to use ```alpha_log_cond``` and ```beta_log_cond``` which you implemented above. \n",
        "\n",
        "_Hint_: You may find ```torch.rand``` useful for simulating uniform random variables, or ```torch.distributions.Bernoulli``` for sampling binary random variables with specified probabilities."
      ],
      "metadata": {
        "id": "eY2QN14fMj6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mh_step_alpha_beta(alpha, beta, rhos, c_alpha, d_alpha, c_beta, d_beta, l=20.):\n",
        "    \"\"\"\n",
        "    Performs a MH step for both alpha and beta.\n",
        "    Args:\n",
        "        alpha, beta, rhos: current values of these random variables\n",
        "        c_alpha, d_alpha, c_beta, d_beta: hyperparameters\n",
        "        l: standard deviation of proposal distribution\n",
        "    Returns:\n",
        "        new_alpha, new_beta: new values of alpha and beta after MH step\n",
        "    \"\"\"\n",
        "    new_alpha = Normal(alpha, l).sample()\n",
        "\n",
        "    # If proposal is negative, we can reject immediately.\n",
        "    if new_alpha < 0:\n",
        "        new_alpha = alpha\n",
        "    # Otherwise, we must check the accept-reject condition.\n",
        "    else:\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    ##\n",
        "    \n",
        "    new_beta = Normal(beta, l).sample()\n",
        "    # If proposal is negative, we can reject immediately.\n",
        "    if new_beta < 0:\n",
        "        new_beta = beta\n",
        "    # Otherwise, we must check the accept-reject condition.\n",
        "    else:\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    ##\n",
        "\n",
        "    return new_alpha, new_beta"
      ],
      "metadata": {
        "id": "LdIYb4nQJSto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 6: Implement a Gibbs sampler\n",
        "\n",
        "Using ```mh_step_alpha_beta``` and ```sample_rhos```, implement a Gibbs sampler for the proposed model. \n",
        "\n",
        "Using the generated samples, we will estimate the posterior mean and standard deviation of $\\rho_1, \\dots, \\rho_4$, and $\\frac{\\alpha}{\\alpha + \\beta}$. The latter quantity represents the base level of support for Kerry in the broader population, and is of particular interest for who will win the election.\n",
        "\n",
        "We will also track the joint log-probability of our sample throughout the iterations of the sampler. This will be useful as a diagnostic tool to check whether our sampler is correctly implemented. To easily evaluate the joint probability in code, we recommend making use of the `torch.distributions` imported above."
      ],
      "metadata": {
        "id": "mOSVFIUkSTk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rhos_0 = xs / ns\n",
        "def gibbs(n_iter,\n",
        "          alpha=torch.tensor(10.),\n",
        "          beta=torch.tensor(10.),\n",
        "          rhos=rhos_0,\n",
        "          c_alpha=torch.tensor(6.25), \n",
        "          d_alpha=torch.tensor(0.025),\n",
        "          c_beta=torch.tensor(6.25),\n",
        "          d_beta=torch.tensor(0.025),\n",
        "          burn_in=0.5):\n",
        "    \"\"\"\n",
        "    Performs Gibbs sampling in the Bayesian hierarchical model of polling data.\n",
        "    Args:\n",
        "        n_iter: number of iterations of Gibbs sampling\n",
        "        alpha, beta, rhos: initial values of these random variables\n",
        "        c_alpha, d_alpha, c_beta, d_beta: hyperparameters\n",
        "        burn_in: fraction of samples to discard before computing expectations\n",
        "    Returns:\n",
        "        rhos_mean, rhos_std: posterior mean and standard deviation of rhos\n",
        "        prob_mean, prob_std: posterior mean and standard deviation of alpha/(alpha+beta)\n",
        "        lps: torch tensor of size 'n_iter' containing log joint probabilities.\n",
        "    \"\"\"\n",
        "    rhos_samples = []\n",
        "    prob_samples = []\n",
        "    lps = torch.zeros(n_iter)\n",
        "\n",
        "    for it in range(n_iter):\n",
        "        # Resample alpha, beta using a single Metropolis-Hastings step.\n",
        "        ##\n",
        "        # YOUR CODE HERE\n",
        "        # alpha, beta = ...\n",
        "        ##        \n",
        "\n",
        "        # Resample rhos using the complete conditional.\n",
        "        ##\n",
        "        # YOUR CODE HERE\n",
        "        # rhos = ...\n",
        "        ##\n",
        "\n",
        "        if it > n_iter * burn_in:\n",
        "            rhos_samples += [rhos]\n",
        "            prob_samples += [alpha / (alpha + beta)]\n",
        "\n",
        "        # Evaluate log-joint probability at current sample\n",
        "        ##\n",
        "        # YOUR CODE HERE\n",
        "        #\n",
        "        ##\n",
        "\n",
        "    rhos_samples = torch.stack(rhos_samples)\n",
        "    prob_samples = torch.stack(prob_samples)\n",
        "    rhos_mean, rhos_std = torch.mean(rhos_samples, axis=0), torch.std(rhos_samples, axis=0)\n",
        "    prob_mean, prob_std = torch.mean(prob_samples), torch.std(prob_samples)\n",
        "\n",
        "    return rhos_mean, rhos_std, prob_mean, prob_std, lps"
      ],
      "metadata": {
        "id": "xoxOcbMF2-G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 7: Model Diagnostics\n",
        "\n",
        "Run Gibbs sampling for 10000 iterations (this should take about 15 seconds to run). Use the default values provided in the function signature. Plot the evolution of the joint-log-probability over iterations. "
      ],
      "metadata": {
        "id": "ieU_gCeUtE_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Gibbs sampling for 10000 iterations\n",
        "##\n",
        "# YOUR CODE HERE\n",
        "#\n",
        "##"
      ],
      "metadata": {
        "id": "V29kiA7y1bcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the joint-log probability over iterations\n",
        "##\n",
        "# YOUR CODE HERE\n",
        "#\n",
        "##"
      ],
      "metadata": {
        "id": "2mJ0sWnmjYef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 7: Investigations\n",
        "\n",
        "#### Part (a):\n",
        "\n",
        "What are the posterior means and standard deviations for $\\rho_1, ..., \\rho_4$ and $\\frac{\\alpha}{\\alpha + \\beta}$? Do these results seem reasonable to you?"
      ],
      "metadata": {
        "id": "vQKXKrhItwmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the posterior means and standard deviations.\n",
        "##\n",
        "# YOUR CODE HERE\n",
        "#\n",
        "##"
      ],
      "metadata": {
        "id": "_brqCrlT1rVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wilm4Xqn2pKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part (b):\n",
        "\n",
        "Find a setting of the hyperparameters $c_\\alpha, d_\\alpha, c_\\beta, d_\\beta$ which will result in a prior that heavily favors Kerry. Re-run the Gibbs sampler with these settings. What is the new mean and standard deviation for $\\frac{\\alpha}{\\alpha + \\beta}$?"
      ],
      "metadata": {
        "id": "nlXbHeg31qav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Gibbs sampling with new hyperparameters and print the posterior mean and \n",
        "# standard deviation.\n",
        "##\n",
        "# YOUR CODE HERE\n",
        "# \n",
        "##"
      ],
      "metadata": {
        "id": "A5GvR9qG3vQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "RV3VAwbj43uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 8: Reflections\n",
        "\n",
        "#### Part (a):\n",
        "\n",
        "Describe a setting in which a Gibbs sampler will be slow to mix. Why might our parametrization of the beta distribution in terms of $\\alpha$, $\\beta$ slow down the Gibbs sampler? Can you think of a different parametrization which would work better?"
      ],
      "metadata": {
        "id": "VesEVvJbtlfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GaX8iE0aOMbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part (b):\n",
        "\n",
        "Describe one way in which a Markov Chain using a Metropolis-Hastings adjusted transition may be slow to mix. Which variable in the code above could be tuned in order to speed up convergence?"
      ],
      "metadata": {
        "id": "OF1QXidUOLVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9zXqXcGGOPWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission Instructions\n",
        "\n",
        "\n",
        "**Formatting:** check that your code does not exceed 80 characters in line width. You can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
        "\n",
        "Download your notebook in .ipynb format and remove the Open in Colab button.  Then run the following command to convert to a PDF:\n",
        "```\n",
        "jupyter nbconvert --to pdf <yourname>_hw1.ipynb\n",
        "```\n",
        "\n",
        "\n",
        "**Installing nbconvert:**\n",
        "\n",
        "If you're using Anaconda for package management, \n",
        "```\n",
        "conda install -c anaconda nbconvert\n",
        "```\n",
        "\n",
        "**Upload** your .ipynb and .pdf files to Gradescope. "
      ],
      "metadata": {
        "id": "8RQzBPoTRgEV"
      }
    }
  ]
}