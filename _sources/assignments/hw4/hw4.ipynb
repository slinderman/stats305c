{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WjlVkfNJrIU7"
   },
   "source": [
    "# HW4: Bayesian Mixture Models\n",
    "\n",
    "---\n",
    "\n",
    "**Name:**\n",
    "\n",
    "**Names of any collaborators:**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1XkQMbsmJmQF"
   },
   "source": [
    "## Background\n",
    "\n",
    "In this homework assignment we will investigate image segmentation ---specifically, separating the background from the foreground of the image. To do so, you'll fit Bayesian mixtures of Gaussians using the expectation-maximization (EM) algorithm.\n",
    "\n",
    "The figure below shows the original input image and the resulting segmentations into background and foreground. By the end of this assignment, you will have implemented the algorithm to achieve this segmentation.\n",
    "\n",
    "Reference on image segmentation: https://en.wikipedia.org/wiki/Image_segmentation\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/slinderman/stats305c/main/assignments/hw4/images/fox_seg.png' width=800px>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WTfhXntid1kn"
   },
   "source": [
    "\n",
    "### Model\n",
    "\n",
    "We will use a simple mixture model to cluster the pixels (with the number of clusters $K = 2$ in our image segmentation problem). The likelihood is a mixture of Gaussian distributions.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_n \\mid z_n, \\{\\mu_k, \\Sigma_k\\}_{k=1}^K &\\sim \\mathcal{N}(\\mu_{z_n}, \\Sigma_{z_n}) \\\\\n",
    "z_n \\mid \\pi &\\sim \\text{Categorical}(\\pi)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $x_n \\in \\mathbb{R}^D$ is distributed according to a Gaussian distribution with the specified mean, $\\mu_k$, and covariance, $\\Sigma_k$, for its corresponding cluster $z_n = k$,\n",
    "and $z_n$ is distributed as a multinomial with hyperparameter $\\pi$. \n",
    "We will represent the images as a set of $N$ pixels, $\\{x_n\\}_{n=1}^N$, each in $D=3$ dimensional space, since there are three color channels (red, green, and blue).\n",
    "\n",
    "We specify the following priors on $\\mu_k$, $\\Sigma_k$, and $\\pi$.\n",
    "- Assume a normal-inverse-Wishart prior prior for each cluster mean and covariance.\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "p(\\mu_k, \\Sigma_k) &= \\mathrm{IW}(\\Sigma_k \\mid \\Sigma_0, \\nu_0) \\, \\mathcal{N}(\\mu_k \\mid \\mu_0, \\kappa_0^{-1} \\Sigma_k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here $\\Sigma_0, \\nu_0, \\mu_0, \\kappa_0$ are hyper-parameters.\n",
    "\n",
    "- We give a symmetric Dirichlet distribution prior to the mixing proportions, $\\pi$:\n",
    "\n",
    "$$ \n",
    "p(\\pi \\mid \\alpha) = \\text{Dirichlet}(\\alpha 1_K)\n",
    "$$\n",
    "\n",
    "where $1_K$ is an all-ones vector of length $K$ and $\\alpha$ is a hyperparameter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "unwDpLf6rIU-"
   },
   "source": [
    "## Problem 1 [math]: EM calculations\n",
    "In this problem, you will derive the EM procedure for our Bayesian model.\n",
    "For notational simplicity, let \n",
    "\n",
    "$$\n",
    "\\theta = (\\{\\mu_k, \\Sigma_k\\}_{k=1}^K, \\pi)\n",
    "$$\n",
    "\n",
    "be the tuple of parameters we wish to estimate via EM.\n",
    "Let $\\theta^{(i)}$ be the parameter value at iteration $i$.\n",
    "Recall the EM procedure is given by two steps:\n",
    "\n",
    "- **Expectation step (E-step)**:\n",
    "Compute\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_n(z_n)\n",
    "&=\n",
    "p(z_n \\mid x_n, \\theta^{(i)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- **Maximization step (M-step)**:\n",
    "Find new parameters \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta^{(i+1)}\n",
    "=\n",
    "\\underset{\\theta}{\\operatorname{argmax}} \n",
    "\\mathbb{E}_{q}\n",
    "[\\log p(\\mathbf{X}, \\mathbf{Z}, \\theta)]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "You will need these derivations to be correct for the implementation in Problem 2 to be correct, so we highly recommend taking the time to double-check them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xm16kPsCQl_j"
   },
   "source": [
    "### Problem 1a: Derive the posterior distribution for $q_n(z_n) = p(z_n | x_n, \\theta)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkJywHcKidfr"
   },
   "source": [
    "---\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TCBgIXSQaVe0"
   },
   "source": [
    "### Problem 1b: Derive the expected log probability\n",
    "\n",
    "Show that \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_q\\left[ \\log p(X, Z, \\theta) \\right]\n",
    "&= \n",
    "\\underbrace{\\sum\\limits_{k=1}^K\n",
    "\\left[ \n",
    "\\sum\\limits_{n=1}^N \n",
    "\\left[ \n",
    "\\omega_{nk}\n",
    "\\log \\mathcal{N}(x_n \\mid \\mu_{k}, \\Sigma_k)\n",
    "\\right]\n",
    "+\n",
    "\\log p(\\mu_k, \\Sigma_k)\n",
    "\\right]}_{\\mathcal{L}_1(\\mu, \\Sigma)}\n",
    "\\\\&\\qquad +\n",
    "\\underbrace{\\sum\\limits_{k=1}^K\n",
    "\\left[\n",
    "\\sum_{n=1}^N\n",
    "\\left[\\omega_{nk} \\log \\pi_k \\right]\n",
    "+ (\\alpha_k-1) \\log \\pi_k\n",
    "\\right]}_{\\mathcal{L}_2(\\pi)}\n",
    "+\n",
    "C\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "for some constant $C$, where $\\omega_{nk} = q_n(z_n =k)$, and\n",
    "where $\\mathcal{L}_1, \\mathcal{L}_2$ represent the terms in the expected log probability that depend on $\\{\\mu_k, \\Sigma_k\\}_{k=1}^K$ and $\\pi$, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MjfBVIXigKA"
   },
   "source": [
    "---\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WPF61K0HrIU-"
   },
   "source": [
    "### Problem 1c: Expand $\\mathcal{L}_1$ in exponential family form.\n",
    "\n",
    "Show that $\\log p(x_n\\mid z_n=k, \\mu_k, \\Sigma_k)$ and $\\log p(\\mu_k, \\Sigma_k)$ can be represented as the following:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log p(x_n\\mid z_n=k, \\mu_k, \\Sigma_k)\n",
    "&=\n",
    "t(x_n)^\\top \\eta_k - A(\\eta_k) + c\n",
    "\\\\\n",
    "\\log p(\\mu_k, \\Sigma_k)\n",
    "&=\n",
    "\\phi^\\top \\eta_k - \\nu A(\\eta_k) + c'\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "for some contants $c$, $c'$, functions $t$, $A$ (**explicitly find these**), hyperparameters $\\phi$, $\\nu$ (**explicitly find these**), where,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\eta_k &:= \\left(-\\frac{1}{2}\\log|\\Sigma_k|, -\\frac{1}{2}\\Sigma_k^{-1}, \\Sigma_k^{-1} \\mu_k, -\\frac{1}{2} \\mu_k^\\top \\Sigma_k^{-1} \\mu_k \\right) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here, inner-product between elements $a, b$ of the form $\\eta_k$ is defined to be\n",
    "\n",
    "$$\n",
    "\\langle a, b \\rangle := a_1 b_1 + \\mathrm{Tr}(a_2 b_2) + a_3^\\top b_3 + a_4 b_4\n",
    "$$\n",
    "\n",
    "Deduce that $\\mathcal{L}_1$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_1(\\mu, \\Sigma)\n",
    "&=\n",
    "\\sum\\limits_{k=1}^K\n",
    "\\left[\n",
    "\\sum\\limits_{n=1}^N\n",
    "\\left[\n",
    "\\omega_{nk}\n",
    "(t(x_n)^\\top \\eta_k - A(\\eta_k))\n",
    "\\right]\n",
    "+\n",
    "\\phi^\\top \\eta_k - \\nu A(\\eta_k)\n",
    "\\right] + c\n",
    "\\\\\n",
    "&=\n",
    "\\sum\\limits_{k=1}^K\n",
    "\\left[\n",
    "\\phi_{k}^\\top \\eta_k\n",
    "- \\nu_{k} A(\\eta_k)\n",
    "\\right] + c\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi_{k}\n",
    "&=\n",
    "\\phi + \\sum\\limits_{n=1}^N \\omega_{n,k} t(x_n)\n",
    "\\\\\n",
    "\\nu_{k}\n",
    "&=\n",
    "\\nu + \\sum\\limits_{n=1}^N \\omega_{n,k}\n",
    "\\\\\n",
    "\\omega_{n,k}\n",
    "&=\n",
    "q_n(z_n=k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Conclude that each summand of $\\mathcal{L}_1$ is the log-pdf (up to a constant) of some\n",
    "Normal-Inverse-Wishart (NIW) distribution of $(\\mu_k, \\Sigma_k)$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gDOgLT3AgQxU"
   },
   "source": [
    "### Problem 1d: Maximize $\\mathcal{L}_1$.\n",
    "\n",
    "Find the mode of an NIW distribution for $(\\mu, \\Sigma)$ with parameters \n",
    "$(\\Sigma_0, \\nu_0, \\kappa_0, \\mu_0)$.\n",
    "Use this result and (c) to find the closed-form solution for maximizing $\\mathcal{L}_1$\n",
    "w.r.t. $\\mu_k, \\Sigma_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0un4YEwiuS5"
   },
   "source": [
    "---\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SuoO_XlgKtPp"
   },
   "source": [
    "### Problem 1e: Maximize $\\mathcal{L}_2$.\n",
    "\n",
    "Find the maximizing solution $\\pi^*$ of $\\mathcal{L}_2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nkdGCJPiwei"
   },
   "source": [
    "---\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rVsZpiHLrIU_"
   },
   "source": [
    "## Problem 2 [code]: Implement EM for the Gaussian mixture model\n",
    "\n",
    "We have provided starter code below.\n",
    "First, you need to fill it with your own implementation of the EM algorithm. This entails writing three functions:\n",
    "1. `log_probability`, which computes the log probability $\\log p(X, \\theta)$ \n",
    "2. `e_step`, which computes the posteriors $q_n(z_n)$ for each data point, fixing the current parameters.\n",
    "3. `m_Step`, which returns new parameters, fixing the current posteriors.\n",
    "\n",
    "Then, you will test your code on a simple example, using the code we have proved.\n",
    "\n",
    "You may not rely on external implementations such as those offered by Tensorflow or scikit-learn.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "x1LV_ie_gIcj"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmjv6bBRrIVA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import MultivariateNormal, Categorical, Dirichlet\n",
    "\n",
    "from tqdm.auto import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WFdglrp1gJzH"
   },
   "source": [
    "### Helpers\n",
    "\n",
    "We have provided a helper function to compute the inverse Wishart log probability since this is not one of the standard distributions in `torch.distributions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2g0i-JDU09n"
   },
   "outputs": [],
   "source": [
    "def invwishart_log_prob(Sigma, nu0, Sigma0):\n",
    "    \"\"\"\n",
    "    Helper function to compute the inverse Wishart log probability, since its\n",
    "    not given in torch.distributions.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Sigma:      (..., D, D) batch of covariance matrices\n",
    "    nu0:        scalar degree of freedom of inverse Wishart distribution\n",
    "    Sigma0:     (D, D) scale matrix for inverse Wishart distribution\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    lp:         (...,) a batch of log probabilities\n",
    "    \"\"\"\n",
    "    D = Sigma.shape[-1]\n",
    "    assert Sigma.shape[-2:] == (D, D)\n",
    "    assert Sigma0.shape[-2:] == (D, D)\n",
    "    nu0 = torch.tensor(nu0)\n",
    "\n",
    "    lp = -(nu0 + D + 1) / 2 * torch.logdet(Sigma)\n",
    "    lp -= torch.linalg.solve(Sigma, Sigma0)\\\n",
    "        .diagonal(dim1=-1, dim2=-2).sum(axis=-1) / 2\n",
    "\n",
    "    # log normalizing constant\n",
    "    lp += nu0 / 2 * torch.logdet(Sigma0)\n",
    "    lp -= nu0 * D / 2 * torch.log(torch.tensor(2.0))\n",
    "    lp -= torch.special.multigammaln(nu0 / 2, D)\n",
    "    return lp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N08-YKrygUmL"
   },
   "source": [
    "### Problem 2a: Implement the `log_probability` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8T3gXbCWrIVB"
   },
   "outputs": [],
   "source": [
    "def log_probability(X, mus, Sigmas, pi,\n",
    "                    alpha, mu0, kappa0, nu0, Sigma0):\n",
    "    \"\"\"\n",
    "    Compute the log probability \\log p(X, \\theta), summing over the discrete\n",
    "    cluster assignments.\n",
    "\n",
    "    Hint: You may use the invwishart_log_prob function above.\n",
    "    Hint: You may also want to use torch.logsumexp to do the sum over z.\n",
    "\n",
    "    Args:\n",
    "    - X:        (N, D) tensor of data points\n",
    "    - mus:      (K, D) tensor of cluster means\n",
    "    - Sigmas:   (K, D, D) tensor of cluster covariances\n",
    "    - pi:       (K,) tensor of cluster weights\n",
    "    - alpha:    (K,) concentration of the Dirichlet prior\n",
    "    - mu0:      (D,) tensor with the prior mean\n",
    "    - kappa0:   scalar prior precision\n",
    "    - nu0:      scalar prior degrees of freedom\n",
    "    - Sigma0:   (D, D) tensor of prior scale of the covariance\n",
    "\n",
    "    Returns:\n",
    "    - lp:       scalar log probability of the data and parameters, summing over\n",
    "                the discrete latent variables\n",
    "    \"\"\"\n",
    "    lp = 0\n",
    "\n",
    "    ###\n",
    "    # Your code here.\n",
    "    ##\n",
    "    return lp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qM3GBfCfga8e"
   },
   "source": [
    "### Problem 2b: Implement the `e_step` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_2MysECgYxl"
   },
   "outputs": [],
   "source": [
    "def e_step(X, mus, Sigmas, pi):\n",
    "    \"\"\"\n",
    "    Perform one E step to compute the posterior \n",
    "\n",
    "        q_n(z_n) = p(z_n | x_n, \\theta)\n",
    "\n",
    "    for each data point. \n",
    "\n",
    "    Args:\n",
    "    - X:        (N, D) tensor of data points\n",
    "    - mus:      (K, D) tensor of cluster means\n",
    "    - Sigmas:   (K, D, D) tensor of cluster covariances\n",
    "    - pi:       (K,) tensor of cluster weights\n",
    "\n",
    "    Returns:\n",
    "    - Q:        (N, K) tensor of responsibilities; i.e. posterior probabilities. \n",
    "                Each row should be non-negative and sum to one\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K, _ = mus.shape\n",
    "    q = torch.zeros((N, K))\n",
    "\n",
    "    ###\n",
    "    # Your code here.\n",
    "    ##\n",
    "    return q\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v217EFapgfua"
   },
   "source": [
    "### Problem 2c: Implement the `m_step` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxBlK5WYgd3C"
   },
   "outputs": [],
   "source": [
    "def m_step(X, q, alpha, mu0, kappa0, nu0, Sigma0):\n",
    "    \"\"\"\n",
    "    Perform one M-step to find new parameters given the current posterior\n",
    "    and hyperparameters.\n",
    "\n",
    "    Args:\n",
    "    - X:        (N, D) data matrix\n",
    "    - q:        (N, K) responsibilities; i.e. posterior probabilities\n",
    "    - alpha:    (K,) concentration of the Dirichlet prior\n",
    "    - mu0:      (D,) tensor with the prior mean\n",
    "    - kappa0:   scalar prior precision\n",
    "    - nu0:      scalar prior degrees of freedom\n",
    "    - Sigma0:   (D, D) tensor of prior scale of the covariance\n",
    "\n",
    "    Returns:\n",
    "    - mus:      (K, D) new means for each cluster\n",
    "    - Sigmas:   (K, D, D) new covariances for each cluster\n",
    "    - pi:       (K,) new cluster probabilities\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    _, K = q.shape\n",
    "\n",
    "    ### \n",
    "    # Your code here.\n",
    "    ##\n",
    "    return mus, Sigmas, pi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KX0FR5PvglYA"
   },
   "source": [
    "### EM function [given]\n",
    "\n",
    "We've provided an `em` function to run EM on a given dataset with the specified hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vPxUX4NgjTU"
   },
   "outputs": [],
   "source": [
    "def em(X, \n",
    "       K=2, \n",
    "       n_iter=100, \n",
    "       alpha=torch.ones(3),\n",
    "       mu0=torch.zeros(3),\n",
    "       kappa0=1.0,\n",
    "       nu0=4.0,\n",
    "       Sigma0=torch.eye(3)):\n",
    "    \"\"\"\n",
    "    EM algorithm.\n",
    "\n",
    "    Args:\n",
    "    - X: Matrix of size (N, D). Each row of X stores one data point\n",
    "    - K: the desired number of clusters in the model. Default: 2\n",
    "    - n_iter: number of iterations of EM. Default: 100\n",
    "    - alpha0: prior concentration of cluster probabilities\n",
    "    - mu0, kappa0, nu0, Sigma0: parameters of normal-inverse-Wishart prior.\n",
    "        Their shapes must be consistent with D, the data dimension.\n",
    "        \n",
    "    Returns:\n",
    "    - mus: cluster means\n",
    "    - Sigmas: cluster covariances\n",
    "    - pi: cluster assignment probabilities\n",
    "    - q: posterior probability of Z | X, mus, Sigmas, pi with final params.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert alpha.shape == (K,)\n",
    "    assert mu0.shape == (D,)\n",
    "    assert Sigma0.shape == (D, D)\n",
    "    hypers = (alpha, mu0, kappa0, nu0, Sigma0)\n",
    "\n",
    "    # Initialize cluster parameters\n",
    "    pi = alpha / torch.sum(alpha)\n",
    "    mus = X[Categorical(logits=torch.zeros(N)).sample((K,))]\n",
    "    Sigmas = Sigma0.repeat(K, 1, 1)\n",
    "\n",
    "    # Initialize log prob outputs\n",
    "    lps = []\n",
    "\n",
    "    # Run EM\n",
    "    for _ in trange(n_iter):\n",
    "        q = e_step(X, mus, Sigmas, pi)\n",
    "        lps.append(log_probability(X, mus, Sigmas, pi, *hypers))\n",
    "        mus, Sigmas, pi = m_step(X, q, *hypers)\n",
    "        \n",
    "    # Run one last E-step to tighten the bound\n",
    "    q = e_step(X, mus, Sigmas, pi)\n",
    "    lps.append(log_probability(X, mus, Sigmas, pi, *hypers))\n",
    "\n",
    "    return torch.tensor(lps), mus, Sigmas, pi, q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FaMLn9gNrIVC"
   },
   "source": [
    "### Test your implementation on a toy dataset\n",
    "Test your example on a synthetic data set.\n",
    "\n",
    "For example, the ground truth could be two clusters, with means $[5,5]$ \n",
    "and $[8,8]$ with identity covariance matrices, respectively.\n",
    "You could generate $100$ points in each cluster. \n",
    "\n",
    "Whichever example you choose, be sure to specify it and show that your implementation roughly recovers the ground truth by displaying the cluster means/covariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uq4ISfRHcdNK"
   },
   "outputs": [],
   "source": [
    "def confidence_ellipse(mean, cov, ax, n_std=3.0, facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Modified from: https://matplotlib.org/3.5.0/gallery/\\\n",
    "        statistics/confidence_ellipse.html\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean: vector-like, shape (n,)\n",
    "        Mean vector.\n",
    "        \n",
    "    cov : matrix-like, shape (n, n)\n",
    "        Covariance matrix.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to draw the ellipse into.\n",
    "\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "\n",
    "    **kwargs\n",
    "        Forwarded to `~matplotlib.patches.Ellipse`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "    \"\"\"\n",
    "    # compute the 2D covariance ellipse\n",
    "    pearson = cov[0, 1] / torch.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    ell_radius_x = torch.sqrt(1 + pearson)\n",
    "    ell_radius_y = torch.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), \n",
    "                      width=ell_radius_x * 2, \n",
    "                      height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, \n",
    "                      **kwargs)\n",
    "\n",
    "    # Calculating the standard deviation\n",
    "    # the square root of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale = torch.sqrt(torch.diag(cov) * n_std)\n",
    "    \n",
    "    # Transform the ellipse by rotating, scaling, and translating\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(*scale) \\\n",
    "        .translate(*mean)\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "\n",
    "    # Add the patch to the axis\n",
    "    return ax.add_patch(ellipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4oznRQgrIVC"
   },
   "outputs": [],
   "source": [
    "def test_toy(seed=305+ord('c'),\n",
    "             n_test=200,\n",
    "             mus=torch.Tensor([[5,5], [8,8]]),\n",
    "             covs=torch.eye(2).repeat(2,1,1),\n",
    "             K=2,\n",
    "             n_iter=300,\n",
    "             ):\n",
    "    K, D = mus.shape\n",
    "    assert covs.shape == (K, D, D)\n",
    "    \n",
    "    # Generate n_test random data points from each of K classes and combine\n",
    "    torch.manual_seed(seed)\n",
    "    X = MultivariateNormal(mus, covs).sample((n_test,)).reshape(-1, D)\n",
    "    \n",
    "    # Run the EM algorithm\n",
    "    em_results = em(X, K=K, n_iter=n_iter,\n",
    "                    alpha=torch.ones(K),\n",
    "                    mu0=torch.zeros(D),\n",
    "                    kappa0=1.0,\n",
    "                    nu0=3.0,\n",
    "                    Sigma0=torch.eye(D))\n",
    "    \n",
    "    # Return data and results\n",
    "    return (X, *em_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJEXkgrVWY_1"
   },
   "outputs": [],
   "source": [
    "K = 2\n",
    "X, lps, means, covs, probs, q = test_toy(K=K)\n",
    "\n",
    "# display the results  \n",
    "for k in range(K):\n",
    "    print(\"Cluster \", k, \":\")\n",
    "    print(\"\\t mu:    \", means[k,:])\n",
    "    print(\"\\t Sigma: \", covs[k,:,:])\n",
    "    print(\"\\t probs: \", probs[k])\n",
    "    print(\"\")\n",
    "\n",
    "# Plot the log probabilities over EM iterations\n",
    "plt.figure()\n",
    "plt.plot(lps[1:])\n",
    "plt.xlabel(\"EM iteration\")\n",
    "plt.ylabel(\"log probability\")\n",
    "\n",
    "# create a second figure to plot the clustered data\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# plot scatter \n",
    "ax.scatter(X[:,0], X[:,1], c=torch.argmax(q, 1), marker='.')\n",
    "\n",
    "for i in range(K):\n",
    "  # plot mean as red dots\n",
    "  ax.scatter(means[i,0], means[i,1], c='red')\n",
    "\n",
    "  # plot covariance ellipses\n",
    "  confidence_ellipse(means[i,:], covs[i], ax, n_std=1, \n",
    "                     edgecolor='red', linestyle=':')\n",
    "  confidence_ellipse(means[i,:], covs[i], ax, n_std=2, \n",
    "                     edgecolor='red', linestyle=':')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gQubLj61rIVD"
   },
   "source": [
    "## Problem 3 [short answer]: Perform image segmentation \n",
    "\n",
    "**All you have to do for this part is run the code we've provided below to test your EM implementation on a couple image segmentation problems and then answer the discussion questions below.**\n",
    "\n",
    "Now that you have implemented the EM algorithm, you are ready to perform image segmentation!\n",
    "\n",
    "First, we'll download some test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTXtcu05ZpXI"
   },
   "outputs": [],
   "source": [
    "# First, download the files from the github page\n",
    "!wget -nc https://raw.githubusercontent.com/slinderman/stats305c/main/assignments/hw4/images/fox.png\n",
    "!wget -nc https://raw.githubusercontent.com/slinderman/stats305c/main/assignments/hw4/images/cow.png\n",
    "!wget -nc https://raw.githubusercontent.com/slinderman/stats305c/main/assignments/hw4/images/owl.png\n",
    "!wget -nc https://raw.githubusercontent.com/slinderman/stats305c/main/assignments/hw4/images/zebra.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-IvkFY4bf9D"
   },
   "source": [
    "Next, we've written some helper functions to run your EM code to segment the images, print summaries of the results, and make some nice plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fJTBqbyrIVD"
   },
   "outputs": [],
   "source": [
    "def load_image(filename):\n",
    "    image = plt.imread(filename + \".png\")[:, :, :3]\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # get height, width and number of channels\n",
    "    H, W, C = image.shape\n",
    "    X = image.copy().astype(float)\n",
    "\n",
    "    # reshape into pixels, each has 3 channels (RGB)\n",
    "    X = X.reshape((H * W, C)) \n",
    "    return image, torch.Tensor(X)\n",
    "\n",
    "def save_segmentation(image, assignments, filename=None):\n",
    "    import numpy as np\n",
    "    fig, axs = plt.subplots(1, K + 1, figsize=(4 * (K + 1), 4))\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_axis_off()\n",
    "    axs[0].set_title(\"original image\")\n",
    "    \n",
    "    for k in range(K):\n",
    "        im = image.copy()\n",
    "        im[assignments != k] = np.nan\n",
    "        axs[k+1].imshow(im)\n",
    "        axs[k+1].set_axis_off()\n",
    "        axs[k+1].set_title(\"component {}\".format(k))\n",
    "    \n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)\n",
    "\n",
    "def run_segmentation(filename, \n",
    "                     K=2, \n",
    "                     seed=305 + ord('c'),\n",
    "                     n_iter=100,\n",
    "                     alpha=100):\n",
    "    # Load the specified image\n",
    "    image, X = load_image(filename)\n",
    "\n",
    "    # Run EM on a GMM with K classes\n",
    "    torch.manual_seed(seed)\n",
    "    lps, means, covs, probs, q = em(X, K=K, n_iter=100, \n",
    "                                    alpha=alpha * torch.ones(K))\n",
    "    assignments = torch.argmax(q, axis=1).reshape(image.shape[:2])\n",
    "\n",
    "    # Print the results\n",
    "    print(filename + \" results:\")\n",
    "    for k in range(K):\n",
    "        print(\"Cluster \", k, \":\")\n",
    "        print(\"\\t mu:    \", means[k,:])\n",
    "        print(\"\\t Sigma: \", covs[k,:,:])\n",
    "        print(\"\\t probs: \", probs[k])\n",
    "        print(\"\")\n",
    "\n",
    "    # Plot the log probability over iterations\n",
    "    plt.figure()\n",
    "    plt.plot(lps[1:])\n",
    "    plt.xlabel(\"EM iteration\")\n",
    "    plt.ylabel(\"log probability\")\n",
    "\n",
    "    # Save \n",
    "    save_segmentation(image, assignments, filename=filename + \"_seg.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "D9iuHyslbppo"
   },
   "source": [
    "### Finally, run the segmentation for each image\n",
    "Please run all of these cells! \n",
    "It should only take a few seconds for each cell to complete. E.g. our reference implementation takes 21 seconds for `fox`, 4 seconds for `cow`, 2 seconds for `owl`, and 12 seconds for `zebra`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFQJyypVrIVD"
   },
   "outputs": [],
   "source": [
    "run_segmentation(\"fox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcUGw2iRbuER"
   },
   "outputs": [],
   "source": [
    "run_segmentation(\"cow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XoPJhNzcBrQ"
   },
   "outputs": [],
   "source": [
    "run_segmentation(\"owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0xs7JuDcGmJ"
   },
   "outputs": [],
   "source": [
    "run_segmentation(\"zebra\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OIqktdiAcn3f"
   },
   "source": [
    "### Problem 3a: Multiple restarts\n",
    "Explain why you might need multiple restarts for EM to obtain the best results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UBn77bndMDd"
   },
   "source": [
    "---\n",
    "\n",
    "_Your answer here._\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "c4Ct9JXDcvpD"
   },
   "source": [
    "### Problem 3b: Model improvements\n",
    "\n",
    "How could you extend this model -- e.g. by building in more prior information about images -- to improve the background segmentations? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_b0ZL4sdO-w"
   },
   "source": [
    "---\n",
    "\n",
    "_Your answer here._\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "swApRqlBrIVE"
   },
   "source": [
    "## Submission Instructions\n",
    "\n",
    "\n",
    "**Formatting:** check that your code does not exceed 80 characters in line width. If you're working in Colab, you can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
    "\n",
    "Download your notebook in .ipynb format and use the following commands to convert it to PDF:\n",
    "```\n",
    "jupyter nbconvert --to pdf hw4_yourname.ipynb\n",
    "```\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "- `nbconvert`: If you're using Anaconda for package management, \n",
    "```\n",
    "conda install -c anaconda nbconvert\n",
    "```\n",
    "\n",
    "**Upload** your .pdf files to Gradescope. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STATS305C Assignment 4: Bayesian Mixture Models",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
