{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DIaL2XtgC5_s"
   },
   "source": [
    "# HW3: Continuous Latent Variable Models\n",
    "\n",
    "---\n",
    "\n",
    "**Name:** \n",
    "\n",
    "**Collaborators:** \n",
    "\n",
    "---\n",
    "\n",
    "This homework explores continuous latent variable models like PCA and factor analysis. We will work with a synthetic dataset (MNIST digits) where we artificially mask out some pixels. Then we'll see how well we can reconstruct the images by performing Bayesian inference in a factor analysis model with missing data.\n",
    "\n",
    "This application may seem a bit contrived -- who cares about MNIST digits? -- but it has real-world applications. For example, [Markowitz et al (2018)](https://www.sciencedirect.com/science/article/pii/S0092867418305129) used this technique to find a low-dimensional embedding of images of partially occluded mice. \n",
    "\n",
    "Along the way, we'll build some intuition for PCA, hone our Gibbs sampling skills, and as a bonus, you can learn about multivariate Gaussian distribution for matrices called the matrix normal distribution.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mhJyPVgS6Y9b"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYTJS_5FHb6J"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.distributions import Gamma, Normal, Bernoulli, MultivariateNormal, \\\n",
    "    TransformedDistribution\n",
    "from torch.distributions.transforms import PowerTransform\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import Blues\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yazMOk_d421D"
   },
   "outputs": [],
   "source": [
    "class ScaledInvChiSq(TransformedDistribution):\n",
    "    \"\"\"\n",
    "    Implementation of the scaled inverse \\chi^2 distribution defined in class.\n",
    "    We will implement it as a transformation of a gamma distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, dof, scale):\n",
    "        base = Gamma(dof / 2, dof * scale / 2)\n",
    "        transforms = [PowerTransform(-1)]\n",
    "        TransformedDistribution.__init__(self, base, transforms)\n",
    "        self.dof = dof\n",
    "        self.scale = scale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GZrktw9Su6p8"
   },
   "source": [
    "### Download the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQnZ7R50nMxb"
   },
   "outputs": [],
   "source": [
    "# Download MNIST training data and convert to float32\n",
    "# Only use a subset of the images\n",
    "N = 10000\n",
    "X3d_true = MNIST(root='.', train=True, transform=None, download=True).data\n",
    "X3d_true = X3d_true.type(torch.float32)\n",
    "X3d_true = X3d_true[:N]\n",
    "_, H, W = X3d_true.shape\n",
    "\n",
    "# Add some noise to the images so they are not strictly integers\n",
    "# Otherwise we get weird numerical bugs in the Gibbs sampling code!\n",
    "torch.manual_seed(305)\n",
    "X3d_true += Normal(0, 3).sample(X3d_true.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ-kzSnXv5yI"
   },
   "source": [
    "### Write simple functions to mask off some of the data \n",
    "\n",
    "We'll make three types of masks:\n",
    "- Lines through the center of the image\n",
    "- Circles of random radius\n",
    "- Speckle, where each pixel is missing at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnbYQw3064ZH",
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def random_line_mask(num_samples, \n",
    "                     mask_size=(28, 28), \n",
    "                     lw=2):\n",
    "    \"\"\"\n",
    "    Make a mask from a line through the center of the image.\n",
    "\n",
    "    Args:\n",
    "        num_samples: number of masks to generate\n",
    "        mask_size: pixels by pixels\n",
    "        lw: line width in pixels\n",
    "\n",
    "    Returns:\n",
    "        masks: (num_samples,) + mask_size array of binary masks\n",
    "\n",
    "    \"\"\"\n",
    "    # Sample random orientations for each line\n",
    "    us = Normal(0, 1).sample((num_samples, 2))\n",
    "    us /= torch.norm(us, dim=1, keepdim=True)\n",
    "\n",
    "    # Get distance of each xy coordinate to the line\n",
    "    # this is the norm of (x, y) - (xp, yph) where (xp, yp)\n",
    "    # is the projection onto the line\n",
    "    X, Y = torch.meshgrid(torch.arange(mask_size[0]), \n",
    "                          torch.arange(mask_size[1]))\n",
    "    xy = torch.column_stack([X.ravel(), Y.ravel()])\n",
    "    xy = xy - torch.tensor(mask_size) / 2.0\n",
    "    \n",
    "    # Project onto the line\n",
    "    # xpyp.shape == (num_samples, num_points, 2)\n",
    "    xpyp = (us @ xy.T).unsqueeze(2) * us.unsqueeze(1)  \n",
    "    dist = torch.norm(xy - xpyp, dim=2)\n",
    "\n",
    "    # Make masks based on a distance threshold\n",
    "    return (dist < lw).reshape((num_samples,) + mask_size)\n",
    "    \n",
    "\n",
    "def random_circle_mask(num_samples, \n",
    "                       mask_size=(28, 28),\n",
    "                       std_origin=3.0,\n",
    "                       mean_radius=3.0,\n",
    "                       df_radius=7.0):\n",
    "    \"\"\"\n",
    "    Sample random circular masks.\n",
    "\n",
    "    Args:\n",
    "        num_samples: number of masks to generate\n",
    "        mask_size: mask size in pixels\n",
    "        std_origin: standard deviation of the origin in pixels\n",
    "        mean_radius: mean radius of the circular masks\n",
    "        df_radius: degrees of freedom of a chi^2 distribution on radii.\n",
    "\n",
    "    Returns:\n",
    "        masks: (num_samples,) + mask_size array of binary masks\n",
    "    \"\"\"\n",
    "    centers = Normal(0, std_origin).sample((num_samples, 2))\n",
    "    radii = 0.1 + Gamma(df_radius / 2,\n",
    "                      df_radius / mean_radius / 2.0).sample((num_samples,))\n",
    "\n",
    "    # Determine whether each point is inside the corresponding circle\n",
    "    X, Y = torch.meshgrid(torch.arange(mask_size[0]),\n",
    "                          torch.arange(mask_size[1]))\n",
    "    X = X - mask_size[0] / 2.0\n",
    "    Y = Y - mask_size[1] / 2.0\n",
    "    xy = torch.column_stack([X.ravel(), Y.ravel()]) # (num_points, 2)\n",
    "    dist = torch.norm(centers.unsqueeze(1) - xy, dim = 2)\n",
    "    return  (dist < radii.unsqueeze(1)).reshape((num_samples,) + mask_size)\n",
    "\n",
    "\n",
    "def random_speckle_mask(num_samples,\n",
    "                        mask_size=(28, 28),\n",
    "                        p_missing=0.1):\n",
    "    \"\"\"\n",
    "    Sample a random speckle mask where each pissing is missing with equal \n",
    "    probability.\n",
    "\n",
    "    Args:\n",
    "        num_samples: number of masks to sample\n",
    "        p_speckle: probability that a pixel is missing\n",
    "\n",
    "    Returns:\n",
    "        masks: (num_samples,) + mask_size binary array\n",
    "    \"\"\"\n",
    "    masks = Bernoulli(p_missing).sample((num_samples,) + mask_size)\n",
    "    return masks.type(torch.BoolTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RRdBzJcmDLNs"
   },
   "source": [
    "### Make masks and apply them to each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PP5mFEu0v8dy"
   },
   "outputs": [],
   "source": [
    "# Make masks for each data point\n",
    "torch.manual_seed(305)\n",
    "line_masks = random_line_mask(N // 3)\n",
    "circ_masks = random_circle_mask(N // 3)\n",
    "spck_masks = random_speckle_mask(N - len(line_masks) - len(circ_masks))\n",
    "mask3d = torch.cat([line_masks, circ_masks, spck_masks])[torch.randperm(N)]\n",
    "\n",
    "# Make the training data by substituting 255 (the max value of a uint8) \n",
    "# for each missing pixel\n",
    "X3d = torch.clone(X3d_true)\n",
    "X3d[mask3d] = 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xHip3XlRDUrJ"
   },
   "source": [
    "### Plot the masks and the masked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJkmBu-r5ndD"
   },
   "outputs": [],
   "source": [
    "# Plot a few masks\n",
    "fig, axs = plt.subplots(5, 5, figsize=(8, 8))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        axs[i, j].imshow(mask3d[i * 5 + j], interpolation=\"none\")\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "fig.suptitle(\"Random Masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoL1njfVEF4M"
   },
   "outputs": [],
   "source": [
    "# Plot a few masked data points\n",
    "fig, axs = plt.subplots(5, 5, figsize=(8, 8))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        axs[i, j].imshow(X3d[i * 5 + j], interpolation=\"none\")\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "fig.suptitle(\"Masked Data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N0i92KsIDacp"
   },
   "source": [
    "### Flatten the data and masks into 2D tensors\n",
    "\n",
    "The masked data is now stored in the tensor `X3d`, which has shape `(60000, 28, 28)`. We will flatten the tensor into `X`, which has shape `(60000, 784)`, and consider each row to be a vector-valued observation. We'll do the same for the masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9fwRA4sDeyQ"
   },
   "outputs": [],
   "source": [
    "X_true = X3d_true.reshape((N, -1))\n",
    "X = X3d.reshape((N, -1))\n",
    "mask = mask3d.reshape((N, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xfBtkhd8Hkx"
   },
   "source": [
    "**Note:** From here on out, you should only need `X` and `mask` in your code algorithm. `X_true` is reserved for validation purposes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aGJd4H2ECy9I"
   },
   "source": [
    "## Part 1: Principal Components Analysis and the SVD\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KOz9Dd1B7rDr"
   },
   "source": [
    "### Problem 1a [Code]: Run PCA on directly on the masked data\n",
    "\n",
    "In this problem, you'll investigate what happens if you run PCA on `X` directly.  \n",
    "\n",
    "Implement PCA by taking the SVD of the centered and rescaled data matrix. Plot the first 25 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyIH7kHD7DCE"
   },
   "outputs": [],
   "source": [
    "def pca(X):\n",
    "    \"\"\"\n",
    "    Compute the principal components and the fraction of variance explained \n",
    "    using the SVD of the scaled and centered data matrix. \n",
    "\n",
    "    Args:\n",
    "        X: a shape (N, D) tensor\n",
    "\n",
    "    Returns:\n",
    "        pcs: a shape (D, D) tensor whose columns are the full set of D principal\n",
    "            components. This matrix should be orthogonal.\n",
    "\n",
    "        var_explained: a shape (D,) tensor whose entries are the variance \n",
    "            explained by each corresponding principal component.\n",
    "    \"\"\"\n",
    "    ## \n",
    "    # Your code below.\n",
    "    #\n",
    "    ##\n",
    "    return pcs, var_explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6gVrldUE6j-"
   },
   "source": [
    "We have provided some code below to run your code and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxtwfqmX_c3i"
   },
   "outputs": [],
   "source": [
    "def plot_pca(pcs, var_explained):\n",
    "    \"\"\"\n",
    "    Helper function to plot the principal components and the variance explained,\n",
    "    aka scree plot.\n",
    "    \"\"\"\n",
    "    # Plot the first 25 principal components\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(8, 8))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            axs[i, j].imshow(pcs[:, i * 5 + j].reshape((28, 28)), \n",
    "                            interpolation=\"none\")\n",
    "            axs[i, j].set_xticks([])\n",
    "            axs[i, j].set_yticks([])\n",
    "            axs[i, j].set_title(\"PC {}\".format(i * 5 + j + 1))\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Make the scree plot\n",
    "    plt.figure()\n",
    "    plt.plot(torch.cumsum(var_explained, dim=0))\n",
    "    plt.xlabel(\"Number of PCs\")\n",
    "    plt.xlim(0, 784)\n",
    "    plt.ylabel(\"Fraction of Variance Explained\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5DdKuBpFZBU"
   },
   "outputs": [],
   "source": [
    "# Plot the pca results for X, the flattened, masked data\n",
    "plot_pca(*pca(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_4EWkkrAxE0"
   },
   "outputs": [],
   "source": [
    "# Compare the results to PCA on the X_true, the flattened true data\n",
    "plot_pca(*pca(X_true))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "p5ZZKE9lGCns"
   },
   "source": [
    "### Problem 1b [Short Answer]: Why does PCA on the masked data need so many more components?\n",
    "\n",
    "PCA needs far fewer components to reach 90% variance explained on the real data (`X_true`) than it does on the masked data (`X`). Intuitively, why is that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnMvbmLbGdwH"
   },
   "source": [
    "---\n",
    "\n",
    "_Your answer here._\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LyjAfpByCcQa"
   },
   "source": [
    "## Part 2: Gibbs Sampling for Factor Analysis with Missing Data\n",
    "\n",
    "Now we will try to fit a continuous latent variable model to the masked data by treating the masked pixels as missing data. As in lecture, we will assume a conjugate prior of the form,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sigma_d^2 &\\sim \\chi^{-2}(\\nu_0, \\sigma_0^2) \\\\\n",
    "\\mathbf{w}_d &\\sim \\mathcal{N}(\\mathbf{0}, \\tfrac{\\sigma_d^2}{\\kappa_0} \\mathbf{I}) \\\\\n",
    "\\mu_d &\\sim \\mathcal{N}(0, \\tfrac{\\sigma_d^2}{\\lambda_0})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The only thing we've added is a prior on the mean, which we previously assumed to be fixed at zero.\n",
    "\n",
    "Given the parameters, the distribution on latent variables and data is,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{z}_n &\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\\n",
    "\\mathbf{x}_n &\\sim \\mathcal{N}(\\mathbf{W} \\mathbf{z}_n + \\boldsymbol{\\mu}, \\mathrm{diag}(\\boldsymbol{\\sigma}^2))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W} \\in \\mathbb{R}^{D \\times M}$ is a matrix with rows $\\mathbf{w}_d$, $\\boldsymbol{\\mu} = [\\mu_1, \\ldots, \\mu_D]^\\top$ and $\\boldsymbol{\\sigma}^2 = [\\sigma_1^2, \\ldots, \\sigma_D^2]^\\top$.\n",
    "\n",
    "The graphical model (omitting the hyperparameters) looks like this:\n",
    "\n",
    "<img src=\"https://dl.dropbox.com/s/xwojsl2jfkolxj3/fa_missing_data2.png?dl=0\" alt=\"Factor Analysis with Missing Data Graphical Model\" width=\"600\"/>\n",
    "\n",
    "Here, the $d$th coordinate is missing from the $n$-th data point. On other data points, other subsets of coordinates may be missing.\n",
    "\n",
    "\n",
    "To formalize the problem, let\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{X}_{\\mathsf{obs}} &= \\{x_{n,d}: x_{n,d} \\text{ is observed}\\} \\\\\n",
    "\\mathbf{X}_{\\mathsf{miss}} &= \\{x_{n,d}: x_{n,d} \\text{ is missing}\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "denote the observed and missing data, respectively. \n",
    "\n",
    "**Our goal** is to infer the posterior distribution over parameters and latent variables and _missing_ data given only the _observed_ data and hyperparamters,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{W}, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma^2}, \\mathbf{Z}, \\mathbf{X}_{\\mathsf{miss}} \\mid \\mathbf{X}_{\\mathsf{obs}}, \\boldsymbol{\\eta}),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\eta} = (\\nu_0, \\sigma_0^2, \\kappa_0, \\lambda_0)$ are the hyperparameters.\n",
    "\n",
    "To do so, we will implement a Gibbs sampling algorithm that alternates between updating the parameters $\\mathbf{W}$ and $\\boldsymbol{\\sigma^2}$ and the latent variables $\\mathbf{z}_n$ for each data point, and then we'll add one more step: sampling new values for the missing data $\\mathbf{X}_{\\mathsf{miss}}$ from their conditional distribution. With samples of $\\mathbf{X}_{\\mathsf{miss}}$, for example, we can approximate the posterior distribution over the masked regions of the image. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LayHmAZXUFdF"
   },
   "source": [
    "### Problem 2a [Math]: Derive the complete conditional distributions for the Gibbs sampler\n",
    "\n",
    "Specifically, derive closed form expressions for the following conditional distributions:\n",
    "- $p(\\mathbf{w_d} \\mid \\{\\mathbf{w}_i\\}_{i \\neq d}, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2, \\mathbf{Z}, \\mathbf{X}_{\\mathsf{miss}}, \\mathbf{X}_{\\mathsf{obs}}, \\boldsymbol{\\eta})$\n",
    "- $p(\\mu_d \\mid \\{\\mu_i\\}_{i \\neq d}, \\mathbf{W}, \\boldsymbol{\\sigma}^2, \\mathbf{Z}, \\mathbf{X}_{\\mathsf{miss}}, \\mathbf{X}_{\\mathsf{obs}}, \\boldsymbol{\\eta})$\n",
    "- $p(\\sigma_d^2 \\mid \\{\\sigma_i^2\\}_{i \\neq d}, \\mathbf{W}, \\boldsymbol{\\mu}, \\mathbf{Z}, \\mathbf{X}_{\\mathsf{miss}}, \\mathbf{X}_{\\mathsf{obs}}, \\boldsymbol{\\eta})$\n",
    "- $p(\\mathbf{z}_n \\mid \\mathbf{W}, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2, \\{\\mathbf{z}_i\\}_{i\\neq n}, \\mathbf{X}_{\\mathsf{miss}}, \\mathbf{X}_{\\mathsf{obs}}, \\boldsymbol{\\eta})$\n",
    "- $p(x_{n,d} \\mid \\mathbf{W}, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2, \\mathbf{Z}, \\mathbf{X}_{\\mathsf{obs}}, \\boldsymbol{\\eta})$  for each missing entry $x_{n,d}$\n",
    "\n",
    "_Hint: Your expressions may not depend on all of the conditioned upon variables._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kG8aR0l8uvAU"
   },
   "source": [
    "---\n",
    "\n",
    "_Your answer here._\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Svn980HGu0br"
   },
   "source": [
    "### Problem 2b [Short answer]: Which Gibbs steps can be performed in parallel?\n",
    "\n",
    "As in Assignment 2, some of these updates can be performed in parallel using a blocked Gibbs udpate. Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7_yk4SfvFtb"
   },
   "source": [
    "---\n",
    "\n",
    "_Your answer here._\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MXkk7wczv1fn"
   },
   "source": [
    "### Problem 2c [Code]: Implement the Gibbs sampler\n",
    "\n",
    "Finish the functions below to implement the udpates you derived above. We have provided some function headers to help you organize your solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNfFdeUif1tO"
   },
   "outputs": [],
   "source": [
    "def log_probability(X, Z, W, mu, sigmasq, nu0, sigmasq0, kappa0, lambda0):\n",
    "    \"\"\"\n",
    "    Evaluate the log joint probability of the _complete_ data and all the \n",
    "    latent variables and parameters.\n",
    "\n",
    "    Args:\n",
    "        X: shape (N,D) tensor with the complete data (current samples of the \n",
    "            missing data are filled in)\n",
    "        Z: shape (N,M) tensor with the latent variables\n",
    "        W: shape (D,M) tensor of weights\n",
    "        mu: shape (D,) tensor with the mean parameter\n",
    "        sigmasq: shape (D,) tensor with the variance parameters\n",
    "        nu0, sigmasq0: scalar hyperparameters for the prior on variance\n",
    "        kappa0: scalar hyperparameter for the prior on weights\n",
    "        lambda0: scalar hyperparameter for the prior on mean\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your code here.\n",
    "    #\n",
    "    # Hint: Take advantage of Pytorch distributions' support for broadcasting\n",
    "    # to evaluate many log probabilities at once.\n",
    "    ##\n",
    "    return lp\n",
    "\n",
    "\n",
    "def gibbs_sample_latents(W, mu, sigmasq, X):\n",
    "    \"\"\"\n",
    "    Sample new weights W given the other parameters, latent variables, and \n",
    "    hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        W: shape (D,M) tensor of weights\n",
    "        mu: shape (D,) tensor with the mean \n",
    "        sigmasq: shape (D,) tensor with variance parameters\n",
    "        X: shape (N,D) tensor with the complete data (current samples of the \n",
    "            missing data are filled in)\n",
    "\n",
    "    Returns:\n",
    "        Z: shape (N,M) tensor with latent variables sampled from their \n",
    "            conditional\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your code here.\n",
    "    # \n",
    "    # Hint: use the MultivariateNormal distribution object and take advantage\n",
    "    # of its broadcasting capabilities to sample the rows of Z in parallel.\n",
    "    #\n",
    "    # Hint: `torch.linalg.solve(J, h.unsqueeze(2))` will broadcast a solve of a\n",
    "    # a shape (M, M) tensor `J` with a shape (N, M) tensor `h`. It gives a \n",
    "    # tensor of shape (N, M, 1). If you're not careful with broadcasting, you \n",
    "    # can get out of memory issues and crash the kernel.\n",
    "    ##\n",
    "    return Z\n",
    "\n",
    "\n",
    "def gibbs_sample_weights(mu, sigmasq, Z, X, kappa0):\n",
    "    \"\"\"\n",
    "    Sample new weights W given the other parameters, latent variables, and \n",
    "    hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        mu: shape (D,) tensor with the mean parameter\n",
    "        sigmasq: shape (D,) tensor with the variance parameters\n",
    "        Z: shape (N,M) tensor with the latent variables\n",
    "        X: shape (N,D) tensor with the complete data (current samples of the \n",
    "            missing data are filled in)\n",
    "        kappa0: scalar hyperparameter for the prior on weights\n",
    "\n",
    "    Returns:\n",
    "        W: shape (D,M) tensor of weights sampled from its conditional\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your code here.\n",
    "    # \n",
    "    # Hint: you can use the MultivariateNormal distribution object and take \n",
    "    # advantage of its broadcasting capabilities to sample many rows of W in \n",
    "    # parallel.\n",
    "    ##\n",
    "    return W\n",
    "\n",
    "\n",
    "def gibbs_sample_mean(W, sigmasq, Z, X, lambda0):\n",
    "    \"\"\"\n",
    "    Sample new weights W given the other parameters, latent variables, and \n",
    "    hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        W: shape (D,M) tensor of weights\n",
    "        sigmasq: shape (D,) tensor with the variance parameters\n",
    "        Z: shape (N,M) tensor with the latent variables\n",
    "        X: shape (N,D) tensor with the complete data (current samples of the \n",
    "            missing data are filled in)\n",
    "        lambda0: scalar hyperparameter for the prior on mean\n",
    "\n",
    "    Returns:\n",
    "        mu: shape (D,) tensor with the mean sampled from its conditional\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your code here.\n",
    "    #\n",
    "    ##\n",
    "    return mu\n",
    "\n",
    "\n",
    "def gibbs_sample_variance(W, mu, Z, X, nu0, sigmasq0, kappa0, lambda0):\n",
    "    \"\"\"\n",
    "    Sample new weights W given the other parameters, latent variables, and \n",
    "    hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        W: shape (D,M) tensor of weights\n",
    "        mu: shape (D,) tensor with the mean \n",
    "        Z: shape (N,M) tensor with the latent variables\n",
    "        X: shape (N,D) tensor with the complete data (current samples of the \n",
    "            missing data are filled in)\n",
    "        nu0, sigmasq0: scalar hyperparameters for the prior on variance\n",
    "        kappa0: scalar hyperparameter for the prior on weights\n",
    "        lambda0: scalar hyperparameter for the prior on mean\n",
    "\n",
    "    Returns:\n",
    "        sigmasq: shape (D,) tensor with variance sampled from its conditional\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your code here.\n",
    "    # \n",
    "    # Hint: You may use the ScaledInvChiSq distribution provide above. It also\n",
    "    # supports broadcasting.\n",
    "    ##\n",
    "    return sigmasq\n",
    "\n",
    "\n",
    "def gibbs_sample_missing_data(W, mu, sigmasq, Z, X, mask):\n",
    "    \"\"\"\n",
    "    Sample new weights W given the other parameters, latent variables, and \n",
    "    hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        W: shape (D,M) tensor of weights\n",
    "        mu: shape (D,) tensor with the mean \n",
    "        sigmasq: shape (D,) tensor with variance parameters\n",
    "        Z: shape (N,M) tensor with the latent variables\n",
    "        X: shape (N,D) tensor with the complete data (current samples of the \n",
    "            missing data are filled in)\n",
    "        mask: shape (N,D) boolean tensor where 1 (True) specifies that the \n",
    "            corresponding entry in X is missing and needs to be resampled.\n",
    "\n",
    "    Returns:\n",
    "        X: shape (N,D) tensor which is the same as the given X in entries where\n",
    "            mask == 0 (False), but which has new values sampled from their \n",
    "            conditional distribution in entries where mask == 1 (True).\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your code here.\n",
    "    # \n",
    "    # Hint: Pytorch supports the same sorts of indexing tricks as numpy. \n",
    "    # See: https://pytorch.org/cppdocs/notes/tensor_indexing.html\n",
    "    # For example, you can use `X[mask] = vals` to set only the entries where \n",
    "    # the boolean mask is 1 (True). In this expression, `vals` is a 1d tensor\n",
    "    # whose length equals the number of missing values, \n",
    "    # i.e. `len(vals) = mask.sum()`. \n",
    "    ##\n",
    "    return X\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "402NDGXHz8Pj"
   },
   "source": [
    "### Run the Gibbs Sampler [Provided]\n",
    "\n",
    "We have provided a simple function to run your Gibbs sampling code on the masked data from above. Collecting 200 Gibbs samples takes about 5 minutes with my implementation (on a Colab notebook, not using the GPU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWUw-I5v0MGq"
   },
   "outputs": [],
   "source": [
    "def gibbs(X, \n",
    "          mask, \n",
    "          M=50,\n",
    "          nu0=1.1, \n",
    "          sigmasq0=10., \n",
    "          kappa0=0.01, \n",
    "          lambda0=0.01, \n",
    "          N_samples=200):\n",
    "    \"\"\"\n",
    "    Run the Gibbs sampler.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        X: shape (N,D) tensor with the complete data (current samples of the \n",
    "            missing data are filled in)\n",
    "        mask: shape (N,D) boolean tensor where 1 (True) specifies that the \n",
    "            corresponding entry in X is missing and needs to be resampled.\n",
    "        M: the dimension of the continuous latent variables\n",
    "        nu0, sigmasq0: scalar hyperparameters for the prior on variance\n",
    "        kappa0: scalar hyperparameter for the prior on weights\n",
    "        lambda0: scalar hyperparameter for the prior on mean\n",
    "        N_samples:  number of Gibbs iterations to run\n",
    "    \n",
    "    Returns:\n",
    "\n",
    "    Dictionary with samples of the parameters tausq, mu, thetas, sigmasqs, and \n",
    "    the log joint probability at each iteration.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "\n",
    "    # We will be updating X in place each time we sample missing data.\n",
    "    # Rather than overwriting the data that's passed in, we'll make a clone \n",
    "    # and update that instead.\n",
    "    X = torch.clone(X)\n",
    "\n",
    "    # Similarly, all the missing data is currently set to 255 (the high value).\n",
    "    # Let's initialize the missing data with the mean of the observed data.\n",
    "    fmask = mask.type(torch.float32)\n",
    "    N_obs = torch.sum(1 - fmask, dim=0)\n",
    "    X_mean = torch.sum(X * (1 - fmask), dim=0) / N_obs\n",
    "    X[mask] = X_mean.repeat(N, 1)[mask]\n",
    "\n",
    "    # Initialize the mean \\mu to the sample mean and the variance \\sigmasq to\n",
    "    # the sample variance of the observed data. Initialize the weights and the \n",
    "    # latent variables randomly.\n",
    "    mu = X_mean\n",
    "    sigmasq = torch.sum((X - X_mean)**2 * (1 - fmask), dim=0) / N_obs\n",
    "    W = Normal(0, 1).sample((D, M))\n",
    "    Z = Normal(0, 1).sample((N, M))\n",
    "\n",
    "    # Compute the initial log probability\n",
    "    lp = log_probability(X, Z, W, mu, sigmasq, nu0, sigmasq0, kappa0, lambda0)\n",
    "    \n",
    "    # Initialize the output\n",
    "    samples = [(torch.clone(X[mask]), Z, W, mu, sigmasq, lp)]\n",
    "\n",
    "    # Run the Gibbs sampler\n",
    "    for itr in trange(N_samples - 1):\n",
    "        # Cycle through each update \n",
    "        Z = gibbs_sample_latents(W, mu, sigmasq, X)\n",
    "        W = gibbs_sample_weights(mu, sigmasq, Z, X, kappa0)\n",
    "        mu = gibbs_sample_mean(W, sigmasq, Z, X, lambda0)\n",
    "        sigmasq = gibbs_sample_variance(W, mu, Z, X, \n",
    "                                        nu0, sigmasq0, kappa0, lambda0)\n",
    "        X = gibbs_sample_missing_data(W, mu, sigmasq, Z, X, mask)\n",
    "\n",
    "        # Compute the log probability\n",
    "        lp = log_probability(X, Z, W, mu, sigmasq, \n",
    "                             nu0, sigmasq0, kappa0, lambda0)\n",
    "                \n",
    "        # Update the sample list\n",
    "        samples.append((torch.clone(X[mask]), Z, W, mu, sigmasq, lp))\n",
    "\n",
    "    # Combine the output into a dictionary with a cool python zip trick\n",
    "    samples_dict = dict()\n",
    "    keys = [\"X_miss\", \"Z\", \"W\", \"mu\", \"sigmasq\", \"lps\"]\n",
    "    values = zip(*samples)\n",
    "    for key, value in zip(keys, values):\n",
    "        samples_dict[key] = torch.stack(value)\n",
    "\n",
    "    return samples_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJiBENjGAvu7"
   },
   "outputs": [],
   "source": [
    "# This takes about 5-6 min with my code. For debugging purposes, you may want\n",
    "# to reduce N_samples, but please reset it to 200 for your final analysis.\n",
    "N_samples = 200\n",
    "samples = gibbs(X, mask, M=50, N_samples=N_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "G3tDWrL6IVlb"
   },
   "source": [
    "### Plot your results [Provided]\n",
    "\n",
    "The code below generates the following plots:\n",
    "- Trace of the log joint probability\n",
    "- The first 25 data points with their missing values filled in with the average of $\\mathbf{X}_{\\mathsf{miss}}$ from the last half of the Gibbs samples. - 25 factors from the final Gibbs sample arranged into a 5x5 grid where each factor is shown as a 28x28 pixel image.\n",
    "- The root mean squared error of the reconstructed image over iterations.\n",
    "- Plot of the mean $\\boldsymbol{\\mu}$ averaged over the last half of the Gibbs samples, shown as a 28x28 pixel image\n",
    "- Plot of the variance $\\boldsymbol{\\sigma}^2$ averaged over the last half of the Gibbs samples, shown as a 28x28 pixel image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zpDU4upGYwo"
   },
   "outputs": [],
   "source": [
    "offset = 5\n",
    "plt.plot(torch.arange(offset, N_samples), samples[\"lps\"][offset:])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Log Joint Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-GyGNXxH82r"
   },
   "outputs": [],
   "source": [
    "# Plot the masked and reconstructed data, using the mean of X_miss samples\n",
    "X_miss = samples['X_miss'][N_samples//2:].mean(dim=0)\n",
    "X_recon = torch.clone(X)\n",
    "X_recon[mask] = X_miss\n",
    "\n",
    "# Plot a few masked data points\n",
    "fig, axs = plt.subplots(5, 5, figsize=(16, 8))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        im = torch.column_stack([X[i * 5 + j].reshape(28, 28),\n",
    "                                 X_recon[i * 5 + j].reshape(28, 28)])\n",
    "        axs[i, j].imshow(im, interpolation=\"none\", vmin=0, vmax=255)\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "fig.suptitle(\"Masked and Reconstructed Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMiZP2NmIyAc"
   },
   "outputs": [],
   "source": [
    "# Plot the reconstruction error across Gibbs iterations\n",
    "rmse = torch.sqrt(((samples['X_miss'] - X_true[mask])**2).mean(axis=1))\n",
    "plt.plot(rmse)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0qjqEedHv5n"
   },
   "outputs": [],
   "source": [
    "# Plot the first 25 principal components\n",
    "W = samples['W'][-1]\n",
    "fig, axs = plt.subplots(5, 5, figsize=(8, 8))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        axs[i, j].imshow(W[:, i * 5 + j].reshape((28, 28)), \n",
    "                        interpolation=\"none\")\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "        axs[i, j].set_title(\"Factor {}\".format(i * 5 + j + 1))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8i1eqjdVNZjT"
   },
   "outputs": [],
   "source": [
    "# Plot the posterior mean of $\\mu$\n",
    "plt.imshow(samples[\"mu\"][N_samples//2:].mean(dim=0).reshape(28, 28))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Mean Image\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTjVkOCyNco5"
   },
   "outputs": [],
   "source": [
    "# Plot the posterior mean of $\\sigma^2$\n",
    "plt.imshow(torch.sqrt(samples[\"sigmasq\"][N_samples//2:])\\\n",
    "           .mean(0).reshape(28, 28))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Per-Pixel Variance\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wChozMHcMck9"
   },
   "source": [
    "### Problem 2d [Short answer]: Discussion\n",
    "\n",
    "Were you surprised at how well (or poorly) you were able to reconstruct the masked images using factor analysis? Could you imagine alternative approaches that might perform better, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSCaCOksM-BR"
   },
   "source": [
    "---\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LWHWZ4-PHxqm"
   },
   "source": [
    "## Bonus: The matrix normal distribution\n",
    "\n",
    "In the model above, we put a prior on the weights $\\mathbf{W} \\in \\mathbb{R}^{D \\times M}$ by assuming each row to be an independent multivariate normal vector,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{W}) &= \\prod_{d=1}^D \\mathcal{N}(\\mathbf{w}_d \\mid \\mathbf{0}, \\tfrac{\\sigma_d^2}{\\kappa_0} \\mathbf{I}).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "However, in class we noted that it's a bit strange to put a prior on the rows when it's the columns (i.e. the principal components) that we really care about. \n",
    "\n",
    "For this bonus problem, we'll derive a **matrix normal** prior distribution instead. The matrix normal is a distribution on matrices $\\mathbf{W} \\in \\mathbb{R}^{D \\times M}$ with three parameters: a mean $\\mathbf{M} \\in \\mathbb{R}^{D \\times M}$, a positive definite covariance among the rows $\\mathbf{\\Sigma}_r \\in \\mathbb{R}_{\\succeq 0}^{D \\times D}$, and a positive definite covariance among the columns $\\mathbf{\\Sigma}_c \\in \\mathbb{R}_{\\succeq 0}^{M \\times M}$. \n",
    "\n",
    "The matrix normal distribution is equivalent to a multivariate distribution on the vectorized (aka flattened or raveled) matrix where the covariance matrix obeys a special, Kronecker-factored form. Specifically,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{W} \\sim \\mathcal{MN}(\\mathbf{M}, \\mathbf{\\Sigma}_r, \\mathbf{\\Sigma}_c)\n",
    "\\iff \\mathrm{vec}(\\mathbf{W}) \\sim \\mathcal{N}(\\mathrm{vec}(\\mathbf{M}), \\mathbf{\\Sigma}_r \\otimes \\mathbf{\\Sigma}_c),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathrm{vec}(\\cdot)$ is the vectorization operation that ravels a matrix into a vector (here in row-major, i.e. C order) and $\\otimes$ denotes the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product). \n",
    "\n",
    "For example, suppose\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{M} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{vec}\\left( \\mathbf{M} \\right) = [1, 2, 3, 4, 5, 6]^\\top.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The vectorized matrix is the concatenation of its rows.\n",
    "\n",
    "To illustrate the Kronecker product, suppose\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{\\Sigma}_r = \\begin{bmatrix} \n",
    "    1 & 0 & 0 \\\\ \n",
    "    0 & 1 & 0 \\\\\n",
    "    0 & 0 & 1 \n",
    "    \\end{bmatrix}, \\quad\n",
    "    \\mathbf{\\Sigma}_c = \n",
    "    \\begin{bmatrix} 1 & -1 \\\\ -1 & 2 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{\\Sigma}_r \\otimes \\mathbf{\\Sigma}_c =\n",
    "    \\begin{bmatrix} \n",
    "    1 & -1 & 0 & 0 & 0 & 0\\\\ \n",
    "    -1 & 2 & 0 & 0 & 0 & 0\\\\\n",
    "    0 & 0 & 1 & -1 & 0 & 0 \\\\\n",
    "    0 & 0 & -1 & 2 & 0 & 0 \\\\\n",
    "    0 & 0 & 0 & 0 & 1 & -1 \\\\\n",
    "    0 & 0 & 0 & 0 & -1 & 2\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{\\Sigma}_r$ is the identity matrix, each row $\\mathbf{w}_d \\in \\mathbb{R}^2$ is an independent multivariate normal random variable with covariance $\\mathbf{\\Sigma}_c$. With this example in mind, we now see that the prior we used in Part 2 was really a special case of the matrix normal distribution with $\\mathbf{M} = \\mathbf{0}$, $\\mathbf{\\Sigma}_r = \\mathrm{diag}([\\sigma_1^2, \\ldots, \\sigma_D^2])$, and $\\mathbf{\\Sigma}_c = \\kappa_0^{-1} \\mathbf{I}$.\n",
    "\n",
    "We can derive the matrix normal density by starting from the multivariate normal density on the vectorized matrix,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{W} \\mid \\mathbf{M}, \\mathbf{\\Sigma}_r, \\mathbf{\\Sigma}_c)\n",
    "&= (2 \\pi)^{-\\frac{DM}{2}} |\\mathbf{\\Sigma}_r \\otimes \\mathbf{\\Sigma}_c | \n",
    "\\exp \\left\\{ -\\frac{1}{2} \\mathrm{vec}(\\mathbf{W} - \\mathbf{M})^\\top (\\mathbf{\\Sigma}_r \\otimes \\mathbf{\\Sigma}_c)^{-1} \\mathrm{vec}(\\mathbf{W} - \\mathbf{M}) \\right\\} \\\\\n",
    "&= (2 \\pi)^{-\\frac{DM}{2}} |\\mathbf{\\Sigma}_r|^M |\\mathbf{\\Sigma}_c|^D  \n",
    "\\exp \\left\\{ -\\frac{1}{2} \\mathrm{vec}(\\mathbf{W} - \\mathbf{M})^\\top (\\mathbf{\\Sigma}_r^{-1} \\otimes \\mathbf{\\Sigma}_c^{-1}) \\mathrm{vec}(\\mathbf{W} - \\mathbf{M}) \\right\\} \\\\\n",
    "&= (2 \\pi)^{-\\frac{DM}{2}} |\\mathbf{\\Sigma}_r|^M |\\mathbf{\\Sigma}_c|^D  \n",
    "\\exp \\left\\{ -\\frac{1}{2} \\mathrm{vec}(\\mathbf{W} - \\mathbf{M})^\\top \\mathrm{vec}(\\mathbf{\\Sigma}_r^{-1}(\\mathbf{W} - \\mathbf{M}) \\mathbf{\\Sigma}_c^{-1}) \\right\\} \\\\\n",
    "&= (2 \\pi)^{-\\frac{DM}{2}} |\\mathbf{\\Sigma}_r|^M |\\mathbf{\\Sigma}_c|^D  \n",
    "\\exp \\left\\{ -\\frac{1}{2}\\mathrm{Tr} \\left[ \\mathbf{\\Sigma}_c^{-1} (\\mathbf{W} - \\mathbf{M})^\\top \\mathbf{\\Sigma}_r^{-1} (\\mathbf{W} - \\mathbf{M}) \\right] \\right\\} \\\\\n",
    "&\\propto \\exp \\left\\{ -\\frac{1}{2}\\mathrm{Tr} \\left[ \\mathbf{\\Sigma}_c^{-1} \\mathbf{W}^\\top \\mathbf{\\Sigma}_r^{-1} \\mathbf{W} \\right] + \\mathrm{Tr} \\left[\\mathbf{\\Sigma}_c^{-1} \\mathbf{M}^\\top \\mathbf{\\Sigma}_r^{-1} \\mathbf{W} \\right] \\right\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "_Note: the definitions given here are appropriate for Python/PyTorch, where vectorization is performed in row-major order. This is in contrast to the definition on [Wikipedia](https://en.wikipedia.org/wiki/Matrix_normal_distribution), which assumes column-major order, as in Matlab or R. The only difference is ther order of the Kronecker product is flipped._\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Dxr1TPSSVJdT"
   },
   "source": [
    "### Bonus Problem [Math]: Derive the conditional distribution of the factor analysis weights under a matrix normal prior\n",
    "\n",
    "Now consider the factor analysis model,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{z}_n &\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\\n",
    "\\mathbf{x}_n &\\sim \\mathcal{N}(\\mathbf{W} \\mathbf{z}_n + \\boldsymbol{\\mu}, \\mathrm{diag}(\\boldsymbol{\\sigma}^2))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\mu} = [\\mu_1, \\ldots, \\mu_D]^\\top$ and $\\boldsymbol{\\sigma}^2 = [\\sigma_1^2, \\ldots, \\sigma_D^2]^\\top$. \n",
    "\n",
    "Suppose we put the following matrix normal prior on the weights and variances,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sigma_d^2 &\\sim \\chi^{-2}(\\nu_0, \\sigma_0^2) \\\\\n",
    "\\boldsymbol{\\mu} &\\sim \\mathcal{N}(\\mathbf{0}, \\mathrm{diag}(\\boldsymbol{\\sigma}^2) / \\lambda_0) \\\\\n",
    "\\mathbf{W} &\\sim \\mathcal{MN}(\\mathbf{0}, \\mathrm{diag}(\\boldsymbol{\\sigma}^2), \\mathbf{\\Sigma}_c)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\Sigma}_c$ is the prior covariance among the columns.\n",
    "\n",
    "Derive the complete conditional distribution of the weights,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{W} \\mid \\{\\mathbf{z}_n, \\mathbf{x}_n\\}_{n=1}^N, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2, \\mathbf{\\Sigma}_c) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Finally, let $\\mathbf{\\Sigma}_c^{-1} \\to \\mathbf{0}$. What does the conditional mean of $\\mathbf{W}$ converge to? Does this expression look familiar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21OlVYKMZqUn"
   },
   "source": [
    "---\n",
    "\n",
    "_Your answer here._\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AdWfXqj4MBPs"
   },
   "source": [
    "## Submission Instructions\n",
    "\n",
    "\n",
    "**Formatting:** check that your code does not exceed 80 characters in line width. You can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
    "\n",
    "Download your notebook in .ipynb format and remove the Open in Colab button.  Then run the following command to convert to a PDF:\n",
    "```\n",
    "jupyter nbconvert --to pdf <yourname>_hw3.ipynb\n",
    "```\n",
    "\n",
    "**Installing nbconvert:**\n",
    "\n",
    "If you're using Anaconda for package management, \n",
    "```\n",
    "conda install -c anaconda nbconvert\n",
    "```\n",
    "\n",
    "If you can't get `nbconvert` to work, you may print to PDF using your browswer, but please make sure that none of your code, text, or math is cut off.\n",
    "\n",
    "**Upload** your .pdf files to Gradescope. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STATS305C Assignment 3: Factor Analysis with Missing Data",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
