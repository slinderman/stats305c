{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "# HW 5: Poisson Matrix Factorization\n",
    "\n",
    "---\n",
    "\n",
    "**Name:**\n",
    "\n",
    "**Collaborators:**\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_6W3YHJpPVeO"
   },
   "source": [
    "## Background\n",
    "\n",
    "**Poisson matrix factorization** (PMF) is a mixed membership model like LDA, and it has close ties to non-negative factorization of count matrices. Let $\\mathbf{X} \\in \\mathbb{N}^{N \\times M}$ denote a count matrix with entries $x_{n,m}$. We model each entry as a Poisson random variable,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_{n,m} &\\sim \\mathrm{Po}\\Big(\\boldsymbol{\\theta}_{n}^\\top \\boldsymbol{\\eta}_{m} \\Big)\n",
    "= \\mathrm{Po}\\Big(\\sum_{k=1}^K \\theta_{n,k} \\eta_{m,k} \\Big),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\theta}_{n} \\in \\mathbb{R}_+^K$ and $\\boldsymbol{\\eta}_{n} \\in \\mathbb{R}_+^K$ are _non-negative_ feature vectors for row $n$ and column $m$, respectively. \n",
    "\n",
    "PMF has been used for recommender systems, aka collaborative filtering. In a recommender system, the rows correspond to users, the columns to items, and the entries $x_{n,m}$ to how much user $n$ liked item $m$ (on a scale of $0,1,2,\\ldots$ stars, for example). The $K$ feature dimensions capture different aspects of items that users may weight in their ratings.\n",
    "\n",
    "Note that the Poisson rate must be non-negative. It is sufficient to ensure $\\boldsymbol{\\theta}_{n}$ and $\\boldsymbol{\\eta}_{m}$ are non-negative. To that end, PMF uses gamma priors,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta_{n,k} &\\sim \\mathrm{Ga}(\\alpha_\\theta, \\beta_\\theta) \\\\\n",
    "\\eta_{m,k} &\\sim \\mathrm{Ga}(\\alpha_\\eta, \\beta_\\eta),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\alpha_\\star$ and $\\beta_\\star$ are hyperparameters. When $\\alpha_\\star < 1$, the gamma distribution has a sharp peak at zero and the prior induces sparsity in the feature vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LQJf08BS4ml4"
   },
   "source": [
    "### Latent variable formulation\n",
    "\n",
    "PMF can be rewritten in terms of a latent variable model. Note that,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_{n,m} \\sim \\mathrm{Po}\\Big(\\sum_{k=1}^K \\theta_{n,k} \\eta_{m,k} \\Big)\n",
    "\\iff x_{n,m} &= \\sum_{k=1}^K z_{n,m,k} \\\\\n",
    "z_{n,m,k} &\\sim \\mathrm{Po}(\\theta_{nk} \\eta_{mk}) \\quad \\text{independently}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "From this perspective, a user's rating of an item is a sum of ratings along each feature dimension, and each feature rating is an independent Poisson random variable. \n",
    "\n",
    "The joint distribution is,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{X}, \\mathbf{Z}, \\boldsymbol{\\Theta}, \\mathbf{H}) \n",
    "&= \n",
    "\\left[\\prod_{n=1}^N \\prod_{m=1}^M \\mathbb{I}\\Big[x_{n,m}=\\sum_{k=1}^K z_{n,m,k} \\Big] \n",
    "\\prod_{k=1}^K \\mathrm{Po}(z_{n,m,k} \\mid \\theta_{n,k} \\eta_{m,k}) \n",
    "\\right] \\\\\n",
    "&\\qquad\n",
    "\\times \\left[ \\prod_{n=1}^N \\prod_{k=1}^K \\mathrm{Ga}(\\theta_{n,k} \\mid \\alpha_\\theta, \\beta_\\theta) \\right]\n",
    "\\times \\left[ \\prod_{m=1}^M \\prod_{k=1}^K \\mathrm{Ga}(\\eta_{m,k} \\mid \\alpha_\\eta, \\beta_\\eta) \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{Z} \\in \\mathbb{N}^{N\\times M \\times K}$ denotes the _tensor_ of feature ratings, $\\boldsymbol{\\Theta} \\in \\mathbb{R}_+^{N \\times K}$ is a matrix with rows $\\boldsymbol{\\theta}_n$, and $\\mathbf{H} \\in \\mathbb{R}_+^{M \\times K}$ is a matrix with rows $\\boldsymbol{\\eta}_m$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3XxyCKo4ml8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Distribution, Gamma, Poisson, Multinomial\n",
    "from torch.distributions.kl import kl_divergence\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8FSrKLdg4ml5"
   },
   "source": [
    "## Problem 1: Conditional distributions [math]\n",
    "\n",
    "Since this model is constructed from conjugate exponential family distributions, the conditionals are available in closed form. We will let $\\mathbf{z}_{n,m} = (z_{n,m,1}, \\ldots, z_{n,m,K})$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ultqt08q_Bf3"
   },
   "source": [
    "### Problem 1a: Derive the conditional for $\\mathbf{z}_{n, m}$\n",
    "\n",
    "Find the conditional density $p(\\mathbf{z}_{n,m} \\mid x_{n,m}, \\boldsymbol{\\theta}_{n}, \\boldsymbol{\\eta}_{m})$. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2Vdam-OhLD2d"
   },
   "source": [
    "---\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "c6VrbwDkLcsL"
   },
   "source": [
    "### Problem 1b: Derive the conditional for $\\theta_{n,k}$\n",
    "\n",
    "Find the conditional density $p(\\theta_{n,k} \\mid \\mathbf{Z}, \\mathbf{H})$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps_368f2LpUx"
   },
   "source": [
    "---\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2f273L0JLvlS"
   },
   "source": [
    "### Problem 1c: Derive the conditional for $\\eta_{m, k}$\n",
    "\n",
    "Find the conditional density $p(\\eta_{m, k} \\mid \\mathbf{Z}, \\mathbf{\\Theta})$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3Aa7AjQGL4rs"
   },
   "source": [
    "---\n",
    "*Your answer here.*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AdPvBSxY4ml5"
   },
   "source": [
    "## Problem 2: Coordinate ascent variational inference [math]\n",
    "\n",
    "We will perform inference in this model using a mean-field variational posterior which factorizes according to:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q(\\mathbf{Z}, \\mathbf{H}, \\boldsymbol{\\Theta}) \n",
    "&= q(\\mathbf{Z}) q(\\mathbf{H}) q(\\boldsymbol{\\Theta}) \\\\\n",
    "&= \\left[\\prod_{n = 1}^N \\prod_{m = 1}^M q(\\mathbf{z}_{n, m}) \\right] \\left[\\prod_{n = 1}^N \\prod_{k = 1}^K q(\\theta_{n, k}) \\right] \\left[ \\prod_{m = 1}^M \\prod_{k = 1}^K q(\\eta_{m, k}) \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The optimal mean field factors will have the same forms as the conditional distributions above.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8yOHjAg_-6W7"
   },
   "source": [
    "### Problem 2a: Derive the CAVI update for $q(\\mathbf{z}_{n, m})$\n",
    "\n",
    "Show that, fixing $q(\\mathbf{H})$ and $q(\\boldsymbol{\\Theta})$, the optimal $q(\\mathbf{z}_{n, m})$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q(\\mathbf{z}_{n,m}; \\boldsymbol{\\lambda}^{(z)}_{n,m}) \n",
    "&= \\mathrm{Mult}(\\mathbf{z}_{n,m}; x_{n,m}, \\boldsymbol{\\lambda}^{(z)}_{n,m}) \\\\\n",
    "\\log \\lambda^{(z)}_{n,m,k} &= \\mathbb{E}_q[\\log \\theta_{n,k} + \\log \\eta_{m,k}] + c\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zx34Sb7bO-ZK"
   },
   "source": [
    "---\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O36hX7FwOUOR"
   },
   "source": [
    "### Problem 2b: Derive the CAVI update for $q(\\theta_{n, k})$\n",
    "\n",
    "Show that, fixing $q(\\mathbf{Z})$ and $q(\\mathbf{H})$, the optimal $q(\\theta_{n, k})$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q(\\theta_{n,k}; {\\lambda}^{(\\theta)}_{n,k,1}, {\\lambda}^{(\\theta)}_{n,k,2}) \n",
    "&= \\mathrm{Ga}(\\theta_{n,k}; {\\lambda}^{(\\theta)}_{n,k,1}, {\\lambda}^{(\\theta)}_{n,k,2}) \\\\\n",
    "{\\lambda}^{(\\theta)}_{n,k,1} &=  \\alpha_\\theta + \\sum_{m=1}^M \\mathbb{E}_q[z_{n,m,k}] \\\\\n",
    "{\\lambda}^{(\\theta)}_{n,k,2} &=  \\beta_\\theta + \\sum_{m=1}^M \\mathbb{E}_q[\\eta_{m,k}] \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2dA1ZVMHPA0p"
   },
   "source": [
    "---\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N4u7e28XOpE9"
   },
   "source": [
    "### Problem 2c: Derive the CAVI update for $q(\\eta_{m, k})$\n",
    "Show that, fixing $q(\\mathbf{Z})$ and $q(\\boldsymbol{\\Theta})$, the optimal $q(\\eta_{m, k})$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q(\\eta_{m,k}; {\\lambda}^{(\\eta)}_{m,k,1}, {\\lambda}^{(\\eta)}_{m,k,2}) \n",
    "&= \\mathrm{Ga}(\\eta_{m,k}; {\\lambda}^{(\\eta)}_{m,k,1}, \\lambda^{(\\eta)}_{m,k,2}) \\\\\n",
    "{\\lambda}^{(\\eta)}_{m,k,1} &=  \\alpha_\\eta + \\sum_{n=1}^N \\mathbb{E}_q[z_{n,m,k}] \\\\\n",
    "{\\lambda}^{(\\eta)}_{m,k,2} &=  \\beta_\\eta + \\sum_{n=1}^N \\mathbb{E}_q[\\theta_{n,k}] \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JgJfQxODPEMY"
   },
   "source": [
    "---\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KMn16lbeOrUM"
   },
   "source": [
    "### Problem 2d: Find the expected sufficient statistics\n",
    "\n",
    "To update the variational factors, we need the expectations $\\mathbb{E}_q[z_{n, m, k}]$, $\\mathbb{E}_q[\\log \\theta_{n,k} + \\log \\eta_{m,k}]$, $\\mathbb{E}_q[\\theta_{n, k}]$, and $\\mathbb{E}_q[\\eta_{m, k}]$. Assume that each factor follows the forms derived above. That is, assume $q(\\mathbf{z}_{n, m})$ is multinomial with parameters $\\lambda_{n, m}^{(z)}$ while $q(\\theta_{n, k})$ and $q(\\eta_{m k})$ are gamma with parameters $\\left( \\lambda_{n, k, 1}^{(\\theta)}, \\lambda_{n, k, 2}^{(\\theta)} \\right)$ and $\\left( \\lambda_{m, k, 1}^{(\\eta)}, \\lambda_{m, k, 2}^{(\\eta)} \\right)$, respectively. Derive what each of these expectations are in closed form."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KnWv0c0_Q_Qa"
   },
   "source": [
    "---\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "g1RURoje4ml9"
   },
   "source": [
    "## Problem 3: Implement Coordinate Ascent Variational Inference [code]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lylKxs6kIieE"
   },
   "source": [
    "First we'll give some helper functions and objects. Because PyTorch doesn't offer support for batched multinomial distributions in which the total counts differ (e.g. each $\\mathbf{z}_{n, m}$ follows a multinomial distribution in which the total count is $x_{n, m}$), we have defined a `BatchedMultinomial` distribution for your convenience. This distribution doesn't support sampling, but will return the mean of each Multinomial variable in its batch. This is exactly what is needed for the CAVI updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtHlT9H64ml9"
   },
   "outputs": [],
   "source": [
    "def gamma_expected_log(gamma_distbn):\n",
    "    \"\"\"Helper function to compute the expectation of log(X) where X follows a \n",
    "    gamma distribution.\n",
    "    \"\"\"\n",
    "    return torch.digamma(gamma_distbn.concentration) - torch.log(gamma_distbn.rate)\n",
    "\n",
    "class BatchedMultinomial(Multinomial):\n",
    "    \"\"\" \n",
    "    Creates a Multinomial distribution parameterized by `total_count` and\n",
    "    either `probs` or `logits` (but not both). The innermost dimension of\n",
    "    `probs` indexes over categories. All other dimensions index over batches.\n",
    "\n",
    "    The `probs` argument must be non-negative, finite and have a non-zero sum,\n",
    "    and it will be normalized to sum to 1 along the last dimension. `probs` will \n",
    "    return this normalized value. The `logits` argument will be interpreted as \n",
    "    unnormalized log probabilities and can therefore be any real number. It will\n",
    "    likewise be normalized so that the resulting probabilities sum to 1 along\n",
    "    the last dimension. `logits` will return this normalized value.\n",
    "\n",
    "    Args:\n",
    "        total_count (Tensor): number of trials\n",
    "        probs (Tensor): event probabilities\n",
    "            Has shape total_count.shape + (num_categories,)\n",
    "        logits (Tensor): event log probabilities (unnormalized)\n",
    "            Has shape total_count.shape + (num_categories,)\n",
    "\n",
    "    Note: this text is mostly from the PyTorch documentation for the \n",
    "        Multinomial distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, total_count, probs=None, logits=None, validate_args=None):\n",
    "        super().__init__(probs=probs, logits=logits, validate_args=validate_args)\n",
    "        self.total_count = total_count\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.total_count[..., None] * self.probs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Nog6IcLJ-2xa"
   },
   "source": [
    "### Problem 3a: Implement a CAVI update step\n",
    "\n",
    "Using the update equations derived in Problem 2, complete the `cavi_step` function below. \n",
    "\n",
    "*Hint:* Given a `Distribution` named `d`, `d.mean` returns the mean of that distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtLgDbEoUmRI"
   },
   "outputs": [],
   "source": [
    "def cavi_step(X, q_z, q_theta, q_eta, alpha_theta, beta_theta, alpha_eta, beta_eta):\n",
    "    \"\"\"One step of CAVI.\n",
    "\n",
    "    Args:\n",
    "        X: torch.tensor of shape (N, M)\n",
    "        q_z: variational posterior over z, BatchedMultinomial distribution\n",
    "        q_theta: variational posterior over theta, Gamma distribution\n",
    "        q_eta: variational posterior over eta, Gamma distribution\n",
    "\n",
    "    Returns:\n",
    "        (q_z, q_theta, q_eta): Updated distributions after performing CAVI updates\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your code here\n",
    "\n",
    "    # Update q_z\n",
    "    q_z = BatchedMultinomial(...)\n",
    "    \n",
    "    # Update the per-user posterior q_theta\n",
    "    q_theta = Gamma(...)\n",
    "    \n",
    "    # Update the per-item posterior q_eta\n",
    "    q_eta = Gamma(...)\n",
    "    \n",
    "    #\n",
    "    ##\n",
    "\n",
    "    return q_z, q_theta, q_eta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YZ6VM8Si4ml6"
   },
   "source": [
    "### Problem 3b: ELBO Calculation [math]\n",
    "\n",
    "Recall that the evidence lower bound is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(q) = \\mathbb{E}_q \\left[ \\log p(\\mathbf{X}, \\mathbf{Z}, \\mathbf{H}, \\boldsymbol{\\Theta}) - \\log q(\\mathbf{Z}, \\mathbf{H}, \\boldsymbol{\\Theta}) \\right]\n",
    "$$\n",
    "\n",
    "Assume that $q(\\mathbf{Z})$ has support contained in $\\{\\mathbf{Z}: \\sum_{k=1}^K z_{n, m, k} = x_{n, m} \\text{ for all } n, m\\}$. Show that we can rewrite $\\mathcal{L}(q)$ as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(q) = \\mathbb{E}_q [\\log p(\\mathbf{Z} \\mid \\boldsymbol{\\Theta}, \\mathbf{H}) - \\log q(\\mathbf{Z})]  - \\mathrm{KL}(q(\\boldsymbol{\\Theta}) || p(\\boldsymbol{\\Theta})) - \\mathrm{KL}(q(\\mathbf{H}) || p(\\mathbf{H}))\n",
    "$$\n",
    "\n",
    "Next, use that $q(\\mathbf{z}_{n,m}; \\boldsymbol{\\lambda}^{(z)}_{n,m}) =  \\mathrm{Mult}(\\mathbf{z}_{n,m}; x_{n,m}, \\boldsymbol{\\lambda}^{(z)}_{n,m})$ and by plug in the densities of the Poisson and Multinomial distributions to show that we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\mathbb{E}_q [\\log p(\\mathbf{Z} \\mid \\boldsymbol{\\Theta}, \\mathbf{H}) - \\log q(\\mathbf{Z})] \n",
    "= \\\\\n",
    "&\\qquad \\sum_{n = 1}^N \\sum_{m = 1}^M \\mathbb{E}_q \\left[ \\sum_{k =1}^K - \\theta_{n, k} \\eta_{m, k}  + z_{n, m, k} \\log( \\theta_{n, k} \\eta_{m, k} ) - z_{n,m, k} \\log(\\lambda_{n, m, k}^{(z)}) \\right] - \\log(x_{n, m}!)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Explain why we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\mathbb{E}_q \\left[ - \\theta_{n, k} \\eta_{m, k}  + z_{n, m, k} \\log( \\theta_{n, k} \\eta_{m, k} ) - z_{n,m, k} \\log(\\lambda_{n, m, k}^{(z)}) \\right] = \\\\\n",
    "&\\qquad - \\mathbb{E}_q \\left[\\theta_{n, k}\\right] \\mathbb{E}_q \\left[\\eta_{m, k}\\right]  + \\mathbb{E}_q \\left[z_{n, m, k} \\right] \\left( \\mathbb{E}_q \\left[\\log( \\theta_{n, k}) \\right] + \\mathbb{E}_q \\left[\\log (\\eta_{m, k} )\\right] - \\log(\\lambda_{n, m, k}^{(z)}) \\right)  \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ciO4CzAue2g-"
   },
   "source": [
    "---\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eOXsV6g2e1Md"
   },
   "source": [
    "### Problem 3c: Implement the ELBO [code]\n",
    "\n",
    "Using our expression above, write a function which evaluates the evidence lower bound.\n",
    "\n",
    "*Hints:*\n",
    "- Use the `kl_divergence` function imported above to compute the KL divergence between two `Distributions` in the same family.\n",
    "- Recall that for integers $n$, $\\Gamma(n + 1) = n!$ where $\\Gamma$ is the [Gamma function](https://en.wikipedia.org/wiki/Gamma_function). $\\log \\Gamma$ is implemented in PyTorch as `torch.lgamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrKXvI_ZZDwj"
   },
   "outputs": [],
   "source": [
    "def expected_poisson_logpdf(x, expected_rate, expected_log_rate):\n",
    "    \"\"\"Helper function to compute the expected logpdf under a Poisson.\n",
    "    \"\"\"\n",
    "    return -torch.special.gammaln(x + 1) + x * expected_log_rate - expected_rate\n",
    "\n",
    "def elbo(X, q_z, q_theta, q_eta, p_theta, p_eta):\n",
    "    \"\"\"Compute the evidence lower bound.\n",
    "    \n",
    "    Args:\n",
    "        X: torch.tensor of shape (N, M)\n",
    "        q_z: variational posterior over z, BatchedMultinomial distribution\n",
    "        q_theta: variational posterior over theta, Gamma distribution\n",
    "        q_eta: variational posterior over eta, Gamma distribution\n",
    "        p_theta: prior over theta, Gamma distribution\n",
    "        p_eta: prior over eta, Gamma distribution\n",
    "\n",
    "    Returns:\n",
    "        elbo: torch.tensor of shape [] \n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your code below\n",
    "    elbo = ...\n",
    "    #\n",
    "    ##\n",
    "    return elbo / torch.sum(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "S5Nb7H2RrFdv"
   },
   "source": [
    "### Implement CAVI loop [given]\n",
    "\n",
    "Using your functions defined above, complete the function `cavi` below. `cavi` loops for some number of iterations, updating each of the variational factors in sequence and evaluating the ELBO at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fL0Sh0l44ml9"
   },
   "outputs": [],
   "source": [
    "from torch.distributions import Uniform\n",
    "\n",
    "def cavi(data, \n",
    "         num_factors=10, \n",
    "         num_iters=100, \n",
    "         tol=1e-5, \n",
    "         alpha_theta=0.1,\n",
    "         beta_theta=1.0,\n",
    "         alpha_eta=0.1,\n",
    "         beta_eta=1.0,\n",
    "         seed=0\n",
    "        ):\n",
    "    \"\"\"Run coordinate ascent VI for Poisson matrix factorization.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "        elbos, (q_z, q_theta, q_eta):\n",
    "    \"\"\"\n",
    "    data = data.float()\n",
    "    N, M = data.shape\n",
    "    K = num_factors      # short hand\n",
    "    \n",
    "    # Initialize the variational posteriors.\n",
    "    q_eta = Gamma(Uniform(0.5 * alpha_eta, 1.5 * alpha_eta).sample((M, K)),\n",
    "                  Uniform(0.5 * beta_eta, 1.5 * beta_eta).sample((M, K)))\n",
    "    q_theta = Gamma(Uniform(0.5 * alpha_theta, 1.5 * alpha_theta).sample((N, K)),\n",
    "                    Uniform(0.5 * beta_theta, 1.5 * beta_theta).sample((N, K)))\n",
    "    q_z = BatchedMultinomial(data, logits=torch.zeros((N, M, K)))\n",
    "\n",
    "    p_theta = Gamma(alpha_theta, beta_theta)\n",
    "    p_eta = Gamma(alpha_eta, beta_eta)\n",
    "    \n",
    "    # Run CAVI\n",
    "    elbos = [elbo(data, q_z, q_theta, q_eta, p_theta, p_eta)]\n",
    "    for itr in trange(num_iters):\n",
    "        q_z, q_theta, q_eta = cavi_step(data, q_z, q_theta, q_eta,\n",
    "                                        alpha_theta, beta_theta,\n",
    "                                        alpha_eta, beta_eta)\n",
    "        \n",
    "        elbos.append(elbo(data, q_z, q_theta, q_eta, p_theta, p_eta))\n",
    "    return torch.tensor(elbos), (q_z, q_theta, q_eta)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TntW7ldISAZB"
   },
   "source": [
    "### Test your implementation on a toy dataset\n",
    "\n",
    "To check your implementation is working properly, we will fit a mean-field variational posterior using data sampled from the true model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNrtTl7F4ml8"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "N = 100   # num \"users\"\n",
    "M = 1000 # num \"items\"\n",
    "K = 5     # number of latent factors\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # sparse gamma prior with mean alpha/beta \n",
    "beta = 1.0\n",
    "\n",
    "# Sample data from the model\n",
    "torch.manual_seed(305)\n",
    "theta = Gamma(alpha, beta).sample(sample_shape=(N, K))\n",
    "eta = Gamma(alpha, beta).sample(sample_shape=(M, K))\n",
    "data = Poisson(theta @ eta.T).sample()\n",
    "\n",
    "print(data.shape)\n",
    "# Plot the data matrix\n",
    "plt.imshow(data, aspect=\"auto\", vmax=5, cmap=\"Greys\")\n",
    "plt.xlabel(\"items\")\n",
    "plt.ylabel(\"users\")\n",
    "plt.colorbar()\n",
    "\n",
    "print(\"Max data:  \", data.max())\n",
    "print(\"num zeros: \", torch.sum(data == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIhT2Gxb4ml9"
   },
   "outputs": [],
   "source": [
    "elbos, (q_z, q_theta, q_eta) = cavi(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJAlsyjd4ml9"
   },
   "outputs": [],
   "source": [
    "plt.plot(elbos[1:])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"ELBO per entry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ko8IF2E4ml-"
   },
   "outputs": [],
   "source": [
    "true_rates = theta @ eta.T\n",
    "inf_rates = q_theta.mean @ q_eta.mean.T\n",
    "\n",
    "# Plot the data matrix\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(true_rates, aspect=\"auto\", vmax=3, cmap=\"Greys\")\n",
    "plt.xlabel(\"items\")\n",
    "plt.ylabel(\"users\")\n",
    "plt.title(\"true rates\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(inf_rates, aspect=\"auto\", vmax=3, cmap=\"Greys\")\n",
    "plt.xlabel(\"items\")\n",
    "plt.ylabel(\"users\")\n",
    "plt.title(\"inferred rates\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jlg3mYF-Sj7J"
   },
   "source": [
    "## Problem 4: Run your code on a downsampled LastFM dataset\n",
    "\n",
    "Next, we will use data gathered from [Last.FM](www.last.fm) users to fit a PMF model. We use a downsampled version of the [Last.FM-360K users](http://ocelma.net/MusicRecommendationDataset/lastfm-360K.html) dataset. This dataset records how many times each user played an artist's songs. We downsample the data to include only the 2000 most popular artists, as measured by how many users listened to the artist at least once, and the 1000 most prolific users, as measured by how many artists they have listened to.\n",
    "\n",
    "In the code below , we use `lfm` to represent the data matrix $X$ in the model. That is, `lfm[n, d]` denotes how many times the `n`-th user played a song by the `d`-th artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGw03qy3A1oF"
   },
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/slinderman/stats305c/main/assignments/hw5/subsampled_last_fm.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dowD2V2ZTCIh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lfm_df = pd.read_csv('subsampled_last_fm.csv')\n",
    "lfm = lfm_df.pivot_table(index='UserID', columns='ItemID', aggfunc=sum)\\\n",
    "    .fillna(0).astype(int).to_numpy()\n",
    "lfm = torch.tensor(lfm, dtype=torch.int)\n",
    "print(lfm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqdbIfoLTLpI"
   },
   "outputs": [],
   "source": [
    "plt.imshow(lfm, aspect=\"auto\", vmax=100, cmap=\"Greys\")\n",
    "plt.xlabel(\"items\")\n",
    "plt.ylabel(\"users\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v4zM70VW9thg"
   },
   "source": [
    "Using the code below, run coordinate ascent variational inference on this dataset. Our implementation takes around 10-15 minutes to finish, and achieves an ELBO of $-1590$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WVXLxo5eThIe"
   },
   "outputs": [],
   "source": [
    "elbos, (q_z, q_theta, q_eta) = cavi(lfm, \n",
    "     num_factors=40, \n",
    "     num_iters=200, \n",
    "     alpha_theta=1.,\n",
    "     beta_theta=0.5,\n",
    "     alpha_eta=1.,\n",
    "     beta_eta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PU-9DAWLWQdN"
   },
   "outputs": [],
   "source": [
    "print(elbos[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N9-gi38rbtJu"
   },
   "source": [
    "### Investigate \"genres\"\n",
    "\n",
    "The columns of $\\mathbf{H}$ correspond to weights on artists. Intuitively, each of the $K$ columns should put weight on subsets of artists that are often played together. We might think of these columns as reflecting different \"genres\" of music. The code below the top 10 artists for a few of these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T66gYwPEG5Yw"
   },
   "outputs": [],
   "source": [
    "# Find the 10 most used genres\n",
    "genre_loading = q_theta.mean.sum(0)\n",
    "genre_order = torch.argsort(genre_loading, descending=True)\n",
    "\n",
    "# Print the top 10 artists for each of the top 10 genres\n",
    "for genre in genre_order[:10]:\n",
    "    print(\"genre \", genre)\n",
    "    artist_idx = torch.argsort(q_eta.mean[:, genre], \n",
    "                               descending=True)[:10].numpy()\n",
    "    subset = lfm_df[lfm_df['ItemID'].isin(artist_idx)]\n",
    "    print(subset[['ItemID', 'Artist']].drop_duplicates())\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "l4e3QGzE23z0"
   },
   "source": [
    "### Problem 4a\n",
    "\n",
    "Inspect the data either using the csv file or the pandas dataframe and choose a user who has listened to artists you recognize. If you are not familiar with any of the artists, use the listener with UserID 349, who mostly listens to hip-hop artists. For the particular user $n$ you choose, find the 10 artists who are predicted to have the most plays by sorting the vector of mean song counts predicted by the model, i.e. the $n^{\\text{th}}$ row of $\\mathbb{E}_q [\\boldsymbol{\\Theta} \\mathbf{H}^\\top ]$. Are these artists you would expect the user would enjoy? Are there any artists that the user has not listened to?\n",
    "\n",
    "*Hint: Use `torch.argsort(..., descending=True)` to return the indices of the largest elements of a vector in descending order.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckzacw9157yV"
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Your code here.\n",
    "##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kZ0e3wajNBqB"
   },
   "source": [
    "## Problem 5: Reflections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jucxw8ESm1xX"
   },
   "source": [
    "### Problem 5a\n",
    "\n",
    "Discuss one advantage and one disadvantage of fitting a posterior using variational inference vs. sampling from the posterior using MCMC."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eTBD3Xp6z0FX"
   },
   "source": [
    "---\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yu-cce1knLN-"
   },
   "source": [
    "### Problem 5b\n",
    "\n",
    "First, explain why the assumption that $\\mathbf{Z}, \\mathbf{H}$ and $\\boldsymbol{\\Theta}$ are independent in the posterior will never hold. \n",
    "\n",
    "Next, recall that maximizing the ELBO is equivalent to minimizing the KL divergence between the approximate posterior and the true posterior. In general, how will the approximate posterior differ from the true posterior, given that the variational family does not include the true posterior?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5jzvSBwQzxkz"
   },
   "source": [
    "---\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zY7NzMa1nhuO"
   },
   "source": [
    "### Problem 5c\n",
    "\n",
    "Suppose we are using this model to recommend new items to users. Describe one improvement that could be made to the model which you think would lead to better recommendations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PLCKj_q2zuFX"
   },
   "source": [
    "---\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3-nF2rIqzrkm"
   },
   "source": [
    "## Submission Instructions\n",
    "\n",
    "\n",
    "**Formatting:** check that your code does not exceed 80 characters in line width. If you're working in Colab, you can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
    "\n",
    "Download your notebook in .ipynb format and use the following commands to convert it to PDF:\n",
    "```\n",
    "jupyter nbconvert --to pdf hw5_yourname.ipynb\n",
    "```\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "- `nbconvert`: If you're using Anaconda for package management, \n",
    "```\n",
    "conda install -c anaconda nbconvert\n",
    "```\n",
    "\n",
    "**Upload** your .pdf files to Gradescope. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STATS305C HW5: Poisson Matrix Factorization",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}