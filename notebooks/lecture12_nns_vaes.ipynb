{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STATS305C Lecture 12: Neural Networks in PyTorch",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 12: Neural Networks in PyTorch\n",
        "\n",
        "\n",
        "STATS305C, Stanford University, Spring 2022\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slinderman/stats305c/blob/master/notebooks/lecture12_nns_vaes.ipynb)\n"
      ],
      "metadata": {
        "id": "DIaL2XtgC5_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Background: \n",
        "\n",
        "In this notebook, we will explore automatic differentiation, neural networks, and amortized variational inference. PyTorch makes it easy to construct such networks and train them with backpropagation. We'll give you a few examples to help get you started.\n",
        "\n"
      ],
      "metadata": {
        "id": "5ZQgpVFL_pz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.distributions import Normal\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "torch.manual_seed(305)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.cm import Blues\n",
        "import seaborn as sns\n",
        "sns.set_context(\"notebook\")"
      ],
      "metadata": {
        "id": "cYTJS_5FHb6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Regression with Deep Neural Networks\n",
        "\n",
        "We'll start off with a simple regressoin problem: train a neural network to approximate the function $f(x) = \\sin(x)$. We'll give the network lots of noisy observations $(x_n, y_n)$ where $y_n \\sim \\mathcal{N}(f(x_n), \\sigma^2)$. We'll use a simple feedforward network and train it with SGD."
      ],
      "metadata": {
        "id": "N1ulvTBkDkKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulate data"
      ],
      "metadata": {
        "id": "igZX7lHF7wjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the dataset\n",
        "torch.manual_seed(0)\n",
        "N = 10000\n",
        "sigma = 0.1\n",
        "xs = torch.randn(N)\n",
        "ys = torch.sin(xs) + sigma * torch.randn(N)\n",
        "\n",
        "x_grid = torch.linspace(-3, 3, 50)\n",
        "plt.plot(xs, ys, 'k.', alpha=0.1)\n",
        "plt.plot(x_grid, torch.sin(x_grid), lw=3)"
      ],
      "metadata": {
        "id": "XjtPOyNI6poO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct a feedforward neural network\n",
        "\n",
        "This code implements a simple feedforward neural network class that inherits from `nn.Module`. Modules implement a `forward` function that maps inputs to outputs. The class variables (`self.fc1` and `self.fc2` are Modules themselves, each with their own parameters. When it comes to training the network, we can use the `parameters()` function to get the list of trainable parameters - this function introspects on each of the class variables and extracts the parameters associated with each layer.\n",
        "\n",
        "_Note:_ for simple networks like this one, PyTorch offers a [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) object that saves you a few lines of code. Here we've built the module from scratch to show the more general approach. We'll extend this module in Part 2."
      ],
      "metadata": {
        "id": "aJR0F3-k7th6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    \"\"\"A simple feedforward neural network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer_size=20):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Construct linear \"fully connected\" (fc) layers\n",
        "        self.fc1 = nn.Linear(in_features=1, out_features=layer_size)\n",
        "        self.fc2 = nn.Linear(in_features=layer_size, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Implements the forward pass of the network.\n",
        "\n",
        "        Args:\n",
        "            x: torch.tensor of shape (N,)\n",
        "        Returns:\n",
        "            z: torch.tensor of shape (N,) containing the logits\n",
        "        \"\"\"\n",
        "        # Note we have to unsqueeze the inputs to make them explicitly 1D\n",
        "        h1 = F.relu(self.fc1(x.unsqueeze(1)))\n",
        "        # Note we have to squeeze the outputs to make them explicitly 0D\n",
        "        preds = self.fc2(h1).squeeze(1)\n",
        "        return preds\n",
        "\n",
        "# Construct an instance of the model. It will be given with random weights \n",
        "# per PyTorch's default initialization strategy.\n",
        "model = SimpleNet()"
      ],
      "metadata": {
        "id": "GqTtPs6M71cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the predictions of the randomly initialized model"
      ],
      "metadata": {
        "id": "0q7-aQw3ucRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x_grid, torch.sin(x_grid), lw=3, label=\"true f(x)\")\n",
        "plt.plot(x_grid, model.forward(x_grid).detach(), label=\"model $\\hat{f}(x)$\") \n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "FqnINOSw-YTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimize the model parameters to maximize the likelihood\n",
        "\n",
        "The log likelihood in this model is,\n",
        "\\begin{align}\n",
        "\\log p(\\mathbf{y} \\mid \\mathbf{x}, \\theta, \\sigma^2) \n",
        "&= \\sum_{n=1}^N \\log \\mathcal{N}(y_n \\mid f(x_n; \\theta), \\sigma^2) \\\\\n",
        "&= -\\frac{1}{2\\sigma^2} \\sum_{n=1}^N \\| y_n - f(x_n; \\theta) \\|_2^2 + c\n",
        "\\end{align}\n",
        "where $f: \\mathbb{R} \\mapsto \\mathbb{R}$ is a neural network with parameters $\\theta$. \n",
        "\n",
        "Maximizing the log likelihood is equivalent to **minimizing the mean squared error**,\n",
        "\\begin{align}\n",
        "\\mathcal{L}(\\theta) &= \\frac{1}{N} \\sum_{n=1}^N \\| y_n - f(x_n; \\theta) \\|_2^2 \n",
        "\\end{align}\n",
        "\n",
        "The code below minimizes this objective using Adam, a stochastic optimization algorithm."
      ],
      "metadata": {
        "id": "EvpzYN-fuedl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next we define a loss function to minimize. In our case, we want this \n",
        "# to be related to the log likelihood of ys given xs, which is proportional\n",
        "# to the mean squared error of the predictions.\n",
        "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "\n",
        "# Use the optim package to define an Optimizer that will update the weights of\n",
        "# the model for us. Here we will use Adam; the optim package contains many other\n",
        "# optimization algorithms. The first argument to the Adam constructor tells the\n",
        "# optimizer which Tensors it should update.\n",
        "num_iters = 2000\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for t in range(num_iters):\n",
        "    # Forward pass: compute predicted y by passing x to the model.\n",
        "    y_pred = model(xs)\n",
        "\n",
        "    # Compute and print loss.\n",
        "    loss = loss_fn(y_pred, ys)\n",
        "    if t % 100 == 99:\n",
        "        print(\"Iter\", t, \": \", loss.item())\n",
        "\n",
        "    # Before the backward pass, use the optimizer object to zero all of the\n",
        "    # gradients for the variables it will update (which are the learnable\n",
        "    # weights of the model). This is because by default, gradients are\n",
        "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its parameters\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "xcAHEjzh-MEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the trained model's predictions"
      ],
      "metadata": {
        "id": "_WkFuZr4uknF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_grid = torch.linspace(-3, 3, 50)\n",
        "y_pred = model.forward(x_grid)\n",
        "\n",
        "# We have to \"detach\" the predictions from the computation graph\n",
        "# before we can plot them.\n",
        "y_pred = y_pred.detach()\n",
        "\n",
        "plt.plot(x_grid, torch.sin(x_grid), lw=3, label=\"true f(x)\")\n",
        "plt.plot(x_grid, y_pred, label=\"model $\\hat{f}(x)$\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "mzZRGl459PF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "\n",
        "1. Add another layer to the SimpleNet\n",
        "2. Try predicting beyond the range of the training data. What do the predictions look like?\n",
        "3. Swap out the rectified linear (relu) activation with another nonlinearity, like the sigmoid or the GELU activation.\n",
        "4. Try different optimizers (SGD with and without momentum, RMSProp)\n"
      ],
      "metadata": {
        "id": "Mgd3tPsvPHis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Variational Autoencoders\n",
        "\n",
        "In this section, we will train a variational autoencoder for a simple synthetic data. This is essentially a generalization of the sinusoidal regression problem from above. Here, the data is generated from,\n",
        "\\begin{align}\n",
        "z_n &\\sim \\mathcal{N}(0, 1) \\\\\n",
        "x_n &\\sim \\mathcal{N}(f(z_n), \\sigma^2 I) \n",
        "\\end{align}\n",
        "where \n",
        "\\begin{align}\n",
        "f(z) = \\begin{bmatrix} \\cos(z) \\\\ \\sin(z) \\end{bmatrix}\n",
        "\\end{align}\n",
        "Thus, the data is essentially lives near a 1D nonlinear manifold (here, a circle)."
      ],
      "metadata": {
        "id": "IKp_lRGVgtJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the dataset\n",
        "torch.manual_seed(0)\n",
        "N = 20000\n",
        "sigma = 0.1\n",
        "zs = torch.randn(N)\n",
        "xs = torch.column_stack([torch.cos(zs), \n",
        "                         torch.sin(zs)]) + sigma * torch.randn((N, 2))\n",
        "\n",
        "x_grid = torch.linspace(-3, 3, 50)\n",
        "plt.scatter(xs[:,0], xs[:,1], c=zs, cmap=\"autumn\", ec='w')\n",
        "plt.gca().set_aspect(\"equal\")"
      ],
      "metadata": {
        "id": "dUdWNkjXZYvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a simple data loader for this dataset\n",
        "\n",
        "Data loaders are convenience wrappers for iterating over minibatches of data points."
      ],
      "metadata": {
        "id": "X7GO61ZdDYUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(xs[:(3*N//4)], batch_size=1000, shuffle=True)\n",
        "test_dataloader = DataLoader(xs[(3*N//4):], batch_size=1000, shuffle=True)"
      ],
      "metadata": {
        "id": "GrcSRbHB-55-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2a: Decoder Network\n",
        "\n",
        "Implemen+t a decoder that maps $\\mathbb{R} â†¦ \\mathbb{R}^2$ with a 2 layer decoder."
      ],
      "metadata": {
        "id": "Gz0h_qvdkl47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define decoder architecture\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\" Neural network defining p(x | z) \"\"\"\n",
        "\n",
        "    def __init__(self, data_dim, latent_dim, hidden_dims=[20, 20], noise_scale=0.1):\n",
        "        super().__init__()\n",
        "        self.data_dim = data_dim\n",
        "        self.noise_scale = noise_scale\n",
        "\n",
        "        ###\n",
        "        # Your code here\n",
        "        ##\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\" Computes means of p(x | z).\n",
        "        Args:\n",
        "            z: (N, latent_dim) torch.tensor\n",
        "        Returns:\n",
        "            Normal distribution\n",
        "        \"\"\"\n",
        "        ###\n",
        "        # Your code here\n",
        "        ## \n",
        "        return Normal(..., ...)"
      ],
      "metadata": {
        "id": "YFih1GzaZcaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2b: Encoder Network\n",
        "\n",
        "We will estimate the parameters of the generative model by maximizing the Evidence Lower Bound (ELBO). As the exact posterior $p(\\mathbf{z} \\mid \\mathbf{x})$ is unknown, we will use an approximate, amortized posterior $q_{\\boldsymbol{\\phi}}(\\mathbf{z} \\mid \\mathbf{x}) = \\mathcal{N}(\\mathbf{z} \\mid \\mu_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\text{diag}(\\sigma^2_{\\boldsymbol{\\phi}}(\\mathbf{x})))$. We let $\\left(\\mu_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\log \\sigma^2_{\\boldsymbol{\\phi}}(\\mathbf{x}) \\right) = E_{\\boldsymbol{\\phi}}(\\mathbf{x})$ where $E_{\\boldsymbol{\\phi}}: \\mathbb{R}^{2} \\to \\mathbb{R}^1 \\times \\mathbb{R}^1$ is a neural network with parameters $\\boldsymbol{\\phi}$. \n",
        "\n",
        "As above, we parametrize $E_{\\boldsymbol{\\phi}}$ as a neural network with two layers of hidden units and ReLU activations. We use 20 hidden units in the first layer and 20 in the second. Then we let $\\mu_{\\boldsymbol{\\phi}}$ and $\\log \\sigma^2_{\\boldsymbol{\\phi}}$ be affine functions of the hidden layer activations. Implement the encoder $E_{\\boldsymbol{\\phi}}$ in the code below."
      ],
      "metadata": {
        "id": "JFLYAq5xmwjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define encoder architecture\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\" Neural network defining q(z | x). \"\"\"\n",
        "\n",
        "    def __init__(self, data_dim, latent_dim, hidden_dims=[20, 20]):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        ###\n",
        "        # Your code here\n",
        "        ##\n",
        "    def forward(self, x):\n",
        "        \"\"\" Computes normal distribution q(z | x)\n",
        "\n",
        "        Args:\n",
        "            x: (N, data_dim) torch.tensor\n",
        "        Returns:\n",
        "            Normal distribution\n",
        "        \"\"\"\n",
        "        ###\n",
        "        # Your code here\n",
        "        ##\n",
        "        return Normal(..., ...)"
      ],
      "metadata": {
        "id": "vVPRAfWhZemu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2c: Implement the ELBO\n",
        "\n",
        "\n",
        "In class we derived the local ELBO and showed that it can be written as the **expected log likelihood** minus the **KL divergence to the prior**. With the reparameterization trick for a Gaussian variational posterior, this is:\n",
        "\\begin{align*}\n",
        "\\mathcal{L}_n(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}) \n",
        "&= \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z}_n \\mid \\mathbf{x}_n)} \\left[ \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}_n \\mid \\mathbf{z}_n) \\right] - \\text{KL}\\left( q_{\\boldsymbol{\\phi}}(\\mathbf{z}_n \\mid \\mathbf{x}_n) \\mid\\mid p(\\mathbf{z}_n)\\right) \\\\\n",
        "&= \\mathbb{E}_{\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})} \\left[ \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}_n \\mid \\mu_{\\boldsymbol{\\phi}}(\\mathbf{x}_n) + \\boldsymbol{\\epsilon} \\odot \\sigma_{\\boldsymbol{\\phi}}(\\mathbf{x}_n)) \\right] - \\text{KL}\\left( q_{\\boldsymbol{\\phi}}(\\mathbf{z}_n \\mid \\mathbf{x}_n) \\mid\\mid p(\\mathbf{z}_n)\\right)\n",
        "\\end{align*}\n",
        "(Though we have written the local ELBO as a function of $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\phi}$ only, note that it is of course also a function of the data point $\\mathbf{x}_n$.\n",
        "\n",
        "\n",
        "Implement the estimator of the ELBO $\\hat{\\mathcal{L}}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi})$. We assume sampling of the data point (or minibatch of data points) `x` is done outside of the function, but you must sample the noise variables $\\boldsymbol{\\epsilon}$ within the `elbo` function. You should use the `kl_divergence` function imported above to analytically compute the KL divergence between the Gaussian distributions $q_{\\boldsymbol{\\phi}}(\\mathbf{z} \\mid \\mathbf{x})$ and $p(\\mathbf{z})$. Make sure you use `rsample` on a `Distribution` object to use the reparametrization trick and not `sample`."
      ],
      "metadata": {
        "id": "l9MxjEL_vEyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def elbo(x, encoder, decoder):\n",
        "    \"\"\" Computes a stochastic estimate of the rescaled evidence lower bound\n",
        "\n",
        "    Args:\n",
        "        x: (N, data_dim) torch.tensor\n",
        "        encoder: an Encoder\n",
        "        decoder: a Decoder\n",
        "    Returns:\n",
        "        elbo: a (,) torch.tensor containing the estimate of the ELBO\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # Your code here\n",
        "    ##\n",
        "    return elbo"
      ],
      "metadata": {
        "id": "oD8-PqawoVVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the Training Loop\n",
        "\n",
        "Using our `Encoder` and `Decoder` definitions, as well as the `elbo` function, we have provided training code below. This code uses the [Adam](https://arxiv.org/abs/1412.6980) optimizer, an optimization algorithm which uses the history of past gradients to rescale gradients before applying an update."
      ],
      "metadata": {
        "id": "VWImC7SvwALR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(data_dim=2, latent_dim=1)\n",
        "decoder = Decoder(data_dim=2, latent_dim=1)\n",
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=3e-4)\n",
        "\n",
        "num_epochs = 500\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    train_elbo = 0\n",
        "    for batch_idx, x in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = -elbo(x, encoder, decoder)  \n",
        "        loss.backward()\n",
        "        train_elbo -= loss.item() * len(x)\n",
        "        optimizer.step()\n",
        "            \n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    test_elbo = 0\n",
        "    with torch.no_grad():\n",
        "        for x in test_dataloader:\n",
        "            test_elbo += elbo(x, encoder, decoder).item() * len(x)\n",
        "            \n",
        "    train_elbo /= len(train_dataloader.dataset)\n",
        "    test_elbo /= len(test_dataloader.dataset)\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        print('====> Epoch: {} Average ELBO: {:.4f} Test ELBO: {:.4f}'.format(\n",
        "            epoch, train_elbo, test_elbo))\n"
      ],
      "metadata": {
        "id": "TQixwXfsZiby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize samples from the trained model\n",
        "\n",
        "In addition to the ELBO, we can sample from the trained model to assess its performance. Use the code below to generate an $8 \\times 8$ grid of sampled digits from the model. Note that we follow the common practice of using the mean of $p_{\\boldsymbol{\\theta}}(\\mathbf{x} \\mid \\mathbf{z})$ rather than resampling from this distribution when visualizing samples. Critique these samples. What aspects of the data distribution does the model seem to have trouble learning?"
      ],
      "metadata": {
        "id": "hIbwsU3p3fQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sampled digits from our model\n",
        "decoder.eval()\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(10000, 1)\n",
        "    samples = decoder.forward(z).sample()\n",
        "    \n",
        "plt.scatter(samples[:, 0], samples[:, 1], c=z[:, 0], cmap=\"autumn\", ec='w')\n",
        "plt.gca().set_aspect(1)"
      ],
      "metadata": {
        "id": "wr7oSJPttGQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}