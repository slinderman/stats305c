# STATS305C: Applied Statistics III
Instructor: Scott Linderman <br>
TA: Matt MacKay, James Yang <br>
Term: Spring 2022 <br>
Stanford University

---

## Course Description: 
Probabilistic modeling and inference of multivariate data. Topics may include multivariate Gaussian models, probabilistic graphical models, MCMC and variational Bayesian inference, dimensionality reduction, principal components, factor analysis, matrix completion, topic modeling, and state space models. Extensive work with data involving programming, ideally in Python. 

## Prerequisites:
Students should be comfortable with probability and statistics as well as multivariate calculus and linear algebra. This course will emphasize implementing models and algorithms, so coding proficiency is required.

## Logistics:
- Time: Monday and Wednesday, 11:30am-1pm 
- Level: advanced undergrad and up
- Grading basis: credit or letter grade
- Office hours: 
  - Monday 1-2pm (Scott)
  - Tuesday 11am-12:30pm [Zoom](https://stanford.zoom.us/j/98216869509?pwd=anZoZTJIRmYvOS9ET2Jza1hiclJQUT09) for now (trying to get a room for future OH) (James)
  - Tuesday 5:30-7pm in Fish Bowl, Sequoia Hall (Matt)
- Final evaluation: Exam

## Books
- Bishop. Pattern recognition and machine learning. New York: Springer, 2006. [link](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- Murphy. Probabilistic Machine Learning: An Introduction. MIT Press, 2022. [link](https://probml.github.io/pml-book/book2.html)
- Gelman et al. Bayesian Data Analysis. Chapman and Hall, 2005. [link](http://www.stat.columbia.edu/~gelman/book/)

## Assignments
- [Assignment 1: Bayesian Linear Regression](https://github.com/slinderman/stats305c/blob/main/assignments/hw1/hw1.ipynb). Due **Weds. Apr 6 at midnight** on GradeScope. 

## Tentative Schedule

### Week 1 (3/28 & 3/30): Multivariate Normal Models and Conjugate Priors
- Required Reading: Bishop, Ch 2.3
- Optional Reading: Murphy, Ch 2.3 and 3.2.4

### Week 2 (4/4 & 4/6): Hierarchical Models and Gibbs Sampling
- Required Reading: Bishop, Ch 8.1-8.2 and 11.2-11.3
- Optional Reading: Murphy, Ch 3.5.2, 4.2, and 12.1-12.3
- Optional Reading: Gelman, Ch 5

### Week 3 (4/11 & 4/13): Probabilistic PCA and Factor Analysis
- Required Reading: Bishop, Ch 12.1-12.2 

### Week 4 (4/18 & 4/20): Mixture Models and EM
- Required Reading: Bishop, Ch 9
- Optional Reading: Murphy, Ch 6.7

### Week 5 (4/25 & 4/27): Mixed Membership Models and Mean Field VI
- Required Reading: "Probabilistic topic models" [Blei, 2012](http://www.cs.columbia.edu/~blei/fogm/2020F/readings/Blei2012.pdf)
- Required Reading: "Variational Inference: A Review for Statisticians” [Blei et al, 2017](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773)
- Optional Reading: Murphy, Ch 10.2

### Week 6 (5/2 & 5/4): Variational Autoencoders and Fixed-Form VI
- Required Reading: “An Introduction to Variational Autoencoders” (Ch 1 and 2) [Kingma and Welling, 2019](https://arxiv.org/pdf/1906.02691.pdf)
- Optional Reading: Murphy, Ch 10.3

### Week 7 (5/9 & 5/11): State Space Models and Message Passing
- Required Reading: Bishop, Ch 13
- Optional Reading: Murphy, Ch 8

### Week 8 (5/16 & 5/18): Bayesian Nonparametrics and more MCMC
- Required Reading: Bishop, Ch 6.4
- Optional Reading: [Adams et al, 2019](https://homepages.inf.ed.ac.uk/imurray2/pub/09poisson/adams-murray-mackay-2009b.pdf)

### Weeks 9 and 10: Research Topics in Probabilistic Machine Learning
- TBD
