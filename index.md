# STATS305C: Applied Statistics III
Instructor: Scott Linderman <br>
TA: Xavier Gonzalez <br>
Term: Spring 2023 <br>
Stanford University

---

## Course Description:
Probabilistic modeling and inference of multivariate data. Topics may include multivariate Gaussian models, probabilistic graphical models, MCMC and variational Bayesian inference, dimensionality reduction, principal components, factor analysis, matrix completion, topic modeling, and state space models. Extensive work with data involving Python programming using PyTorch.

## Prerequisites:
Students should be comfortable with probability and statistics as well as multivariate calculus and linear algebra. This course will emphasize implementing models and algorithms, so coding proficiency is required.

## Logistics:
- Time: Tuesday and Thursday, 10:30-11:50am
- Level: advanced undergrad and up
- Grading basis: credit or letter grade
- Office hours:
  - Weds 4:30-5:30pm in Wu Tsai Neurosciences Instiute Room M252G (Scott)
  - Tuesday 5-7pm location TBD (Xavier)

## Books
- Murphy. Probabilistic Machine Learning: Advanced Topics. MIT Press, 2023. [link](https://probml.github.io/pml-book/book2.html)

You may also find the following references helpful:
- Bishop. Pattern recognition and machine learning. New York: Springer, 2006. [link](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- Gelman et al. Bayesian Data Analysis. Chapman and Hall, 2005. [link](http://www.stat.columbia.edu/~gelman/book/)

## Schedule

### Week 1 (3/28 & 3/30): Multivariate Normal Models and Conjugate Priors
- Required Reading: Bishop, Ch 2.3
- Optional Reading: Murphy, Ch 2.3 and 3.2.4

### Week 2 (4/4 & 4/6): Hierarchical Models and Gibbs Sampling
- Required Reading: Bishop, Ch 8.1-8.2 and 11.2-11.3
- Optional Reading: Murphy, Ch 3.5.2, 4.2, and 11.1-11.3
- Optional Reading: Gelman, Ch 5

### Week 3 (4/11 & 4/13): Continuous Latent Variable Models and HMC
- Required Reading: Bishop, Ch 12.1-12.2
- Required Reading: MCMC using Hamiltonian dynamics [Neal, 2012](https://arxiv.org/abs/1206.1901)

### Week 4 (4/18 & 4/20): Mixture Models and EM
- Required Reading: Bishop, Ch 9
- Optional Reading: Murphy, Ch 6.7

### Week 5 (4/25 & 4/27): Mixed Membership Models and Mean Field VI
- Required Reading: "Probabilistic topic models" [Blei, 2012](http://www.cs.columbia.edu/~blei/fogm/2020F/readings/Blei2012.pdf)
- Required Reading: "Variational Inference: A Review for Statisticians” [Blei et al, 2017](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773)
- Optional Reading: Murphy, Ch 10.2

### Week 6 (5/2 & 5/4): Variational Autoencoders and Fixed-Form VI
- Required Reading: “An Introduction to Variational Autoencoders” (Ch 1 and 2) [Kingma and Welling, 2019](https://arxiv.org/pdf/1906.02691.pdf)
- Optional Reading: Murphy, Ch 10.3

### Week 7 (5/9 & 5/11): State Space Models and Message Passing
- Required Reading: Bishop, Ch 13
- Optional Reading: Murphy, Ch 8

### Week 8 (5/16 & 5/18): Bayesian Nonparametrics and more MCMC
- Required Reading: Bishop, Ch 6.4
- Optional Reading: [Kingman, 1993, Ch 1-2](https://global.oup.com/academic/product/poisson-processes-9780198536932)
- Optional Reading: [Adams et al, 2019](https://homepages.inf.ed.ac.uk/imurray2/pub/09poisson/adams-murray-mackay-2009b.pdf)

### Weeks 9 and 10: Research Topics in Probabilistic Machine Learning
- TBD

## Grading
- 8 homeworks: 10% each, total 80%
- Final exam: 15%
- Participation: 5%
